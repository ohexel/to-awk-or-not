{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Next scheduled workshop 10 and 13 September 2021, via Zoom course information Why awk? Awk is useful when the overhead of more sophisticated approaches is not worth the bother. Quote The Enlightened Ones say that... You should never use C if you can do it with a script ; You should never use a script if you can do it with awk ; Never use awk if you can do it with sed ; Never use sed if you can do it with grep . *source: (some years ago) http://awk.info/?whygawk Objectives The material on this site is not a complete guide or awk manual. The purpose of this site is to give an overview of the capabilities of the awk language and to underline some particular strengths or disadvantages . The page aims to promote this tool for use in every-days research work and urges you to find solutions yourself rather than expecting ready-made ones. The material assumes that you are somewhat familiar with grep , sed and have some basic programing experience. What is awk? AWK is an interpreted programming language designed for text processing and typically used as a data extraction and reporting tool. The AWK language is a data-driven scripting language consisting of a set of actions to be taken against streams of textual data - either run directly on files or used as part of a pipeline - for purposes of extracting or transforming text, such as producing formatted reports. The language extensively uses the string datatype, associative arrays (that is, arrays indexed by key strings), and regular expressions. AWK has a limited intended application domain, and was especially designed to support one-liner programs . It is a standard feature of most Unix-like operating systems. source: Wikipedia","title":"Home"},{"location":"#why-awk","text":"Awk is useful when the overhead of more sophisticated approaches is not worth the bother. Quote The Enlightened Ones say that... You should never use C if you can do it with a script ; You should never use a script if you can do it with awk ; Never use awk if you can do it with sed ; Never use sed if you can do it with grep . *source: (some years ago) http://awk.info/?whygawk","title":"Why awk?"},{"location":"#objectives","text":"The material on this site is not a complete guide or awk manual. The purpose of this site is to give an overview of the capabilities of the awk language and to underline some particular strengths or disadvantages . The page aims to promote this tool for use in every-days research work and urges you to find solutions yourself rather than expecting ready-made ones. The material assumes that you are somewhat familiar with grep , sed and have some basic programing experience.","title":"Objectives"},{"location":"#what-is-awk","text":"AWK is an interpreted programming language designed for text processing and typically used as a data extraction and reporting tool. The AWK language is a data-driven scripting language consisting of a set of actions to be taken against streams of textual data - either run directly on files or used as part of a pipeline - for purposes of extracting or transforming text, such as producing formatted reports. The language extensively uses the string datatype, associative arrays (that is, arrays indexed by key strings), and regular expressions. AWK has a limited intended application domain, and was especially designed to support one-liner programs . It is a standard feature of most Unix-like operating systems. source: Wikipedia","title":"What is awk?"},{"location":"1.Simple_example/","text":"1.Simple examples Let's begin simple It is quite easy to use Awk from the command line to perform simple operations on text files. Suppose we have a file named \"coins.txt\" that describes a coin collection. Each line in the file contains the following information: metal weight in ounces date minted country of origin description The file has the contents: gold 1 1986 USA American Eagle gold 1 1908 Austria-Hungary Franz Josef 100 Korona silver 10 1981 USA ingot gold 1 1984 Switzerland ingot gold 1 1979 RSA Krugerrand gold 0.5 1981 RSA Krugerrand gold 0.1 1986 PRC Panda silver 1 1986 USA Liberty dollar gold 0.25 1986 USA Liberty 5-dollar piece silver 0.5 1986 USA Liberty 50-cent piece silver 1 1987 USA Constitution dollar gold 0.25 1987 USA Constitution 5-dollar piece gold 1 1988 Canada Maple Leaf The command bellow will search through the file for lines of text that contain the string \"gold\", and print them out. $ awk '/gold/' coins . txt gold 1 1986 USA American Eagle gold 1 1908 Austria - Hungary Franz Josef 100 Korona gold 1 1984 Switzerland ingot gold 1 1979 RSA Krugerrand gold 0.5 1981 RSA Krugerrand gold 0.1 1986 PRC Panda gold 0.25 1986 USA Liberty 5 - dollar piece gold 0.25 1987 USA Constitution 5 - dollar piece gold 1 1988 Canada Maple Leaf /gold/ defines a matching criteria that will be used on every line of the file. If no command is specified, the default action is to print the matche d lines. This mimics the use of grep or sed , so why would one possibly need awk? Now, suppose we want to list all the coins that were minted before 1980 . We invoke Awk as follows: $ awk '$3 < 1980 {print $3, \" \",$5,$6,$7,$8}' coins . txt 1908 Franz Josef 100 Korona 1979 Krugerrand In the example above, we defined a matching criteria $ 3 < 1980 , so the commands enclosed in the { print $ 3 , \" \" , $ 5 , $ 6 , $ 7 , $ 8 } block will be execut ed only when the criteria is met - i.e. awk will print the values of columns 3,\" \",5,6,7, and 8. The columns are separated (defined) by white space (o ne ore more consecutive blanks) and addressed by the $ sign i.e. $ 1 is the value of the first column, $ 2 - second etc. $ 0 contains the whol e line including the white spaces. What about some math? Can I manipulate or analyze the data? Let's use the following simple file that contains 3 lines with numbers and some text, just to make our life more difficult (or maybe not?) 1 2 3 4 5 6 7 8 9 10 text Let's try to make some calculations with the data in this file - summation and multiplication in this case. $ awk ' {print $1+$2*$3}' data 7 34 79 Again, we did not provide any matching criteria, so the commands will be executed on every line. Try to run the command. What do you think? Does awk provid e the right answer? I am serious! ;-) Let's do the math only if the value in the first column is greater than 4. $ awk '$1 > 4 {print $1+$2*$3}' data 79 Now, $ 1 > 4 is our criteria on when to execute the command block { print $ 1 +$ 2 *$ 3 } . Notice that the criteria is outside the {} block. Awk command-line syntax: $ awk ' /pattern/ {action} ' file1 file2 ... fileN action is performed on every line that matches pattern . If pattern is not provided, action is performed on every line. If action is not provided, then all matching lines are simply sent to standard output. Since patterns and actions are optional, actions must be enclosed in braces to distinguish them from pattern. The statements in an awk program may be indented and formatted using spaces, tabs, and new lines. Two special patterns: BEGIN (execute an action before first input line) and END ( ... after all lines are read.) Simple output examples { print } - will print the whole line to standard out { print $ 0 } - will do the same thing { print $ 1 , $ 3 } - expressions separated by a comma are, by default, separated by a single space when output { print NF , $ 1 , $ NF } - will print the number of fields, the first field, and the last field in the current record { print $ ( NF - 2 ) } prints the third to last field Computing and Printing { print $ 1 , $ 2 * $ 3 } You can also do computations on the field values and include the results in your output { print NR , $ 0 } - will print each line prefixed with its line number Putting Text in the Output { print \"total pay for\" , $ 1 , \"is\" , $ 2 * $ 3 } you can also add other text to the output besides what is in the current record. Note that the inserted text needs to be surrounded by double quotes. { printf ( \"total pay for %s is $%.2f\\n\" , $ 1 , $ 2 * $ 3 ) } when using printf, formatting is under your control, so no automatic spaces or newlines are pr ovided by awk. You have to insert them yourself. { printf ( \"%-8s %6.2f\\n\" , $ 1 , $ 2 * $ 3 ) } - well, this escalated too fast... More on format modifers: gawk documentation Files coins.txt","title":"1.Simple examples"},{"location":"1.Simple_example/#1simple-examples","text":"","title":"1.Simple examples"},{"location":"1.Simple_example/#lets-begin-simple","text":"It is quite easy to use Awk from the command line to perform simple operations on text files. Suppose we have a file named \"coins.txt\" that describes a coin collection. Each line in the file contains the following information: metal weight in ounces date minted country of origin description The file has the contents: gold 1 1986 USA American Eagle gold 1 1908 Austria-Hungary Franz Josef 100 Korona silver 10 1981 USA ingot gold 1 1984 Switzerland ingot gold 1 1979 RSA Krugerrand gold 0.5 1981 RSA Krugerrand gold 0.1 1986 PRC Panda silver 1 1986 USA Liberty dollar gold 0.25 1986 USA Liberty 5-dollar piece silver 0.5 1986 USA Liberty 50-cent piece silver 1 1987 USA Constitution dollar gold 0.25 1987 USA Constitution 5-dollar piece gold 1 1988 Canada Maple Leaf The command bellow will search through the file for lines of text that contain the string \"gold\", and print them out. $ awk '/gold/' coins . txt gold 1 1986 USA American Eagle gold 1 1908 Austria - Hungary Franz Josef 100 Korona gold 1 1984 Switzerland ingot gold 1 1979 RSA Krugerrand gold 0.5 1981 RSA Krugerrand gold 0.1 1986 PRC Panda gold 0.25 1986 USA Liberty 5 - dollar piece gold 0.25 1987 USA Constitution 5 - dollar piece gold 1 1988 Canada Maple Leaf /gold/ defines a matching criteria that will be used on every line of the file. If no command is specified, the default action is to print the matche d lines. This mimics the use of grep or sed , so why would one possibly need awk? Now, suppose we want to list all the coins that were minted before 1980 . We invoke Awk as follows: $ awk '$3 < 1980 {print $3, \" \",$5,$6,$7,$8}' coins . txt 1908 Franz Josef 100 Korona 1979 Krugerrand In the example above, we defined a matching criteria $ 3 < 1980 , so the commands enclosed in the { print $ 3 , \" \" , $ 5 , $ 6 , $ 7 , $ 8 } block will be execut ed only when the criteria is met - i.e. awk will print the values of columns 3,\" \",5,6,7, and 8. The columns are separated (defined) by white space (o ne ore more consecutive blanks) and addressed by the $ sign i.e. $ 1 is the value of the first column, $ 2 - second etc. $ 0 contains the whol e line including the white spaces.","title":"Let's begin simple"},{"location":"1.Simple_example/#what-about-some-math-can-i-manipulate-or-analyze-the-data","text":"Let's use the following simple file that contains 3 lines with numbers and some text, just to make our life more difficult (or maybe not?) 1 2 3 4 5 6 7 8 9 10 text Let's try to make some calculations with the data in this file - summation and multiplication in this case. $ awk ' {print $1+$2*$3}' data 7 34 79 Again, we did not provide any matching criteria, so the commands will be executed on every line. Try to run the command. What do you think? Does awk provid e the right answer? I am serious! ;-) Let's do the math only if the value in the first column is greater than 4. $ awk '$1 > 4 {print $1+$2*$3}' data 79 Now, $ 1 > 4 is our criteria on when to execute the command block { print $ 1 +$ 2 *$ 3 } . Notice that the criteria is outside the {} block.","title":"What about some math? Can I manipulate or analyze the data?"},{"location":"1.Simple_example/#awk-command-line-syntax","text":"$ awk ' /pattern/ {action} ' file1 file2 ... fileN action is performed on every line that matches pattern . If pattern is not provided, action is performed on every line. If action is not provided, then all matching lines are simply sent to standard output. Since patterns and actions are optional, actions must be enclosed in braces to distinguish them from pattern. The statements in an awk program may be indented and formatted using spaces, tabs, and new lines. Two special patterns: BEGIN (execute an action before first input line) and END ( ... after all lines are read.)","title":"Awk command-line syntax:"},{"location":"1.Simple_example/#simple-output-examples","text":"{ print } - will print the whole line to standard out { print $ 0 } - will do the same thing { print $ 1 , $ 3 } - expressions separated by a comma are, by default, separated by a single space when output { print NF , $ 1 , $ NF } - will print the number of fields, the first field, and the last field in the current record { print $ ( NF - 2 ) } prints the third to last field","title":"Simple output examples"},{"location":"1.Simple_example/#computing-and-printing","text":"{ print $ 1 , $ 2 * $ 3 } You can also do computations on the field values and include the results in your output { print NR , $ 0 } - will print each line prefixed with its line number","title":"Computing and Printing"},{"location":"1.Simple_example/#putting-text-in-the-output","text":"{ print \"total pay for\" , $ 1 , \"is\" , $ 2 * $ 3 } you can also add other text to the output besides what is in the current record. Note that the inserted text needs to be surrounded by double quotes. { printf ( \"total pay for %s is $%.2f\\n\" , $ 1 , $ 2 * $ 3 ) } when using printf, formatting is under your control, so no automatic spaces or newlines are pr ovided by awk. You have to insert them yourself. { printf ( \"%-8s %6.2f\\n\" , $ 1 , $ 2 * $ 3 ) } - well, this escalated too fast... More on format modifers: gawk documentation Files coins.txt","title":"Putting Text in the Output"},{"location":"2.Teasing_with_grep/","text":"2.Teasing with grep The most common use is using regular expressions. The construction below will search for \"pattern\" in each line. $ awk ' /pattern/ {action} ' file1 file2 ... fileN What about patter matching only particular field? Here is an example of such matching criterium that targets only the first field/column. $ awk ' $1 ~ /pattern/ {action} ' file1 file2 ... fileN The ! will negate the result. $ awk ' ! /pattern/ {action} ' file1 file2 ... fileN One can match exact text of a field. Matching is case sensitive. $ awk ' $1 == \"text\" {action} ' file1 file2 ... fileN Here is an example of of arithmetic comparison for different fields. In this example the script will match lines with value in the first field larger than 10 AND the second smaller that 7. && is the logical AND , || is for OR . $ awk ' $1 > 10 && $2 < 7 {action} ' file1 file2 ... fileN You can compare between different fields as well... $ awk ' ($1+$2 > 10) && ($3 < $4) {action} ' file1 file2 ... fileN There are two reserved keywords: BEGIN and END . Here is how it works. $ awk ' BEGIN {action_B} /pattern/ {action} END {action_E}' file BEGIN block gives you the opportunity to do something before you even start reading the file {action_B} . This is perfect for variable initialization or gathering data from another files that is needed for the current script. Then, awk will run {action} for each line that matches /pattern/ . At the END awk will run {action_E} . Perfect to print the collected data. Here is a summary of the awk workflow logic. $ awk ' BEGIN {action_B} /pattern1/ {action1} /pattern2/ {action2} END {action_E} ' file1 file2 ... fileN And here is the teaser ;-). Perhaps I am not very familiar with grep but here is a simple example that is rather tricky to do with grep. 1 2 4 3 5 6 5 3 5 6 8 2 4 3 1 5 7 8 2 5 6 1 9 0 1 4 5 6 7 8 Remove all lines that contain \"2\" and \"3\" in any order. These lines are 1 and 2. Possible way to do it: $ grep -v \"2.*3\\|3.*2\" numbers.dat which will grep for the two possible combinations when \"2\" is before \"3\" and the opposite case. This works but already presents us with the problem. What happens if we search for more than two matches? One neds to permutate all the possible configurations... Here is how can be done with awk. $ awk '!(/2/ && /3/) {print $0}' numbers . dat ! ( /2/ && /3/ ) let's decrypt the matching rule. && stands for \"and\" ! negates the logical result ( /2/ && /3/ ) matches any line that contains 2 and 3, then ! negates the result- line that does not contain them - if so, print them. Awk has a bit more elaborate matching capabilities, not mentioning the ability of arithmetical operations in the matching criteria expressions.","title":"2.Teasing with grep"},{"location":"2.Teasing_with_grep/#2teasing-with-grep","text":"The most common use is using regular expressions. The construction below will search for \"pattern\" in each line. $ awk ' /pattern/ {action} ' file1 file2 ... fileN What about patter matching only particular field? Here is an example of such matching criterium that targets only the first field/column. $ awk ' $1 ~ /pattern/ {action} ' file1 file2 ... fileN The ! will negate the result. $ awk ' ! /pattern/ {action} ' file1 file2 ... fileN One can match exact text of a field. Matching is case sensitive. $ awk ' $1 == \"text\" {action} ' file1 file2 ... fileN Here is an example of of arithmetic comparison for different fields. In this example the script will match lines with value in the first field larger than 10 AND the second smaller that 7. && is the logical AND , || is for OR . $ awk ' $1 > 10 && $2 < 7 {action} ' file1 file2 ... fileN You can compare between different fields as well... $ awk ' ($1+$2 > 10) && ($3 < $4) {action} ' file1 file2 ... fileN There are two reserved keywords: BEGIN and END . Here is how it works. $ awk ' BEGIN {action_B} /pattern/ {action} END {action_E}' file BEGIN block gives you the opportunity to do something before you even start reading the file {action_B} . This is perfect for variable initialization or gathering data from another files that is needed for the current script. Then, awk will run {action} for each line that matches /pattern/ . At the END awk will run {action_E} . Perfect to print the collected data. Here is a summary of the awk workflow logic. $ awk ' BEGIN {action_B} /pattern1/ {action1} /pattern2/ {action2} END {action_E} ' file1 file2 ... fileN And here is the teaser ;-). Perhaps I am not very familiar with grep but here is a simple example that is rather tricky to do with grep. 1 2 4 3 5 6 5 3 5 6 8 2 4 3 1 5 7 8 2 5 6 1 9 0 1 4 5 6 7 8 Remove all lines that contain \"2\" and \"3\" in any order. These lines are 1 and 2. Possible way to do it: $ grep -v \"2.*3\\|3.*2\" numbers.dat which will grep for the two possible combinations when \"2\" is before \"3\" and the opposite case. This works but already presents us with the problem. What happens if we search for more than two matches? One neds to permutate all the possible configurations... Here is how can be done with awk. $ awk '!(/2/ && /3/) {print $0}' numbers . dat ! ( /2/ && /3/ ) let's decrypt the matching rule. && stands for \"and\" ! negates the logical result ( /2/ && /3/ ) matches any line that contains 2 and 3, then ! negates the result- line that does not contain them - if so, print them. Awk has a bit more elaborate matching capabilities, not mentioning the ability of arithmetical operations in the matching criteria expressions.","title":"2.Teasing with grep"},{"location":"3.Shell_we_awk/","text":"3.Shell we awk! Q: OK, I see this and that but really, why should I bother with awk? Indeed, why? Lets get back to our very first example with the coins. gold 1 1986 USA American Eagle gold 1 1908 Austria-Hungary Franz Josef 100 Korona silver 10 1981 USA ingot gold 1 1984 Switzerland ingot gold 1 1979 RSA Krugerrand gold 0.5 1981 RSA Krugerrand gold 0.1 1986 PRC Panda silver 1 1986 USA Liberty dollar gold 0.25 1986 USA Liberty 5-dollar piece silver 0.5 1986 USA Liberty 50-cent piece silver 1 1987 USA Constitution dollar gold 0.25 1987 USA Constitution 5-dollar piece gold 1 1988 Canada Maple Leaf Let's, just for a moment, assume that the file contains thousands of lines and your task is to summarize how many coins from each country you have in the file. You can do it in some spreadsheet software, you can write a program in C, C++, Fortran, Python, Perl - I know. You can use a helicopter to travel pretty much everywhere, but are you going to use it for a visit to the nearby grocery store? So, perhaps grep + wc is a good start. $ grep USA coins.txt | wc -l This will give you the number of coins (lines) that have USA in the description. Then you go over all possible countries that are in the file but how many? Do you create a list of all countries and then run a loop over it? Why not give it to awk and use some associative arrays for this purpose? #!/usr/bin/awk -f { country [ $ 4 ] ++ } END { for ( i in country ) print \"Country: \" i , \" count: \" , country [ i ] } This is written in a script file, but if you insist here is the one line equivalent. $ awk '{country[$4]++} END {for (i in country) print \"Country: \" i,\" count: \", country[i]}' coins . txt Notice that we neither defined the names of the countries nor their number in advance... The only restriction is that the name of the country should be a single word. So, on every line { country [ $ 4 ] ++ } will address the element with the value of the 4 th column in the array country and will be increased by one. No need to declare the array in advance, no need to pre-allocate the size or to dynamically allocate the array... Everything is done behind the scene for your convenience... and the price to pay is... speed. In this particular case (with the thousands line file) the speed is not a problem. Try it on the small file. Can you change the program so that it will count how many gold and silver coins there are? Well, coins, countries - anything else? Hm, if you work regularly with unformatted text files, like output from some programs, where the numbers are mixed with a complimentary text - then keep reading... Let's use the output from the Gaussian code as an example ( something from my research ). . . . ( more than 1 000 lines ) . . . --------------------------------------------------------------------- Distance matrix (angstroms): 1 2 3 4 5 1 O 0.000000 2 H 0.992913 0.000000 3 H 3.850087 3.157733 0.000000 4 H 4.103799 3.210799 1.406026 0.000000 5 O 4.087160 3.279779 0.698261 1.006866 0.000000 6 H 7.101652 6.223004 3.619741 3.012387 3.244764 7 H 7.979064 7.127506 4.766153 4.025437 4.489933 8 O 7.015923 6.163580 3.781794 3.039570 3.500909 9 H 6.870878 5.949961 3.670757 3.155240 3.066537 . . . ( more than 1 000 lines ) . . . Hexadecapole moment (field-independent basis, Debye-Ang**3): XXXX= -1559.9870 YYYY= -328.4544 ZZZZ= -2862.8389 XXXY= 1310.6681 XXXZ= 1081.8065 YYYX= 164.5078 YYYZ= -151.4171 ZZZX= 1196.1568 ZZZY= -496.7865 XXYY= -1121.6127 XXZZ= -729.7324 YYZZ= -643.6533 XXYZ= -752.0797 YYXZ= 423.6499 ZZXY= 467.3516 N-N= 2.958297091443D+02 E-N=-1.886329223184D+03 KE= 5.289517204090D+02 Counterpoise: corrected energy = -624.403363077798 Counterpoise: BSSE energy = 0.003673904967 Test job not archived. 1\\1\\NSC-NEOLITH\\SP\\RB3LYP\\6-31++G(d,p)\\H15O8(1-)\\X_PAVMI\\23-Aug-2012\\0 \\\\#B3LYP/6-31++G(d,p) Counterpoise=2 Charge SCF=Tight NoSymm\\\\OH- in H 2O,std. LJ params. for H2O, MC/QM\\\\-1,1\\O,0,0.,0.,0.\\H,0,-0.797387,-0. . . . ( more lines ) . . . The numbers that I am interested in are in bold. There are 71 such pairs in the whole file. I need them tabulated in simple, two-column file that is easy to read, analyze and plot. Here I will not discuss other solutions. Instead, here is a possible awk solution: #!/usr/bin/awk -f BEGIN { AU2eV = 27.211383 } /Distance matrix/ { getline ; getline ; getline ; getline ; getline ; getline ; rOH = $ 5 } /Counterpoise: corrected energy/ { printf \"%.12f %.6f\\n\" , rOH , $ 5 * AU2eV } Here is the result: 0.698261000000 -16990.879059 0.713262000000 -16991.433745 0.728261000000 -16991.924474 0.743262000000 -16992.357423 0.758261000000 -16992.738040 0.773261000000 -16993.071353 0.788261000000 -16993.361823 0.803261000000 -16993.613474 0.818261000000 -16993.830007 0.833261000000 -16994.014768 0.848261000000 -16994.170777 0.863261000000 -16994.300772 0.878261000000 -16994.407219 0.893261000000 -16994.492381 0.908261000000 -16994.558302 0.923262000000 -16994.606840 0.938262000000 -16994.639689 0.953261000000 -16994.658379 0.968261000000 -16994.664300 0.983261000000 -16994.658709 0.998261000000 -16994.642752 1.013261000000 -16994.617475 1.028261000000 -16994.583831 1.043261000000 -16994.542689 . . . BEGIN { AU2eV = 27.211383 } simply defines a conversion factor from atomic units to eV. /Distance matrix/ { getline ; getline ; getline ; getline ; getline ; getline ; rOH = $ 5 } will match lines with the specified criteria, skip 6 lines, then accumulate the 5 th column in variable rOH - the distance between an O and an H in the molecule of interest. /Counterpoise: corrected energy/ { printf \"%.12f %.6f\\n\" , rOH , $ 5 * AU2eV } will match the other interesting line, then prints the collected earlier rOH and corresponding energy converted in eV. Reading the resulting file is trivial in any programming language or plotting program. Look at this alternative solution that uses a smart trick to advance 6 lines after the /Distance matrix/ line credits to Pavol Bauer, workshop 2017.01.13 #!/usr/bin/awk -f BEGIN { AU2eV = 27.211383 } /Distance matrix/ { myNR = NR } NR == myNR + 6 { rOH =$ 5 } /Counterpoise: corrected energy/ { printf \"%.12f %.6f\\n\" , rOH , $ 5 * AU2eV } Other case studies that will be gradually collected for further inspiration. Files gaussian.out","title":"3.Shell we awk"},{"location":"3.Shell_we_awk/#3shell-we-awk","text":"","title":"3.Shell we awk!"},{"location":"3.Shell_we_awk/#q-ok-i-see-this-and-that-but-really-why-should-i-bother-with-awk","text":"Indeed, why? Lets get back to our very first example with the coins. gold 1 1986 USA American Eagle gold 1 1908 Austria-Hungary Franz Josef 100 Korona silver 10 1981 USA ingot gold 1 1984 Switzerland ingot gold 1 1979 RSA Krugerrand gold 0.5 1981 RSA Krugerrand gold 0.1 1986 PRC Panda silver 1 1986 USA Liberty dollar gold 0.25 1986 USA Liberty 5-dollar piece silver 0.5 1986 USA Liberty 50-cent piece silver 1 1987 USA Constitution dollar gold 0.25 1987 USA Constitution 5-dollar piece gold 1 1988 Canada Maple Leaf Let's, just for a moment, assume that the file contains thousands of lines and your task is to summarize how many coins from each country you have in the file. You can do it in some spreadsheet software, you can write a program in C, C++, Fortran, Python, Perl - I know. You can use a helicopter to travel pretty much everywhere, but are you going to use it for a visit to the nearby grocery store? So, perhaps grep + wc is a good start. $ grep USA coins.txt | wc -l This will give you the number of coins (lines) that have USA in the description. Then you go over all possible countries that are in the file but how many? Do you create a list of all countries and then run a loop over it? Why not give it to awk and use some associative arrays for this purpose? #!/usr/bin/awk -f { country [ $ 4 ] ++ } END { for ( i in country ) print \"Country: \" i , \" count: \" , country [ i ] } This is written in a script file, but if you insist here is the one line equivalent. $ awk '{country[$4]++} END {for (i in country) print \"Country: \" i,\" count: \", country[i]}' coins . txt Notice that we neither defined the names of the countries nor their number in advance... The only restriction is that the name of the country should be a single word. So, on every line { country [ $ 4 ] ++ } will address the element with the value of the 4 th column in the array country and will be increased by one. No need to declare the array in advance, no need to pre-allocate the size or to dynamically allocate the array... Everything is done behind the scene for your convenience... and the price to pay is... speed. In this particular case (with the thousands line file) the speed is not a problem. Try it on the small file. Can you change the program so that it will count how many gold and silver coins there are?","title":"Q: OK, I see this and that but really, why should I bother with awk?"},{"location":"3.Shell_we_awk/#well-coins-countries-anything-else","text":"Hm, if you work regularly with unformatted text files, like output from some programs, where the numbers are mixed with a complimentary text - then keep reading... Let's use the output from the Gaussian code as an example ( something from my research ). . . . ( more than 1 000 lines ) . . . --------------------------------------------------------------------- Distance matrix (angstroms): 1 2 3 4 5 1 O 0.000000 2 H 0.992913 0.000000 3 H 3.850087 3.157733 0.000000 4 H 4.103799 3.210799 1.406026 0.000000 5 O 4.087160 3.279779 0.698261 1.006866 0.000000 6 H 7.101652 6.223004 3.619741 3.012387 3.244764 7 H 7.979064 7.127506 4.766153 4.025437 4.489933 8 O 7.015923 6.163580 3.781794 3.039570 3.500909 9 H 6.870878 5.949961 3.670757 3.155240 3.066537 . . . ( more than 1 000 lines ) . . . Hexadecapole moment (field-independent basis, Debye-Ang**3): XXXX= -1559.9870 YYYY= -328.4544 ZZZZ= -2862.8389 XXXY= 1310.6681 XXXZ= 1081.8065 YYYX= 164.5078 YYYZ= -151.4171 ZZZX= 1196.1568 ZZZY= -496.7865 XXYY= -1121.6127 XXZZ= -729.7324 YYZZ= -643.6533 XXYZ= -752.0797 YYXZ= 423.6499 ZZXY= 467.3516 N-N= 2.958297091443D+02 E-N=-1.886329223184D+03 KE= 5.289517204090D+02 Counterpoise: corrected energy = -624.403363077798 Counterpoise: BSSE energy = 0.003673904967 Test job not archived. 1\\1\\NSC-NEOLITH\\SP\\RB3LYP\\6-31++G(d,p)\\H15O8(1-)\\X_PAVMI\\23-Aug-2012\\0 \\\\#B3LYP/6-31++G(d,p) Counterpoise=2 Charge SCF=Tight NoSymm\\\\OH- in H 2O,std. LJ params. for H2O, MC/QM\\\\-1,1\\O,0,0.,0.,0.\\H,0,-0.797387,-0. . . . ( more lines ) . . . The numbers that I am interested in are in bold. There are 71 such pairs in the whole file. I need them tabulated in simple, two-column file that is easy to read, analyze and plot. Here I will not discuss other solutions. Instead, here is a possible awk solution: #!/usr/bin/awk -f BEGIN { AU2eV = 27.211383 } /Distance matrix/ { getline ; getline ; getline ; getline ; getline ; getline ; rOH = $ 5 } /Counterpoise: corrected energy/ { printf \"%.12f %.6f\\n\" , rOH , $ 5 * AU2eV } Here is the result: 0.698261000000 -16990.879059 0.713262000000 -16991.433745 0.728261000000 -16991.924474 0.743262000000 -16992.357423 0.758261000000 -16992.738040 0.773261000000 -16993.071353 0.788261000000 -16993.361823 0.803261000000 -16993.613474 0.818261000000 -16993.830007 0.833261000000 -16994.014768 0.848261000000 -16994.170777 0.863261000000 -16994.300772 0.878261000000 -16994.407219 0.893261000000 -16994.492381 0.908261000000 -16994.558302 0.923262000000 -16994.606840 0.938262000000 -16994.639689 0.953261000000 -16994.658379 0.968261000000 -16994.664300 0.983261000000 -16994.658709 0.998261000000 -16994.642752 1.013261000000 -16994.617475 1.028261000000 -16994.583831 1.043261000000 -16994.542689 . . . BEGIN { AU2eV = 27.211383 } simply defines a conversion factor from atomic units to eV. /Distance matrix/ { getline ; getline ; getline ; getline ; getline ; getline ; rOH = $ 5 } will match lines with the specified criteria, skip 6 lines, then accumulate the 5 th column in variable rOH - the distance between an O and an H in the molecule of interest. /Counterpoise: corrected energy/ { printf \"%.12f %.6f\\n\" , rOH , $ 5 * AU2eV } will match the other interesting line, then prints the collected earlier rOH and corresponding energy converted in eV. Reading the resulting file is trivial in any programming language or plotting program. Look at this alternative solution that uses a smart trick to advance 6 lines after the /Distance matrix/ line credits to Pavol Bauer, workshop 2017.01.13 #!/usr/bin/awk -f BEGIN { AU2eV = 27.211383 } /Distance matrix/ { myNR = NR } NR == myNR + 6 { rOH =$ 5 } /Counterpoise: corrected energy/ { printf \"%.12f %.6f\\n\" , rOH , $ 5 * AU2eV } Other case studies that will be gradually collected for further inspiration. Files gaussian.out","title":"Well, coins, countries - anything else?"},{"location":"4.Brief_commands/","text":"4.Brief commands overview The language looks a little like C but automatically handles input, field splitting, initialization, and memory management. Built-in string and number data types No variable type declarations data [ 1 ] = 330 data [ 3 ] = \"text\" # yes, the elements could be of different type data [ 7 ] = 7.5 day [ \"Monday\" ] = 1 awk is a great prototyping language start with a few lines and keep adding until it does what you want awk gets its input from files redirection and pipes directly from standard input Common commands and constructions - examples Loop for ( i = 1 ; i <= 10 ; i ++ ) { print i } If else statement if ( $ 1 > 2 ) { print $ 2 } else if ( $ 1 < 2 ) { print $ 3 } else { print $ 3 } While statement while ( $ 1 < $ 3 ) { getline ; print } Other print \"Text\" $ 1 \"more text\" printf ( \"Text %g more text\" , $ 1 ) next # (skips the remaining patterns on the current line of input) exit # (skips the rest of the current line) Predefined variables NR - Number of records processed NF - Number of fields in current record FILENAME - name of current input file FS - Field separator, space or TAB by default OFS - Output field separator, space by default ARGC/ARGV - Argument Count, Argument Value array - get arguments from the command line $1 - first field value, $2 - second etc. $0 - contains the entire line Built-in functions Arithmetic sin(), cos(), atan(), exp(), int(), log(), rand(), sqrt() String manipulation functions length(), substitution, find substrings, split strings Output print, printf(), print and printf to file Special system() - executes a Unix command e.g., system(\"date\") to execute \"date\" command. Note double quotes around the Unix command If you have reached this point, perhaps you are already asking where you can read something to begin with. This site is perhaps good for beginners: https://en.wikibooks.org/wiki/AWK","title":"4.Brief commands"},{"location":"4.Brief_commands/#4brief-commands-overview","text":"The language looks a little like C but automatically handles input, field splitting, initialization, and memory management. Built-in string and number data types No variable type declarations data [ 1 ] = 330 data [ 3 ] = \"text\" # yes, the elements could be of different type data [ 7 ] = 7.5 day [ \"Monday\" ] = 1 awk is a great prototyping language start with a few lines and keep adding until it does what you want awk gets its input from files redirection and pipes directly from standard input Common commands and constructions - examples Loop for ( i = 1 ; i <= 10 ; i ++ ) { print i } If else statement if ( $ 1 > 2 ) { print $ 2 } else if ( $ 1 < 2 ) { print $ 3 } else { print $ 3 } While statement while ( $ 1 < $ 3 ) { getline ; print } Other print \"Text\" $ 1 \"more text\" printf ( \"Text %g more text\" , $ 1 ) next # (skips the remaining patterns on the current line of input) exit # (skips the rest of the current line)","title":"4.Brief commands overview"},{"location":"4.Brief_commands/#predefined-variables","text":"NR - Number of records processed NF - Number of fields in current record FILENAME - name of current input file FS - Field separator, space or TAB by default OFS - Output field separator, space by default ARGC/ARGV - Argument Count, Argument Value array - get arguments from the command line $1 - first field value, $2 - second etc. $0 - contains the entire line","title":"Predefined variables"},{"location":"4.Brief_commands/#built-in-functions","text":"","title":"Built-in functions"},{"location":"4.Brief_commands/#arithmetic","text":"sin(), cos(), atan(), exp(), int(), log(), rand(), sqrt()","title":"Arithmetic"},{"location":"4.Brief_commands/#string-manipulation-functions","text":"length(), substitution, find substrings, split strings","title":"String manipulation functions"},{"location":"4.Brief_commands/#output","text":"print, printf(), print and printf to file","title":"Output"},{"location":"4.Brief_commands/#special","text":"system() - executes a Unix command e.g., system(\"date\") to execute \"date\" command. Note double quotes around the Unix command If you have reached this point, perhaps you are already asking where you can read something to begin with. This site is perhaps good for beginners: https://en.wikibooks.org/wiki/AWK","title":"Special"},{"location":"5.String_manipulation/","text":"5.String manipulation This is not exactly Awk specific, but it is helpful to know how one can manipulate string fields. Quite often, you have files with such names, that are alphabetically ordered and it is a bit tricky to put them in order again. Lets assume we have the following data in a file. The lines contain filenames that have common pattern and number that we are interested. Let's \"extract\" the alternating number i.e 1, 10, 123, ... names.dat H2O-1_1.res H2O-1_10.res H2O-1_123.res H2O-1_21.res H2O-1_44.res H2O-1_5.res H2O-1_7.res One possible way , is by removing the pattern that remains unchanged. (BASH can do it as well, way easier with the file names than the lines in a file) $ awk '{ gsub( \"H2O-1_|.res\", \"\" , $1); print $1 }' names . dat This is my preferred way, since it is rather easy to understand and modify. Another way , could be by using backreferences the way sed will do it. $ awk '{ print gensub(/H2O-1_(.*).res/,\"\\\\1\",\"g\")}' names . dat How about we fix the annoyance with the alphabetic sort by replacing 1 with 001 , 5 with 005 , 10 -> 010 , etc. $ awk '{ gsub(\"H2O-1_|.res\",\"\",$1); printf(\"H2O-1_%03g.res\\n\", $1) }' names . dat H2O-1_001.res H2O-1_010.res H2O-1_123.res H2O-1_021.res H2O-1_044.res H2O-1_005.res H2O-1_007.res Now one can pipe to sort and get them sorted. You can sort them in awk as well but the OVERHEAD does not worth the effort. Have a look on other string manipulation functions in awk. printf function is pretty much the same as in many other languages.","title":"5.String manipulation"},{"location":"5.String_manipulation/#5string-manipulation","text":"This is not exactly Awk specific, but it is helpful to know how one can manipulate string fields. Quite often, you have files with such names, that are alphabetically ordered and it is a bit tricky to put them in order again. Lets assume we have the following data in a file. The lines contain filenames that have common pattern and number that we are interested. Let's \"extract\" the alternating number i.e 1, 10, 123, ... names.dat H2O-1_1.res H2O-1_10.res H2O-1_123.res H2O-1_21.res H2O-1_44.res H2O-1_5.res H2O-1_7.res One possible way , is by removing the pattern that remains unchanged. (BASH can do it as well, way easier with the file names than the lines in a file) $ awk '{ gsub( \"H2O-1_|.res\", \"\" , $1); print $1 }' names . dat This is my preferred way, since it is rather easy to understand and modify. Another way , could be by using backreferences the way sed will do it. $ awk '{ print gensub(/H2O-1_(.*).res/,\"\\\\1\",\"g\")}' names . dat How about we fix the annoyance with the alphabetic sort by replacing 1 with 001 , 5 with 005 , 10 -> 010 , etc. $ awk '{ gsub(\"H2O-1_|.res\",\"\",$1); printf(\"H2O-1_%03g.res\\n\", $1) }' names . dat H2O-1_001.res H2O-1_010.res H2O-1_123.res H2O-1_021.res H2O-1_044.res H2O-1_005.res H2O-1_007.res Now one can pipe to sort and get them sorted. You can sort them in awk as well but the OVERHEAD does not worth the effort. Have a look on other string manipulation functions in awk. printf function is pretty much the same as in many other languages.","title":"5.String manipulation"},{"location":"6.One_line_programs/","text":"6.One line programs Many useful awk programs are as short as just a line or two. Here is a collection of useful, short programs to get you started. Some of these programs contain constructs that haven't been covered yet. The description of the program will give you a good idea of what is going on. Here I realized that it is better to post some links and just mention some of my favorites, perhaps. The best AWK one-liners AWK one-liners by Softpanorama awk one-liners by *nix shell","title":"6.One line programs"},{"location":"6.One_line_programs/#6one-line-programs","text":"Many useful awk programs are as short as just a line or two. Here is a collection of useful, short programs to get you started. Some of these programs contain constructs that haven't been covered yet. The description of the program will give you a good idea of what is going on. Here I realized that it is better to post some links and just mention some of my favorites, perhaps. The best AWK one-liners AWK one-liners by Softpanorama awk one-liners by *nix shell","title":"6.One line programs"},{"location":"About/","text":"About A few lines about me: My name is Pavlin Mitev and I work as a researcher at the Department of Chemistry, Uppsala University within the area of computational Chemistry. I use commercial and open source software in my work, on a daily basis. Quite often I need to program, but a large number of the problems I need to solve include preparation of inputs ( in some cases 1100 different inputs ), collection of results from different programs, analysis or modification of the data before further use etc.. Most of the time my data is buried in text readable outputs containing many different parameters, which makes the direct access to the numbers a bit problematic. All programs, more or less, use different formats for their inputs and in some cases there are available suites that are very helpful for this transitions. Unfortunately, quite often they just can't do what you want or it is not yet implemented, or ... Here is how I found myself using awk. Initially, as a middle-ware between programs, then gradually using more of the features that come with the language. Then, I found it so addictive that most of my small tools are written in awk, despite the fact that sometimes there are better alternative solutions. I even use awk to write Python code, only because I find it cumbersome to use Python to collect my data (ex: Python vs. awk ). The purpose of the page is to illustrate some particularly strong sides of the language for text parsing, simple data collection, analysis, and much more. It is inspired by multiple simple solutions, often applied in mine and my colleagues' research work. This side also serves as an auxiliary material to an awk course/seminar given by UPPMAX , Uppsala University. pavlin.mitev @ uppmax.uu.se Seminars are organized on a demand-base and currently follows the schedule for the \"Introductory Linux course\" at UPPMAX. Please, visit the UPPMAX course/seminar page for detailed information - venue, schedule, registration etc.","title":"About"},{"location":"About/#about","text":"","title":"About"},{"location":"About/#a-few-lines-about-me","text":"My name is Pavlin Mitev and I work as a researcher at the Department of Chemistry, Uppsala University within the area of computational Chemistry. I use commercial and open source software in my work, on a daily basis. Quite often I need to program, but a large number of the problems I need to solve include preparation of inputs ( in some cases 1100 different inputs ), collection of results from different programs, analysis or modification of the data before further use etc.. Most of the time my data is buried in text readable outputs containing many different parameters, which makes the direct access to the numbers a bit problematic. All programs, more or less, use different formats for their inputs and in some cases there are available suites that are very helpful for this transitions. Unfortunately, quite often they just can't do what you want or it is not yet implemented, or ... Here is how I found myself using awk. Initially, as a middle-ware between programs, then gradually using more of the features that come with the language. Then, I found it so addictive that most of my small tools are written in awk, despite the fact that sometimes there are better alternative solutions. I even use awk to write Python code, only because I find it cumbersome to use Python to collect my data (ex: Python vs. awk ). The purpose of the page is to illustrate some particularly strong sides of the language for text parsing, simple data collection, analysis, and much more. It is inspired by multiple simple solutions, often applied in mine and my colleagues' research work. This side also serves as an auxiliary material to an awk course/seminar given by UPPMAX , Uppsala University. pavlin.mitev @ uppmax.uu.se Seminars are organized on a demand-base and currently follows the schedule for the \"Introductory Linux course\" at UPPMAX. Please, visit the UPPMAX course/seminar page for detailed information - venue, schedule, registration etc.","title":"A few lines about me:"},{"location":"Links/","text":"Useful places to start learning awk 30 Examples For Awk Command In Text Processing https://en.wikibooks.org/wiki/AWK http://www.grymoire.com/Unix/Awk.html Basic UNIX Commands - awk on YouTube awk Programming in Unix on YouTube Steve's Awk Academy - awk crash course Awk by example - IBM developerWorks Awk for bioinformatics Essential AWK Commands for Next Generation Sequence Analysis Linux and Bioinformatics AWK GTF! How to Analyze a Transcriptome Like a Pro Other links Execute awk online","title":"Links"},{"location":"Links/#useful-places-to-start-learning-awk","text":"30 Examples For Awk Command In Text Processing https://en.wikibooks.org/wiki/AWK http://www.grymoire.com/Unix/Awk.html Basic UNIX Commands - awk on YouTube awk Programming in Unix on YouTube Steve's Awk Academy - awk crash course Awk by example - IBM developerWorks","title":"Useful places to start learning awk"},{"location":"Links/#awk-for-bioinformatics","text":"Essential AWK Commands for Next Generation Sequence Analysis Linux and Bioinformatics AWK GTF! How to Analyze a Transcriptome Like a Pro","title":"Awk for bioinformatics"},{"location":"Links/#other-links","text":"Execute awk online","title":"Other links"},{"location":"Python_vs_awk/","text":"Python vs. awk Well, the title is misleading. I am not going to argue which language is better. Obviously, Python is more powerful and has vastly more features. Instead, here I have selected a particular case where ( I think ) the use of Python is not a good choice at all. Some time ago, I needed to extract frequency data from a Gaussian calculation. As many of us do, I searched the Net for already existing solutions, after all I do not want to rediscover the wheel... Luckily, the very first hit was a very well written Python script that does the job. Here is the link to the original page with the solution. I have to say that ~50 lines of code were a bit too much for me and for this particular task. Here I will paste the code just to support my case. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 #!/usr/bin/python # Compatible with python 2.7 # Reads frequency output from a g09 (gaussian) calculation # Usage ex.: g09freq g09.log ir.dat import sys def ints2float ( integerlist ): for n in range ( 0 , len ( integerlist )): integerlist [ n ] = float ( integerlist [ n ]) return integerlist def parse_in ( infile ): g09output = open ( infile , 'r' ) captured = [] for line in g09output : if ( 'Frequencies' in line ) or ( 'Frc consts' in line ) or ( 'IR Inten' in line ): captured += [ line . strip ( ' \\n ' )] g09output . close () return captured def format_captured ( captured ): vibmatrix = [] steps = len ( captured ) for n in range ( 0 , steps , 3 ): freqs = ints2float ( filter ( None , captured [ n ] . split ( ' ' ))[ 2 : 5 ]) forces = ints2float ( filter ( None , captured [ n + 1 ] . split ( ' ' ))[ 3 : 6 ]) intensities = ints2float ( filter ( None , captured [ n + 2 ] . split ( ' ' ))[ 3 : 6 ]) for m in range ( 0 , 3 ): vibmatrix += [[ freqs [ m ], forces [ m ], intensities [ m ]]] return vibmatrix def write_matrix ( vibmatrix , outfile ): f = open ( outfile , 'w' ) for n in range ( 0 , len ( vibmatrix )): item = vibmatrix [ n ] f . write ( str ( item [ 0 ]) + ' \\t ' + str ( item [ 1 ]) + ' \\t ' + str ( item [ 2 ]) + ' \\n ' ) f . close () return 0 if __name__ == \"__main__\" : infile = sys . argv [ 1 ] outfile = sys . argv [ 2 ] captured = parse_in ( infile ) if len ( captured ) % 3 == 0 : vibmatrix = format_captured ( captured ) else : print 'Number of elements not divisible by 3 (freq+force+intens=3)' exit () success = write_matrix ( vibmatrix , outfile ) if success == 0 : print 'Read %s , parsed it, and wrote %s ' % ( infile , outfile ) A large amount of the code deals with declarations of functions, reading and finding the data. Here is how this problem could be solved with awk: #!/bin/awk -f /Frequencies/ { for ( i = 3 ; i <= NF ; i ++ ) { im ++ ; freq [ im ] =$ i } } /Frc consts/ { for ( i = NF ; i >= 4 ; i -- ) fc [ im - ( NF - i )] =$ i } /IR Inten/ { for ( i = NF ; i >= 4 ; i -- ) ir [ im - ( NF - i )] =$ i } END { for ( i = 1 ; i <= im ; i ++ ) print freq [ i ], fc [ i ], ir [ i ] } Somehow, I think this is much more readable and easier to modify...","title":"Python vs. awk"},{"location":"Python_vs_awk/#python-vs-awk","text":"Well, the title is misleading. I am not going to argue which language is better. Obviously, Python is more powerful and has vastly more features. Instead, here I have selected a particular case where ( I think ) the use of Python is not a good choice at all. Some time ago, I needed to extract frequency data from a Gaussian calculation. As many of us do, I searched the Net for already existing solutions, after all I do not want to rediscover the wheel... Luckily, the very first hit was a very well written Python script that does the job. Here is the link to the original page with the solution. I have to say that ~50 lines of code were a bit too much for me and for this particular task. Here I will paste the code just to support my case. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 #!/usr/bin/python # Compatible with python 2.7 # Reads frequency output from a g09 (gaussian) calculation # Usage ex.: g09freq g09.log ir.dat import sys def ints2float ( integerlist ): for n in range ( 0 , len ( integerlist )): integerlist [ n ] = float ( integerlist [ n ]) return integerlist def parse_in ( infile ): g09output = open ( infile , 'r' ) captured = [] for line in g09output : if ( 'Frequencies' in line ) or ( 'Frc consts' in line ) or ( 'IR Inten' in line ): captured += [ line . strip ( ' \\n ' )] g09output . close () return captured def format_captured ( captured ): vibmatrix = [] steps = len ( captured ) for n in range ( 0 , steps , 3 ): freqs = ints2float ( filter ( None , captured [ n ] . split ( ' ' ))[ 2 : 5 ]) forces = ints2float ( filter ( None , captured [ n + 1 ] . split ( ' ' ))[ 3 : 6 ]) intensities = ints2float ( filter ( None , captured [ n + 2 ] . split ( ' ' ))[ 3 : 6 ]) for m in range ( 0 , 3 ): vibmatrix += [[ freqs [ m ], forces [ m ], intensities [ m ]]] return vibmatrix def write_matrix ( vibmatrix , outfile ): f = open ( outfile , 'w' ) for n in range ( 0 , len ( vibmatrix )): item = vibmatrix [ n ] f . write ( str ( item [ 0 ]) + ' \\t ' + str ( item [ 1 ]) + ' \\t ' + str ( item [ 2 ]) + ' \\n ' ) f . close () return 0 if __name__ == \"__main__\" : infile = sys . argv [ 1 ] outfile = sys . argv [ 2 ] captured = parse_in ( infile ) if len ( captured ) % 3 == 0 : vibmatrix = format_captured ( captured ) else : print 'Number of elements not divisible by 3 (freq+force+intens=3)' exit () success = write_matrix ( vibmatrix , outfile ) if success == 0 : print 'Read %s , parsed it, and wrote %s ' % ( infile , outfile ) A large amount of the code deals with declarations of functions, reading and finding the data. Here is how this problem could be solved with awk: #!/bin/awk -f /Frequencies/ { for ( i = 3 ; i <= NF ; i ++ ) { im ++ ; freq [ im ] =$ i } } /Frc consts/ { for ( i = NF ; i >= 4 ; i -- ) fc [ im - ( NF - i )] =$ i } /IR Inten/ { for ( i = NF ; i >= 4 ; i -- ) ir [ im - ( NF - i )] =$ i } END { for ( i = 1 ; i <= im ; i ++ ) print freq [ i ], fc [ i ], ir [ i ] } Somehow, I think this is much more readable and easier to modify...","title":"Python vs. awk"},{"location":"arrays/","text":"Awk arrays If you want to learn or just check what other \"tricks\" one could do with arrays in Awk, here a suggested tutorial on the topic - look for \"AWK tips and tricks\" section on the page. Quote AWK arrays appear only rarely online in tutorials, and I found myself guilty of using arrays on my data blog ( https://www.datafix.com.au/BASHing/ ) as though I was assuming that every reader was comfortable with them. This is not true, of course, so I wrote a 4-part series on the blog explaining how they work and how you can use them (see the \"AWK tips and tricks\" section of the Bashing data index page). One reader wrote me to say the articles had clarified for him how arrays work and he now felt more confident - always good feedback! Please feel free to link to those articles on your AWK site, or use them as models for your own tutorial work. Dr Robert Mesibov West Ulverstone, Tasmania, Australia 7315","title":"Awk arrays"},{"location":"arrays/#awk-arrays","text":"If you want to learn or just check what other \"tricks\" one could do with arrays in Awk, here a suggested tutorial on the topic - look for \"AWK tips and tricks\" section on the page. Quote AWK arrays appear only rarely online in tutorials, and I found myself guilty of using arrays on my data blog ( https://www.datafix.com.au/BASHing/ ) as though I was assuming that every reader was comfortable with them. This is not true, of course, so I wrote a 4-part series on the blog explaining how they work and how you can use them (see the \"AWK tips and tricks\" section of the Bashing data index page). One reader wrote me to say the articles had clarified for him how arrays work and he now felt more confident - always good feedback! Please feel free to link to those articles on your AWK site, or use them as models for your own tutorial work. Dr Robert Mesibov West Ulverstone, Tasmania, Australia 7315","title":"Awk arrays"},{"location":"awk_bash/","text":"Awk or bash The short answer - keep using bash but have a look at of some awk neat features that might come in handy. cmd = \"ls -lrt\" - lets have the command stored for convenience system ( cmd ) - will make a system call and execute the command, while sending the output to standard output cmd | getline - will do the same but you will read the first line from the output. # then let's read all lines and do something while ( cmd | getline ) { if ( $ 5 > 200 ) system ( \"cp \" $ 9 \" target_folder/\" ) } Essentially, one gets an extremely easy way to communicate between programs and execute system calls in addition to convenient arithmetic functions (for example, bash does not like fractional numbers ) and you add a lot of flexibility to your decision making script... Have a look at the script that I wrote some 100 years ago that prepares a Gnuplot input to fit Birch\u2013Murnaghan equation of state , then gets the results from the fit ( without using temporary files ) and prettifies a little bit the final plot. The latest Gnuplot can actually do this internally, but have a look at what one can do with awk. Awk and multiple files Something that awk is really good at is handling of multiple files, as bash does, but combined with all the tools that comes with the language. Look at this Case study Multiple files - first approach to get an idea why awk could be a better choice in such situation. Awk inside bash script Perhaps, a better idea is to add transparently an awk code into your bash script. Here is a simple example that illustrates three different ways to do it. #!/bin/bash echo \"===================================\" awk -f - <<-EOF /etc/issue BEGIN {print \"Hi\"} {print} EOF echo \"===================================\" sh <<-EOF awk ' BEGIN {print \"Hi 2\"} {print} ' /etc/issue EOF echo \"===================================\" awk ' BEGIN {print \"Hi 3\"} {print} ' /etc/issue","title":"Awk or Bash"},{"location":"awk_bash/#awk-or-bash","text":"The short answer - keep using bash but have a look at of some awk neat features that might come in handy. cmd = \"ls -lrt\" - lets have the command stored for convenience system ( cmd ) - will make a system call and execute the command, while sending the output to standard output cmd | getline - will do the same but you will read the first line from the output. # then let's read all lines and do something while ( cmd | getline ) { if ( $ 5 > 200 ) system ( \"cp \" $ 9 \" target_folder/\" ) } Essentially, one gets an extremely easy way to communicate between programs and execute system calls in addition to convenient arithmetic functions (for example, bash does not like fractional numbers ) and you add a lot of flexibility to your decision making script... Have a look at the script that I wrote some 100 years ago that prepares a Gnuplot input to fit Birch\u2013Murnaghan equation of state , then gets the results from the fit ( without using temporary files ) and prettifies a little bit the final plot. The latest Gnuplot can actually do this internally, but have a look at what one can do with awk.","title":"Awk or bash"},{"location":"awk_bash/#awk-and-multiple-files","text":"Something that awk is really good at is handling of multiple files, as bash does, but combined with all the tools that comes with the language. Look at this Case study Multiple files - first approach to get an idea why awk could be a better choice in such situation.","title":"Awk and multiple files"},{"location":"awk_bash/#awk-inside-bash-script","text":"Perhaps, a better idea is to add transparently an awk code into your bash script. Here is a simple example that illustrates three different ways to do it. #!/bin/bash echo \"===================================\" awk -f - <<-EOF /etc/issue BEGIN {print \"Hi\"} {print} EOF echo \"===================================\" sh <<-EOF awk ' BEGIN {print \"Hi 2\"} {print} ' /etc/issue EOF echo \"===================================\" awk ' BEGIN {print \"Hi 3\"} {print} ' /etc/issue","title":"Awk inside bash script"},{"location":"handy/","text":"Handy to have Awk reference card (.pdf) Here are some functions that are missing in awk and might come in handy to have... The functions have been collected from different sources or written by myself, so always make sure that they work as expected. Please, do not assume that they are always correct! For more complete and up to date list with various functions, look here: http://rosettacode.org/wiki/Category:AWK #============================================== BEGIN { pi = 3.14159265358979 ; rad2deg = 180 . / pi Q = \"\\\"\" ; _ = SUBSEP ; Inf = 1.99999 * ( 2 ** ( 127 )); NegInf = - 1 * 2 ** ( 126 ); White = \"^[ \\t\\n\\r]*$\" ; Number = \"^[+-]?([0-9]+[.]?[0-9]*|[.][0-9]+)([eE][+-]?[0-9]+)?$\" ; } #============================================== function asin ( a ) { return atan2 ( a , sqrt ( 1 - a * a )) } function acos ( a ) { return pi / 2 - asin ( a ) } function abs ( x ) { return ( x >= 0 ) ? x : - x } function sgn ( x ) { return ( x == 0 ) ? 0 : ( ( x > 0 ) ? 1 : - 1 ) } function coth ( x ) { exp2x = exp ( 2.0 * x ); return ( exp2x + 1.0 ) / ( exp2x - 1.0 ) } #vectors function norm ( x ) { return ( sqrt ( x [ 1 ] * x [ 1 ] + x [ 2 ] * x [ 2 ] + x [ 3 ] * x [ 3 ]));} function dotprod ( x , y ) { return ( x [ 1 ] * y [ 1 ] + x [ 2 ] * y [ 2 ] + x [ 3 ] * y [ 3 ] );} function angle ( v1 , v2 ) { myacos = dotprod ( v1 , v2 ) / norm ( v1 ) / norm ( v2 ); if ( myacos > 1.0 ) myacos = 1.0 ; if ( myacos <- 1.0 ) myacos = - 1.0 ; return ( acos ( myacos ) * 180.0 / pi ); } # Printing function barph ( str ) { print str > \"/dev/tty\" } function die ( str ) { barph ( str ); exit 1 } # Arrays # These array store the size of the array in position C<Array[0]>. function top ( a ) { return a [ a [ 0 ]]} function push ( a , x , i ) { i =++ a [ 0 ]; a [ i ] = x ; return i } function push2 ( a , x , y , i ) { i =++ a [ x , 0 ]; a [ x , i ] = y ; return i } function pop ( a , x , i ) { i = a [ 0 ] -- ; if ( ! i ) { return \"\" } else { x = a [ i ]; delete a [ i ]; return x } } # Arrays to strings function saya ( s , a ) { print s ; print a2s ( a )} function a2s ( a , n , pre , i , str ) { for ( i in a ) str = str pre \"[\" i \"]=[\" a [ i ] \"]\\n\" ; return str ; } # Strings function number ( x ) { return x ~ Number } function symbol ( x ) { return ! number ( x ) } function blank ( x ) { return x ~ /^[ \\t]*$/ } function trimLeft ( x ) { sub ( /^[ \\t]*/ , \"\" , x ); return x } function trimRight ( x ) { sub ( /[ \\t]*$/ , \"\" , x ); return x } function trim ( x , y ) { return trimRight ( trimLeft ( x ))} function str2keys ( str , keys , sep , n , i , tmp ) { n = split ( str , tmp , sep ); for ( i in tmp ) keys [ tmp [ i ]]; keys [ 0 ] = n ; } function pairs2nums ( str , pairs , sep , n , i , tmp ) { n = split ( str , tmp , sep ); for ( i = 1 ; i <= n ; i = i + 2 ) { paris [ tmp [ i ]] = tmp [ i + 1 ] + 0 ; pairs [ 0 ] ++ ; } } # Numbers function odd ( x ) { return x % 2 } function even ( x ) { return ! odd ( x )} function most ( x , y ) { if ( x > y ) { return x } else { return y }} function least ( x , y ) { if ( x < y ) { return x } else { return y }} function round ( x ) { if ( x < 0 ) { return int ( x - 0.5 )} else { return int ( x + 0.5 )}} function between ( min , max ) { if ( max == min ) { return min } else { return min + (( max - min ) * rand ())}} function mean ( sumX , n ) { return sumX / n } function sd ( sumSq , sumX , n ) { return sqrt (( sumSq - (( sumX * sumX ) / n )) / ( n - 1 ))} #File exists function exists ( file , dummy , ret ) { ret = 0 ; if ( ( getline dummy < file ) >= 0 ) { ret = 1 ; close ( file )}; return ret ; } # rewind.awk --- rewind the current file and start over function rewind ( i ) { # shift remaining arguments up for ( i = ARGC ; i > ARGIND ; i -- ) ARGV [ i ] = ARGV [ i - 1 ] # make sure gawk knows to keep going ARGC ++ # make current file next to get done ARGV [ ARGIND + 1 ] = FILENAME # do it nextfile }","title":"Handy to have"},{"location":"handy/#handy-to-have","text":"Awk reference card (.pdf) Here are some functions that are missing in awk and might come in handy to have... The functions have been collected from different sources or written by myself, so always make sure that they work as expected. Please, do not assume that they are always correct! For more complete and up to date list with various functions, look here: http://rosettacode.org/wiki/Category:AWK #============================================== BEGIN { pi = 3.14159265358979 ; rad2deg = 180 . / pi Q = \"\\\"\" ; _ = SUBSEP ; Inf = 1.99999 * ( 2 ** ( 127 )); NegInf = - 1 * 2 ** ( 126 ); White = \"^[ \\t\\n\\r]*$\" ; Number = \"^[+-]?([0-9]+[.]?[0-9]*|[.][0-9]+)([eE][+-]?[0-9]+)?$\" ; } #============================================== function asin ( a ) { return atan2 ( a , sqrt ( 1 - a * a )) } function acos ( a ) { return pi / 2 - asin ( a ) } function abs ( x ) { return ( x >= 0 ) ? x : - x } function sgn ( x ) { return ( x == 0 ) ? 0 : ( ( x > 0 ) ? 1 : - 1 ) } function coth ( x ) { exp2x = exp ( 2.0 * x ); return ( exp2x + 1.0 ) / ( exp2x - 1.0 ) } #vectors function norm ( x ) { return ( sqrt ( x [ 1 ] * x [ 1 ] + x [ 2 ] * x [ 2 ] + x [ 3 ] * x [ 3 ]));} function dotprod ( x , y ) { return ( x [ 1 ] * y [ 1 ] + x [ 2 ] * y [ 2 ] + x [ 3 ] * y [ 3 ] );} function angle ( v1 , v2 ) { myacos = dotprod ( v1 , v2 ) / norm ( v1 ) / norm ( v2 ); if ( myacos > 1.0 ) myacos = 1.0 ; if ( myacos <- 1.0 ) myacos = - 1.0 ; return ( acos ( myacos ) * 180.0 / pi ); } # Printing function barph ( str ) { print str > \"/dev/tty\" } function die ( str ) { barph ( str ); exit 1 } # Arrays # These array store the size of the array in position C<Array[0]>. function top ( a ) { return a [ a [ 0 ]]} function push ( a , x , i ) { i =++ a [ 0 ]; a [ i ] = x ; return i } function push2 ( a , x , y , i ) { i =++ a [ x , 0 ]; a [ x , i ] = y ; return i } function pop ( a , x , i ) { i = a [ 0 ] -- ; if ( ! i ) { return \"\" } else { x = a [ i ]; delete a [ i ]; return x } } # Arrays to strings function saya ( s , a ) { print s ; print a2s ( a )} function a2s ( a , n , pre , i , str ) { for ( i in a ) str = str pre \"[\" i \"]=[\" a [ i ] \"]\\n\" ; return str ; } # Strings function number ( x ) { return x ~ Number } function symbol ( x ) { return ! number ( x ) } function blank ( x ) { return x ~ /^[ \\t]*$/ } function trimLeft ( x ) { sub ( /^[ \\t]*/ , \"\" , x ); return x } function trimRight ( x ) { sub ( /[ \\t]*$/ , \"\" , x ); return x } function trim ( x , y ) { return trimRight ( trimLeft ( x ))} function str2keys ( str , keys , sep , n , i , tmp ) { n = split ( str , tmp , sep ); for ( i in tmp ) keys [ tmp [ i ]]; keys [ 0 ] = n ; } function pairs2nums ( str , pairs , sep , n , i , tmp ) { n = split ( str , tmp , sep ); for ( i = 1 ; i <= n ; i = i + 2 ) { paris [ tmp [ i ]] = tmp [ i + 1 ] + 0 ; pairs [ 0 ] ++ ; } } # Numbers function odd ( x ) { return x % 2 } function even ( x ) { return ! odd ( x )} function most ( x , y ) { if ( x > y ) { return x } else { return y }} function least ( x , y ) { if ( x < y ) { return x } else { return y }} function round ( x ) { if ( x < 0 ) { return int ( x - 0.5 )} else { return int ( x + 0.5 )}} function between ( min , max ) { if ( max == min ) { return min } else { return min + (( max - min ) * rand ())}} function mean ( sumX , n ) { return sumX / n } function sd ( sumSq , sumX , n ) { return sqrt (( sumSq - (( sumX * sumX ) / n )) / ( n - 1 ))} #File exists function exists ( file , dummy , ret ) { ret = 0 ; if ( ( getline dummy < file ) >= 0 ) { ret = 1 ; close ( file )}; return ret ; } # rewind.awk --- rewind the current file and start over function rewind ( i ) { # shift remaining arguments up for ( i = ARGC ; i > ARGIND ; i -- ) ARGV [ i ] = ARGV [ i - 1 ] # make sure gawk knows to keep going ARGC ++ # make current file next to get done ARGV [ ARGIND + 1 ] = FILENAME # do it nextfile }","title":"Handy to have"},{"location":"tinyutils/","text":"Tiny utilities by Douglas Scofield https://github.com/douglasgscofield/tinyutils Tiny scripts that work on a single column of data. Some of these transform a single column of their input while passing everything through, some produce summary tables, and some produce a single summary value. All (so far) are in awk, and all have the common options header = 0 which specifies the number of header lines on input to skip, skip_comment = 1 which specifies whether to skip comment lines on input that begin with # , and col = 1 , which specifies which column of the input stream should be examined. Since they are awk scripts, they also have the standard variables for specifying the input field separator FS = \"\\t\" and the output field separator OFS = \"\\t\" . Default output column separator is \"\\t\" . Set any of these variables by using key = value on the command line. For example to find the median of the third column of numbers, when the first 10 lines of input are header: $ median col = 3 header = 10 your.dat Stick these in a pipeline that ends with spark for quick visual summaries. If indels.vcf.gz is a compressed VCF file containing indel calls, then this will print a sparkline of indel sizes in the range of \u00b110bp: $ zcat indels.vcf.gz \\ | stripfilt \\ | awk '{print length($5)-length($4)}' \\ | inrange abs = 10 \\ | hist \\ | cut -f2 \\ | spark \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2582\u2588\u2581\u2587\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 We get the second column of hist output because that's the counts. This clearly shows the overabundance of single-base indels, and a slight overrepresentation of single-base deletions over insertions. ... Please, visit the Git repository for complete documentation and up to date sources: https://github.com/douglasgscofield/tinyutils","title":"tinyutils"},{"location":"tinyutils/#tiny-utilities-by-douglas-scofield","text":"https://github.com/douglasgscofield/tinyutils Tiny scripts that work on a single column of data. Some of these transform a single column of their input while passing everything through, some produce summary tables, and some produce a single summary value. All (so far) are in awk, and all have the common options header = 0 which specifies the number of header lines on input to skip, skip_comment = 1 which specifies whether to skip comment lines on input that begin with # , and col = 1 , which specifies which column of the input stream should be examined. Since they are awk scripts, they also have the standard variables for specifying the input field separator FS = \"\\t\" and the output field separator OFS = \"\\t\" . Default output column separator is \"\\t\" . Set any of these variables by using key = value on the command line. For example to find the median of the third column of numbers, when the first 10 lines of input are header: $ median col = 3 header = 10 your.dat Stick these in a pipeline that ends with spark for quick visual summaries. If indels.vcf.gz is a compressed VCF file containing indel calls, then this will print a sparkline of indel sizes in the range of \u00b110bp: $ zcat indels.vcf.gz \\ | stripfilt \\ | awk '{print length($5)-length($4)}' \\ | inrange abs = 10 \\ | hist \\ | cut -f2 \\ | spark \u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2582\u2588\u2581\u2587\u2582\u2581\u2581\u2581\u2581\u2581\u2581\u2581\u2581 We get the second column of hist output because that's the counts. This clearly shows the overabundance of single-base indels, and a slight overrepresentation of single-base deletions over insertions. ... Please, visit the Git repository for complete documentation and up to date sources: https://github.com/douglasgscofield/tinyutils","title":"Tiny utilities by Douglas Scofield"},{"location":"Bio/NCBI-taxonomy/","text":"Substitute scientific with common species names in a phylogenetic tree file Problem formulated an presented at the workshop by Voichita Marinescu , Department of Medical Biochemistry and Microbiology, Comparative genetics and functional genomics Step 1 - Generate a table with the scientific-common name correspondence We need the correspondence between the scientific and common species names as described in the NCBI Taxonomy Database . We want to do this for any number of species automatically, so we download the entire archive taxdump.tar.gz from the NCBI taxonomy database dump . This archive contains the names.dmp file with the format: 10090 | house mouse | | genbank common name | 10090 | LK3 transgenic mice | | includes | 10090 | mouse | mouse <Mus musculus> | common name | 10090 | Mus musculus Linnaeus, 1758 | | authority | 10090 | Mus musculus | | scientific name | 10090 | Mus sp. 129SV | | includes | 10090 | nude mice | | includes | 10090 | transgenic mice | | includes | Make new folder for this exercise and cd into it. Download the file and extract the names.dmp $ wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz $ tar -xvf taxdump.tar.gz names.dmp taxdump.tar.gz 51MB names.dmp 189MB We want to convert this table into a tab delimited file with this format: taxonID scientific_name common_name genbank_common_name 10090 Mus musculus mouse house mouse We want to add an underscore between the genus (e.g. Mus) and the specific name of the species (e.g. musculus), so the scientific name is listed as in the tree file (e.g Mus_musculus). We want to capitalize the first word in the common name (if not already capitalized). Step 2 - Edit the phylogenetic tree file The phylogenetic tree file used for the 100way alignment is hg38.100way.scientificNames.nh. It can be downloaded from here and details could be found here . Download the file (4.1KB). $ wget http://hgdownload.cse.ucsc.edu/goldenpath/hg38/multiz100way/hg38.100way.scientificNames.nh The format of the tree file is ((((((((((((((((((Homo_sapiens:0.00655, Pan_troglodytes:0.00684):0.00422, Gorilla_gorilla_gorilla:0.008964):0.009693, Pongo_pygmaeus_abelii:0.01894):0.003471, Nomascus_leucogenys:0.02227):0.01204, (((Macaca_mulatta:0.004991, Macaca_fascicularis:0.004991):0.003, Papio_anubis:0.008042):0.01061, Chlorocebus_sabaeus:0.027):0.025):0.02183, (Callithrix_jacchus:0.03, Saimiri_boliviensis:0.01035):0.01965):0.07261, Otolemur_garnettii:0.13992):0.013494, Tupaia_chinensis:0.174937):0.002, (((Spermophilus_tridecemlineatus:0.125468, See a description of the Newick tree format here . The phylogenetic tree could be visualized online at https://itol.embl.de/ (notice that this application takes care of removing the _ from the scientific name). Before After Step 1 !!! WARNING !!! WARNING !!! WARNING !!! The files are in Windows/DOS ASCII text, with CRLF line terminators format which makes awk to misbehave. Check your files and convert them to UNIX format if necessary. # check the format $ file filename filename: ASCII text, with CRLF line terminators # Convert to unix format $ dos2unix filename dos2unix: converting file filename to Unix format ... # check again $ file filename filename: ASCII text Let's first tabulate the NCBI Taxonomy Database in more convenient format for us - getting the relevant information on single line, replace some spaces with underscore symbol _ , remove the extra blanks in front and after the names, etc. names.tab 9605|Homo|Humans| 9606|Homo_sapiens|Man| 9608|Canidae|Dog, coyote, wolf, fox| 9611|Canis|| 9612|Canis_lupus|Grey wolf| 9614|Canis_latrans|Coyote| 9615|Canis_lupus_familiaris|Dogs| 9616|Canis_sp.|| 9619|Dusicyon|| 9620|Cerdocyon_thous|Crab-eating fox| 9621|Lycaon|| 9622|Lycaon_pictus|Painted hunting dog| 9623|Otocyon|| 9624|Otocyon_megalotis|Bat-eared fox| 9625|Vulpes|| 9626|Vulpes_chama|Cape fox| 9627|Vulpes_vulpes|Silver fox| 9629|Vulpes_corsac|Corsac fox| 9630|Vulpes_macrotis|Kit fox| 9631|Vulpes_velox|Swift fox| Might not be the best solution but it is easy to read and modify, for now. Note, we do not need to sort but it will look better if we have the final result in order. $ ./01.tabulate-names.awk names.dmp | sort -g -k 1 > names.tab # Or with bzip2 compression \"on the fly\" $ ./01.tabulate-names.awk < ( bzcat names.dmp.bz2 ) | bzip2 -c > names.tab.bz2 01.tabulate-names.awk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #!/usr/bin/awk -f BEGIN { FS = \"|\" # print \"#taxonID scientific_name common_name genbank_common_name\" } $ 4 ~ \"scientific name\" { sciname [ $ 1 * 1 ] = unds ( Clean ( $ 2 )); next } $ 4 ~ \"common name\" { com_name [ $ 1 * 1 ] = Cap ( Clean ( $ 2 )); next } $ 4 ~ \"genbank common name\" { genbank [ $ 1 * 1 ] = unds ( Clean ( $ 2 )); next } END { for ( i in sciname ) print i \"|\" sciname [ i ] \"|\" com_name [ i ] \"|\" genbank [ i ] } # Function declarations ============================== function Clean ( string ){ sub ( /^[ \\t]+/ , \"\" , string ) sub ( /[ \\t]+$/ , \"\" , string ) return string } function unds ( string ) { gsub ( \" \" , \"_\" , string ); return string } function Cap ( string ) { return toupper ( substr ( string , 0 , 1 )) substr ( string , 2 ) } Note that this script will keep the last information for the corresponding match for each ID. To prevent this we need to take care that any subsequent match is ignored 01.tabulate-names-first.awk #!/usr/bin/awk -f BEGIN { FS = \"|\" # print \"#taxonID scientific_name common_name genbank_common_name\" } $ 4 ~ \"scientific name\" { if ( ! sciname [ $ 1 * 1 ] ) sciname [ $ 1 * 1 ] = unds ( Clean ( $ 2 )); next } $ 4 ~ \"common name\" { if ( ! com_name [ $ 1 * 1 ]) com_name [ $ 1 * 1 ] = Cap ( Clean ( $ 2 )); next } $ 4 ~ \"genbank common name\" { if ( ! genbank [ $ 1 * 1 ] ) genbank [ $ 1 * 1 ] = unds ( Clean ( $ 2 )); next } END { for ( i in sciname ) print i \"|\" sciname [ i ] \"|\" com_name [ i ] \"|\" genbank [ i ] } function Clean ( string ){ sub ( /^[ \\t]+/ , \"\" , string ) sub ( /[ \\t]+$/ , \"\" , string ) return string } function unds ( string ) { gsub ( \" \" , \"_\" , string ); return string } function Cap ( string ) { return toupper ( substr ( string , 0 , 1 )) substr ( string , 2 ) } Step 2 Now we can use the tabulated data in names.tab and perform the replacement in hg38.100way.scientificNames.nh by matching the scientific names in $2 with the common names in $3 - we use FS=\"|\" ((((((((((((((((((Man:0.00655, Chimpanzee:0.00684):0.00422, Western lowland gorilla:0.008964):0.009693, Pongo_pygmaeus_abelii:0.01894):0.003471, White-cheeked Gibbon:0.02227):0.01204, (((Rhesus monkeys:0.004991, Long-tailed macaque:0.004991):0.003, Olive baboon:0.008042):0.01061, Green monkey:0.027):0.025):0.02183, (White-tufted-ear marmoset:0.03, Bolivian squirrel monkey:0.01035):0.01965):0.07261, Small-eared galago:0.13992):0.013494, Again, this might not be the best way but it works. The suggested solutions could be easily merged into a single script. I would prefer to have them in steps, so I can make sure that the first step has completed successfully ( it takes some time ) before I continue. Also I can filter the unnecessary data in the newly tabulated file and use only relevant data or alter further if I need. $ ./02.substitute.awk names.tab hg38.100way.scientificNames.nh > NEW.g38.100way.scientificNames.nh # Or with bzip2 compression \"on the fly\" $ ./02.substitute.awk < ( bzcat names.tab.bz2 ) hg38.100way.scientificNames.nh > NEW.g38.100way.scientificNames.nh $ ./02.substitute.awk names.tab hg38.100way.scientificNames.nh 02.substitute.awk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/usr/bin/awk -f BEGIN { FS = \"|\" # print \"#taxonID scientific_name common_name genbank_common_name\" } NR == FNR { map [ $ 2 ] = $ 3 ; next } { line = $ 0 gsub ( \"[0-9.,;:)( ]\" , \"\" ) if ( $ 0 in map ) sub ( $ 0 , map [ $ 0 ], line ) print line }","title":"Substitute scientific with common species names in a phylogenetic tree file"},{"location":"Bio/NCBI-taxonomy/#substitute-scientific-with-common-species-names-in-a-phylogenetic-tree-file","text":"Problem formulated an presented at the workshop by Voichita Marinescu , Department of Medical Biochemistry and Microbiology, Comparative genetics and functional genomics","title":"Substitute scientific with common species names in a phylogenetic tree file"},{"location":"Bio/NCBI-taxonomy/#step-1-generate-a-table-with-the-scientific-common-name-correspondence","text":"We need the correspondence between the scientific and common species names as described in the NCBI Taxonomy Database . We want to do this for any number of species automatically, so we download the entire archive taxdump.tar.gz from the NCBI taxonomy database dump . This archive contains the names.dmp file with the format: 10090 | house mouse | | genbank common name | 10090 | LK3 transgenic mice | | includes | 10090 | mouse | mouse <Mus musculus> | common name | 10090 | Mus musculus Linnaeus, 1758 | | authority | 10090 | Mus musculus | | scientific name | 10090 | Mus sp. 129SV | | includes | 10090 | nude mice | | includes | 10090 | transgenic mice | | includes | Make new folder for this exercise and cd into it. Download the file and extract the names.dmp $ wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz $ tar -xvf taxdump.tar.gz names.dmp taxdump.tar.gz 51MB names.dmp 189MB We want to convert this table into a tab delimited file with this format: taxonID scientific_name common_name genbank_common_name 10090 Mus musculus mouse house mouse We want to add an underscore between the genus (e.g. Mus) and the specific name of the species (e.g. musculus), so the scientific name is listed as in the tree file (e.g Mus_musculus). We want to capitalize the first word in the common name (if not already capitalized).","title":"Step 1 - Generate a table with the scientific-common name correspondence"},{"location":"Bio/NCBI-taxonomy/#step-2-edit-the-phylogenetic-tree-file","text":"The phylogenetic tree file used for the 100way alignment is hg38.100way.scientificNames.nh. It can be downloaded from here and details could be found here . Download the file (4.1KB). $ wget http://hgdownload.cse.ucsc.edu/goldenpath/hg38/multiz100way/hg38.100way.scientificNames.nh The format of the tree file is ((((((((((((((((((Homo_sapiens:0.00655, Pan_troglodytes:0.00684):0.00422, Gorilla_gorilla_gorilla:0.008964):0.009693, Pongo_pygmaeus_abelii:0.01894):0.003471, Nomascus_leucogenys:0.02227):0.01204, (((Macaca_mulatta:0.004991, Macaca_fascicularis:0.004991):0.003, Papio_anubis:0.008042):0.01061, Chlorocebus_sabaeus:0.027):0.025):0.02183, (Callithrix_jacchus:0.03, Saimiri_boliviensis:0.01035):0.01965):0.07261, Otolemur_garnettii:0.13992):0.013494, Tupaia_chinensis:0.174937):0.002, (((Spermophilus_tridecemlineatus:0.125468, See a description of the Newick tree format here . The phylogenetic tree could be visualized online at https://itol.embl.de/ (notice that this application takes care of removing the _ from the scientific name). Before After","title":"Step 2 - Edit the phylogenetic tree file"},{"location":"Bio/NCBI-taxonomy/#step-1","text":"!!! WARNING !!! WARNING !!! WARNING !!! The files are in Windows/DOS ASCII text, with CRLF line terminators format which makes awk to misbehave. Check your files and convert them to UNIX format if necessary. # check the format $ file filename filename: ASCII text, with CRLF line terminators # Convert to unix format $ dos2unix filename dos2unix: converting file filename to Unix format ... # check again $ file filename filename: ASCII text Let's first tabulate the NCBI Taxonomy Database in more convenient format for us - getting the relevant information on single line, replace some spaces with underscore symbol _ , remove the extra blanks in front and after the names, etc. names.tab 9605|Homo|Humans| 9606|Homo_sapiens|Man| 9608|Canidae|Dog, coyote, wolf, fox| 9611|Canis|| 9612|Canis_lupus|Grey wolf| 9614|Canis_latrans|Coyote| 9615|Canis_lupus_familiaris|Dogs| 9616|Canis_sp.|| 9619|Dusicyon|| 9620|Cerdocyon_thous|Crab-eating fox| 9621|Lycaon|| 9622|Lycaon_pictus|Painted hunting dog| 9623|Otocyon|| 9624|Otocyon_megalotis|Bat-eared fox| 9625|Vulpes|| 9626|Vulpes_chama|Cape fox| 9627|Vulpes_vulpes|Silver fox| 9629|Vulpes_corsac|Corsac fox| 9630|Vulpes_macrotis|Kit fox| 9631|Vulpes_velox|Swift fox| Might not be the best solution but it is easy to read and modify, for now. Note, we do not need to sort but it will look better if we have the final result in order. $ ./01.tabulate-names.awk names.dmp | sort -g -k 1 > names.tab # Or with bzip2 compression \"on the fly\" $ ./01.tabulate-names.awk < ( bzcat names.dmp.bz2 ) | bzip2 -c > names.tab.bz2 01.tabulate-names.awk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #!/usr/bin/awk -f BEGIN { FS = \"|\" # print \"#taxonID scientific_name common_name genbank_common_name\" } $ 4 ~ \"scientific name\" { sciname [ $ 1 * 1 ] = unds ( Clean ( $ 2 )); next } $ 4 ~ \"common name\" { com_name [ $ 1 * 1 ] = Cap ( Clean ( $ 2 )); next } $ 4 ~ \"genbank common name\" { genbank [ $ 1 * 1 ] = unds ( Clean ( $ 2 )); next } END { for ( i in sciname ) print i \"|\" sciname [ i ] \"|\" com_name [ i ] \"|\" genbank [ i ] } # Function declarations ============================== function Clean ( string ){ sub ( /^[ \\t]+/ , \"\" , string ) sub ( /[ \\t]+$/ , \"\" , string ) return string } function unds ( string ) { gsub ( \" \" , \"_\" , string ); return string } function Cap ( string ) { return toupper ( substr ( string , 0 , 1 )) substr ( string , 2 ) } Note that this script will keep the last information for the corresponding match for each ID. To prevent this we need to take care that any subsequent match is ignored 01.tabulate-names-first.awk #!/usr/bin/awk -f BEGIN { FS = \"|\" # print \"#taxonID scientific_name common_name genbank_common_name\" } $ 4 ~ \"scientific name\" { if ( ! sciname [ $ 1 * 1 ] ) sciname [ $ 1 * 1 ] = unds ( Clean ( $ 2 )); next } $ 4 ~ \"common name\" { if ( ! com_name [ $ 1 * 1 ]) com_name [ $ 1 * 1 ] = Cap ( Clean ( $ 2 )); next } $ 4 ~ \"genbank common name\" { if ( ! genbank [ $ 1 * 1 ] ) genbank [ $ 1 * 1 ] = unds ( Clean ( $ 2 )); next } END { for ( i in sciname ) print i \"|\" sciname [ i ] \"|\" com_name [ i ] \"|\" genbank [ i ] } function Clean ( string ){ sub ( /^[ \\t]+/ , \"\" , string ) sub ( /[ \\t]+$/ , \"\" , string ) return string } function unds ( string ) { gsub ( \" \" , \"_\" , string ); return string } function Cap ( string ) { return toupper ( substr ( string , 0 , 1 )) substr ( string , 2 ) }","title":"Step 1"},{"location":"Bio/NCBI-taxonomy/#step-2","text":"Now we can use the tabulated data in names.tab and perform the replacement in hg38.100way.scientificNames.nh by matching the scientific names in $2 with the common names in $3 - we use FS=\"|\" ((((((((((((((((((Man:0.00655, Chimpanzee:0.00684):0.00422, Western lowland gorilla:0.008964):0.009693, Pongo_pygmaeus_abelii:0.01894):0.003471, White-cheeked Gibbon:0.02227):0.01204, (((Rhesus monkeys:0.004991, Long-tailed macaque:0.004991):0.003, Olive baboon:0.008042):0.01061, Green monkey:0.027):0.025):0.02183, (White-tufted-ear marmoset:0.03, Bolivian squirrel monkey:0.01035):0.01965):0.07261, Small-eared galago:0.13992):0.013494, Again, this might not be the best way but it works. The suggested solutions could be easily merged into a single script. I would prefer to have them in steps, so I can make sure that the first step has completed successfully ( it takes some time ) before I continue. Also I can filter the unnecessary data in the newly tabulated file and use only relevant data or alter further if I need. $ ./02.substitute.awk names.tab hg38.100way.scientificNames.nh > NEW.g38.100way.scientificNames.nh # Or with bzip2 compression \"on the fly\" $ ./02.substitute.awk < ( bzcat names.tab.bz2 ) hg38.100way.scientificNames.nh > NEW.g38.100way.scientificNames.nh $ ./02.substitute.awk names.tab hg38.100way.scientificNames.nh 02.substitute.awk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/usr/bin/awk -f BEGIN { FS = \"|\" # print \"#taxonID scientific_name common_name genbank_common_name\" } NR == FNR { map [ $ 2 ] = $ 3 ; next } { line = $ 0 gsub ( \"[0-9.,;:)( ]\" , \"\" ) if ( $ 0 in map ) sub ( $ 0 , map [ $ 0 ], line ) print line }","title":"Step 2"},{"location":"Bio/Stat-large-files/","text":"Statistics on very large columns of values Problem formulated an presented at the workshop by Voichita Marinescu , Department of Medical Biochemistry and Microbiology, Comparative genetics and functional genomics When analyzing variables with large numbers of values, one needs to generate descriptive statistics (e.g. mean, median, std, quartiles, etc.) in order to set thresholds for further analyses. This could easily be done in R if the vector or values could be loaded. But sometimes the number of values is prohibitively large for R, and even pandas in Python may fail. One such example is provided by the conservation scores for each nucleotide position of the MultiZ alignment of 99 vertebrate genomes against the human genome ( UCSC 100way alignment ). You visualized the phylogenetic tree for the species in this alignment in the previous exercise. Using the program phyloP (phylogenetic P-values) from the PHAST package , a conservation score is computed for each position in the human genome resulting in 3 billion values. To identify the most conserved positions (the ones with the highest phyloP scores) one would need to generate descriptive statistics for the score distribution and set thresholds accordingly. We want to output the total number of values (count) and the mean, median, std, min, max, 10%, 25%, 75%, 90% of the score values, and also to plot the histogram. Input The complete file in bigwig format is 5.5GB in size. Do not download the file hg38.phastCons100way.bw from http://hgdownload.soe.ucsc.edu/goldenPath/hg38/phastCons100way/ The bigwig format can be converted to wig format using bigWigToWig https://www.encodeproject.org/software/bigwigtowig/ The wig format can be converted to bed format using wig2bed https://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/wig2bed.html For a short presentation of the main bioinformatics file formats see the UCSC Data File Formats page . In this exercise we will not work with complete file, but with smaller files downloaded using the UCSC Table Browser . We will use a 400kb interval on chr17:7,400,001-7,800,000 The Table Browser allows downloads for up to 100kb. chr17_7.400.001-7.500.000.data chr17_7.500.001-7.600.000.data chr17_7.600.001-7.700.000.data chr17_7.700.001-7.800.000.data !!! WARNING !!! WARNING !!! WARNING !!! The files are in Windows/DOS ASCII text, with CRLF line terminators format which makes awk to misbehave. Check your files and convert them to UNIX format if necessary. # check the format $ file filename filename: ASCII text, with CRLF line terminators # Convert to unix format $ dos2unix filename dos2unix: converting file filename to Unix format ... # check again $ file filename filename: ASCII text The phyloP scores are in the second column. Compute statistics The memory problem for large data could be solved in Python with Dask . Since the analysis uses approximate methods for the quartile and other percentiles, you might need to resort to percentiles_method=\"tdigest\" which improves the final results. Here we will rely on the specifics of the data to implement an easy trick in awk (you can do it in any another language as well). We will calculate a discrete histogram over the values in the second column in the files. This generates too many elements, but looking into the data we can see that resolution is up to the 3 decimal points... So we will reduce the numbers (which in this case is even unnecessary but might be become a potential problem). Step 1 $ ./01.histogram.awk *.data.txt | sort -g -k 1 > hist.dat 01.histogram.awk 1 2 3 4 5 6 #!/usr/bin/awk -f #BEGIN{CONVFMT=\"%.3f\"} # Uncomment if necessary NF == 2 { counts [ $ 2 * 1 ] ++ } END { for ( v in counts ) print v , counts [ v ] } Step 2 Let's see what we got. plot \"hist.dat\" with line You can use other programs to make the histograms in intervals - here we can use gnuplot as example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/usr/bin/gnuplot -persist set no key set noytics # Find the mean mean = system ( \"awk '{sum+=$1*$2; tot+=$2} END{print sum/tot}' hist.dat\" ) set arrow 1 from mean , 0 to mean , graph 1 nohead ls 1 lc rgb \"blue\" set label 1 sprintf ( \" Mean: %s\" , mean ) at mean , screen 0.1 # Histogram binwidth = 0.1 bin ( x , width ) = width * floor ( x / width ) plot 'hist.dat' using ( bin ( $ 1 , binwidth )) : ( 1.0 ) smooth freq with lines Step 3 Let's find some relevant numbers from hist.dat . $ head - n 1 hist . dat | awk '{print \"min:\\t\"$1}' min : - 14.247 $ tail - n 1 hist . dat | awk '{print \"max:\\t\"$1}' max : 10.003 This was easy. How about the real numbers we are after? We need ianother program - again you can do it in other languages as well. $ ./stats.awk hist.dat count: 398516 mean: 0 .361477 10 % :-0.973866 25 % :-0.326732 50 % :0.127701 75 % :0.53989 90 % :1.72925 stats.awk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #!/usr/bin/awk -f BEGIN { # Here sorting is very important, so we can count # all elemnts in ascending numerical order for the indexes - first column PROCINFO [ \"sorted_in\" ] = \"@ind_num_asc\" } { counts [ $ 1 ] += $ 2 sum += $ 1 *$ 2 total += $ 2 } END { quantiles [ 1 ] = 10 ; quantiles [ 2 ] = 25 ; quantiles [ 3 ] = 50 ; quantiles [ 4 ] = 75 ; quantiles [ 5 ] = 90 ; q = 1 ; nq = 5 quantile = quantiles [ q ] print \"count: \" total print \"mean: \" sum / total for ( v in counts ) { comul += counts [ v ] if ( comul >= total * quantile / 100 ) { print quantiles [ q ] \"% :\" v if ( q < nq ) quantile = quantiles [ ++ q ] else exit } } }","title":"Statistics on very large columns of values"},{"location":"Bio/Stat-large-files/#statistics-on-very-large-columns-of-values","text":"Problem formulated an presented at the workshop by Voichita Marinescu , Department of Medical Biochemistry and Microbiology, Comparative genetics and functional genomics When analyzing variables with large numbers of values, one needs to generate descriptive statistics (e.g. mean, median, std, quartiles, etc.) in order to set thresholds for further analyses. This could easily be done in R if the vector or values could be loaded. But sometimes the number of values is prohibitively large for R, and even pandas in Python may fail. One such example is provided by the conservation scores for each nucleotide position of the MultiZ alignment of 99 vertebrate genomes against the human genome ( UCSC 100way alignment ). You visualized the phylogenetic tree for the species in this alignment in the previous exercise. Using the program phyloP (phylogenetic P-values) from the PHAST package , a conservation score is computed for each position in the human genome resulting in 3 billion values. To identify the most conserved positions (the ones with the highest phyloP scores) one would need to generate descriptive statistics for the score distribution and set thresholds accordingly. We want to output the total number of values (count) and the mean, median, std, min, max, 10%, 25%, 75%, 90% of the score values, and also to plot the histogram.","title":"Statistics on very large columns of values"},{"location":"Bio/Stat-large-files/#input","text":"The complete file in bigwig format is 5.5GB in size. Do not download the file hg38.phastCons100way.bw from http://hgdownload.soe.ucsc.edu/goldenPath/hg38/phastCons100way/ The bigwig format can be converted to wig format using bigWigToWig https://www.encodeproject.org/software/bigwigtowig/ The wig format can be converted to bed format using wig2bed https://bedops.readthedocs.io/en/latest/content/reference/file-management/conversion/wig2bed.html For a short presentation of the main bioinformatics file formats see the UCSC Data File Formats page . In this exercise we will not work with complete file, but with smaller files downloaded using the UCSC Table Browser . We will use a 400kb interval on chr17:7,400,001-7,800,000 The Table Browser allows downloads for up to 100kb. chr17_7.400.001-7.500.000.data chr17_7.500.001-7.600.000.data chr17_7.600.001-7.700.000.data chr17_7.700.001-7.800.000.data !!! WARNING !!! WARNING !!! WARNING !!! The files are in Windows/DOS ASCII text, with CRLF line terminators format which makes awk to misbehave. Check your files and convert them to UNIX format if necessary. # check the format $ file filename filename: ASCII text, with CRLF line terminators # Convert to unix format $ dos2unix filename dos2unix: converting file filename to Unix format ... # check again $ file filename filename: ASCII text The phyloP scores are in the second column.","title":"Input"},{"location":"Bio/Stat-large-files/#compute-statistics","text":"The memory problem for large data could be solved in Python with Dask . Since the analysis uses approximate methods for the quartile and other percentiles, you might need to resort to percentiles_method=\"tdigest\" which improves the final results. Here we will rely on the specifics of the data to implement an easy trick in awk (you can do it in any another language as well). We will calculate a discrete histogram over the values in the second column in the files. This generates too many elements, but looking into the data we can see that resolution is up to the 3 decimal points... So we will reduce the numbers (which in this case is even unnecessary but might be become a potential problem).","title":"Compute statistics"},{"location":"Bio/Stat-large-files/#step-1","text":"$ ./01.histogram.awk *.data.txt | sort -g -k 1 > hist.dat 01.histogram.awk 1 2 3 4 5 6 #!/usr/bin/awk -f #BEGIN{CONVFMT=\"%.3f\"} # Uncomment if necessary NF == 2 { counts [ $ 2 * 1 ] ++ } END { for ( v in counts ) print v , counts [ v ] }","title":"Step 1"},{"location":"Bio/Stat-large-files/#step-2","text":"Let's see what we got. plot \"hist.dat\" with line You can use other programs to make the histograms in intervals - here we can use gnuplot as example: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/usr/bin/gnuplot -persist set no key set noytics # Find the mean mean = system ( \"awk '{sum+=$1*$2; tot+=$2} END{print sum/tot}' hist.dat\" ) set arrow 1 from mean , 0 to mean , graph 1 nohead ls 1 lc rgb \"blue\" set label 1 sprintf ( \" Mean: %s\" , mean ) at mean , screen 0.1 # Histogram binwidth = 0.1 bin ( x , width ) = width * floor ( x / width ) plot 'hist.dat' using ( bin ( $ 1 , binwidth )) : ( 1.0 ) smooth freq with lines","title":"Step 2"},{"location":"Bio/Stat-large-files/#step-3","text":"Let's find some relevant numbers from hist.dat . $ head - n 1 hist . dat | awk '{print \"min:\\t\"$1}' min : - 14.247 $ tail - n 1 hist . dat | awk '{print \"max:\\t\"$1}' max : 10.003 This was easy. How about the real numbers we are after? We need ianother program - again you can do it in other languages as well. $ ./stats.awk hist.dat count: 398516 mean: 0 .361477 10 % :-0.973866 25 % :-0.326732 50 % :0.127701 75 % :0.53989 90 % :1.72925 stats.awk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #!/usr/bin/awk -f BEGIN { # Here sorting is very important, so we can count # all elemnts in ascending numerical order for the indexes - first column PROCINFO [ \"sorted_in\" ] = \"@ind_num_asc\" } { counts [ $ 1 ] += $ 2 sum += $ 1 *$ 2 total += $ 2 } END { quantiles [ 1 ] = 10 ; quantiles [ 2 ] = 25 ; quantiles [ 3 ] = 50 ; quantiles [ 4 ] = 75 ; quantiles [ 5 ] = 90 ; q = 1 ; nq = 5 quantile = quantiles [ q ] print \"count: \" total print \"mean: \" sum / total for ( v in counts ) { comul += counts [ v ] if ( comul >= total * quantile / 100 ) { print quantiles [ q ] \"% :\" v if ( q < nq ) quantile = quantiles [ ++ q ] else exit } } }","title":"Step 3"},{"location":"Bio/bioawk/","text":"Bioawk Bioawk is an awk extension for biological formats written by Dr. Heng Li . See the documentation here . Install on MacOS with the Homebrew package manager $ brew install bioawk Conda conda install -c bioconda bioawk Install from source $ git clone git://github.com/lh3/bioawk.git $ cd bioawk && make # Make sure that bioawk is in your $PATH or use the full path to call the executable. Several tutorial on the net suggest installing it with sudo in you system path - no need for that. This is another way to say - avoid unnecessary (it is bad practice) installation of tools as root. You will not be able to do it on any computer center anyway. If you get error about ... make: yacc: Command not found then you need install few packages (root privileges required) $ sudo apt-get install bison byacc Examples bioawk supported formats $ bioawk -c help bed: 1 :chrom 2 :start 3 :end 4 :name 5 :score 6 :strand 7 :thickstart 8 :thickend 9 :rgb 10 :blockcount 11 :blocksizes 12 :blockstarts sam: 1 :qname 2 :flag 3 :rname 4 :pos 5 :mapq 6 :cigar 7 :rnext 8 :pnext 9 :tlen 10 :seq 11 :qual vcf: 1 :chrom 2 :pos 3 :id 4 :ref 5 :alt 6 :qual 7 :filter 8 :info gff: 1 :seqname 2 :source 3 :feature 4 :start 5 :end 6 :score 7 :filter 8 :strand 9 :group 10 :attribute fastx: 1 :name 2 :seq 3 :qual 4 :comment We will use GTF and FASTA files for the chr17:7400001-7800000 region, downloaded using the UCSC Table Browser . chr17_fragm.gtf chr17_fragm.fasta use the file chr17_fragm.gtf Print the length of all the exons (end position - start position): $ bioawk -c gff '$3 ~ /exon/ {print $seqname, $feature, $end-$start}' chr17_fragm.gtf | sort -nk3 use the file chr17_fragm.fasta Count the number of FASTA entries $ bioawk -c fastx 'END{print NR}' chr17_fragm.fasta Reverse complement the sequences $ bioawk -c fastx '{print \">\"$name\"\\n\"revcomp($seq)}' chr17_fragm.fasta > chr17_fragm.revcomp.fasta | head -n 2 $ head -n 2 chr17_fragm.fasta $ head -n 2 chr17_fragm.revcomp.fasta Create a table with the sequence length in the FASTA file. $ bioawk -c fastx '{print $name,length($seq)}' chr17_fragm.fasta > chr17_fragm.fasta.seqlen | head -n 10 chr17_fragm.fasta.seqlen $ head -n 10 chr17_fragm.fasta.seqlen","title":"Bioawk"},{"location":"Bio/bioawk/#bioawk","text":"Bioawk is an awk extension for biological formats written by Dr. Heng Li . See the documentation here .","title":"Bioawk"},{"location":"Bio/bioawk/#install-on-macos-with-the-homebrew-package-manager","text":"$ brew install bioawk","title":"Install on MacOS with the Homebrew package manager"},{"location":"Bio/bioawk/#conda","text":"conda install -c bioconda bioawk","title":"Conda"},{"location":"Bio/bioawk/#install-from-source","text":"$ git clone git://github.com/lh3/bioawk.git $ cd bioawk && make # Make sure that bioawk is in your $PATH or use the full path to call the executable. Several tutorial on the net suggest installing it with sudo in you system path - no need for that. This is another way to say - avoid unnecessary (it is bad practice) installation of tools as root. You will not be able to do it on any computer center anyway. If you get error about ... make: yacc: Command not found then you need install few packages (root privileges required) $ sudo apt-get install bison byacc","title":"Install from source"},{"location":"Bio/bioawk/#examples","text":"","title":"Examples"},{"location":"Bio/bioawk/#bioawk-supported-formats","text":"$ bioawk -c help bed: 1 :chrom 2 :start 3 :end 4 :name 5 :score 6 :strand 7 :thickstart 8 :thickend 9 :rgb 10 :blockcount 11 :blocksizes 12 :blockstarts sam: 1 :qname 2 :flag 3 :rname 4 :pos 5 :mapq 6 :cigar 7 :rnext 8 :pnext 9 :tlen 10 :seq 11 :qual vcf: 1 :chrom 2 :pos 3 :id 4 :ref 5 :alt 6 :qual 7 :filter 8 :info gff: 1 :seqname 2 :source 3 :feature 4 :start 5 :end 6 :score 7 :filter 8 :strand 9 :group 10 :attribute fastx: 1 :name 2 :seq 3 :qual 4 :comment","title":"bioawk supported formats"},{"location":"Bio/bioawk/#we-will-use-gtf-and-fasta-files-for-the-chr177400001-7800000-region-downloaded-using-the-ucsc-table-browser","text":"chr17_fragm.gtf chr17_fragm.fasta use the file chr17_fragm.gtf","title":"We will use GTF and FASTA files for the chr17:7400001-7800000 region, downloaded using the UCSC Table Browser."},{"location":"Bio/bioawk/#print-the-length-of-all-the-exons-end-position-start-position","text":"$ bioawk -c gff '$3 ~ /exon/ {print $seqname, $feature, $end-$start}' chr17_fragm.gtf | sort -nk3 use the file chr17_fragm.fasta","title":"Print the length of all the exons (end position - start position):"},{"location":"Bio/bioawk/#count-the-number-of-fasta-entries","text":"$ bioawk -c fastx 'END{print NR}' chr17_fragm.fasta","title":"Count the number of FASTA entries"},{"location":"Bio/bioawk/#reverse-complement-the-sequences","text":"$ bioawk -c fastx '{print \">\"$name\"\\n\"revcomp($seq)}' chr17_fragm.fasta > chr17_fragm.revcomp.fasta | head -n 2 $ head -n 2 chr17_fragm.fasta $ head -n 2 chr17_fragm.revcomp.fasta","title":"Reverse complement the sequences"},{"location":"Bio/bioawk/#create-a-table-with-the-sequence-length-in-the-fasta-file","text":"$ bioawk -c fastx '{print $name,length($seq)}' chr17_fragm.fasta > chr17_fragm.fasta.seqlen | head -n 10 chr17_fragm.fasta.seqlen $ head -n 10 chr17_fragm.fasta.seqlen","title":"Create a table with the sequence length in the FASTA file."},{"location":"Case_studies/CHGCAR_diff/","text":"Multiple files - VASP CHGCAR difference From time to time, I need to calculate electron charge difference that is tabulated on a regular 3D grid. Common examples in my field will be Gaussian .cube files or VASP CHGCAR files. One can find some scripts, tools and programs that can do this in one way or another. In my case, again, I need something slightly different and... Well, doing it with awk is so simple that I never use other tools but the one I will mention here. With small changes I am able to subtract 6 files at the same time, and since the files tend to be large, I keep them compressed. Some advantages of the script are: extremely small memory footprint ( the calculation is done line by line ) it does not need to know anything about the format, it only expects that all files have the same number of grid points ( in each direction ) the number of atoms in the CHGCAR could be different VASP-CHGCAR-diff.awk #!/usr/bin/awk -f BEGIN { cmd1 = \"bzcat \" ARGV [ 1 ]; cmd2 = \"bzcat \" ARGV [ 2 ]; cmd3 = \"bzcat \" ARGV [ 3 ]; NF = 1 ; while ( NF > 0 ){ cmd2 | & getline ; } cmd2 | & getline ; NF = 1 ; while ( NF > 0 ){ cmd3 | & getline ; } cmd3 | & getline ; NF = 1 ; while ( NF > 0 ){ cmd1 | & getline ; print } cmd1 | & getline ; print ; do { cmd1 | & getline ; split ( $ 0 , d1 ) cmd2 | & getline ; split ( $ 0 , d2 ) cmd3 | & getline ; split ( $ 0 , d3 ) for ( i = 1 ; i <= NF ; i ++ ) printf \"%18.11e \" , d1 [ i ] - d2 [ i ] - d3 [ i ] print \"\" } while ( $ 1 * 1 ==$ 1 ) } cmd1 , cmd2 and cmd3 are the commands that will read the compressed files. NF = 1 ; while ( NF > 0 ){ cmd2 | & getline ; } cmd2 | & getline ; will fast forward to the grid data. The lines in the first file will be used as first lines in the new file. The rest is just reading line by line and calculating the difference column by column until it reaches the end of the file. And here are some charge difference plots produced with Python Files VASP_CHGCAR_diff.awk plot-plane-v01.py","title":"Multiple files - VASP CHGCAR difference"},{"location":"Case_studies/CHGCAR_diff/#multiple-files-vasp-chgcar-difference","text":"From time to time, I need to calculate electron charge difference that is tabulated on a regular 3D grid. Common examples in my field will be Gaussian .cube files or VASP CHGCAR files. One can find some scripts, tools and programs that can do this in one way or another. In my case, again, I need something slightly different and... Well, doing it with awk is so simple that I never use other tools but the one I will mention here. With small changes I am able to subtract 6 files at the same time, and since the files tend to be large, I keep them compressed. Some advantages of the script are: extremely small memory footprint ( the calculation is done line by line ) it does not need to know anything about the format, it only expects that all files have the same number of grid points ( in each direction ) the number of atoms in the CHGCAR could be different VASP-CHGCAR-diff.awk #!/usr/bin/awk -f BEGIN { cmd1 = \"bzcat \" ARGV [ 1 ]; cmd2 = \"bzcat \" ARGV [ 2 ]; cmd3 = \"bzcat \" ARGV [ 3 ]; NF = 1 ; while ( NF > 0 ){ cmd2 | & getline ; } cmd2 | & getline ; NF = 1 ; while ( NF > 0 ){ cmd3 | & getline ; } cmd3 | & getline ; NF = 1 ; while ( NF > 0 ){ cmd1 | & getline ; print } cmd1 | & getline ; print ; do { cmd1 | & getline ; split ( $ 0 , d1 ) cmd2 | & getline ; split ( $ 0 , d2 ) cmd3 | & getline ; split ( $ 0 , d3 ) for ( i = 1 ; i <= NF ; i ++ ) printf \"%18.11e \" , d1 [ i ] - d2 [ i ] - d3 [ i ] print \"\" } while ( $ 1 * 1 ==$ 1 ) } cmd1 , cmd2 and cmd3 are the commands that will read the compressed files. NF = 1 ; while ( NF > 0 ){ cmd2 | & getline ; } cmd2 | & getline ; will fast forward to the grid data. The lines in the first file will be used as first lines in the new file. The rest is just reading line by line and calculating the difference column by column until it reaches the end of the file. And here are some charge difference plots produced with Python Files VASP_CHGCAR_diff.awk plot-plane-v01.py","title":"Multiple files - VASP CHGCAR difference"},{"location":"Case_studies/Dipole_moment/","text":"Dipole moment example Here is a simple script that will calculate the dipole moment of a molecule 1 . In this particular case I have used the result for a water molecule from a Wannier function localization calculation. To keep the script simple, I have added the charges of the species as 5 th column in the file. You can perhaps change it, so it recognizes O with charge of 6 valence electrons, H with \\(1e\\) , and X - Wannier center with charge of \\(-2e\\) . The coordinates are in Angstroms. Here is the code #!/usr/bin/awk -f # x,y,z in Angstroms NR > 2 { mx = mx + $ 2 *$ 5 ; my = my + $ 3 *$ 5 ; mz = mz + $ 4 *$ 5 ; } END { norm = sqrt ( mx ** 2 + my ** 2 + mz ** 2 ); # Convert to Debye toD = 4.80320425 ; mx = mx * toD ; my = my * toD ; mz = mz * toD ; norm = norm * toD ; printf ( \"Dipole Moment\\t x\\t\\t y\\t\\t z\\t\\t| D.Moment |\\n\" ); printf ( \" [ Debye ] \\t%f\\t%f\\t%f\\t%f\\n\" , mx , my , mz , norm ); } and here is the .xyz file with the charges added 7 Wannier centres, written by Wannier90 on 6Aug2013 at 14:23:10 X 6.50000026 6.18706578 6.50000000 -2 X 6.50000000 6.71337672 6.50000002 -2 X 6.50000008 6.95339960 6.49999998 -2 X 6.49999971 6.54636884 6.50000000 -2 H 6.50000000 7.09480000 7.26880000 1 H 6.50000000 7.09480000 5.73120000 1 O 6.50000000 6.50000000 6.50000000 6 Here is the output of the program $ ./dipole_moment.awk waterX.xyz Dipole Moment x y z | D.Moment | [ Debye ] -0.000000 1 .869302 0 .000000 1 .869302 This example should be easy to alter, so you can calculate ( if you want ) the center of the mass of the molecule, the geometrical center, radius of gyration ... Files dipole_moment.awk waterX.xyz Dipole moment of an array of charges \u21a9","title":"Dipole moment example"},{"location":"Case_studies/Dipole_moment/#dipole-moment-example","text":"Here is a simple script that will calculate the dipole moment of a molecule 1 . In this particular case I have used the result for a water molecule from a Wannier function localization calculation. To keep the script simple, I have added the charges of the species as 5 th column in the file. You can perhaps change it, so it recognizes O with charge of 6 valence electrons, H with \\(1e\\) , and X - Wannier center with charge of \\(-2e\\) . The coordinates are in Angstroms. Here is the code #!/usr/bin/awk -f # x,y,z in Angstroms NR > 2 { mx = mx + $ 2 *$ 5 ; my = my + $ 3 *$ 5 ; mz = mz + $ 4 *$ 5 ; } END { norm = sqrt ( mx ** 2 + my ** 2 + mz ** 2 ); # Convert to Debye toD = 4.80320425 ; mx = mx * toD ; my = my * toD ; mz = mz * toD ; norm = norm * toD ; printf ( \"Dipole Moment\\t x\\t\\t y\\t\\t z\\t\\t| D.Moment |\\n\" ); printf ( \" [ Debye ] \\t%f\\t%f\\t%f\\t%f\\n\" , mx , my , mz , norm ); } and here is the .xyz file with the charges added 7 Wannier centres, written by Wannier90 on 6Aug2013 at 14:23:10 X 6.50000026 6.18706578 6.50000000 -2 X 6.50000000 6.71337672 6.50000002 -2 X 6.50000008 6.95339960 6.49999998 -2 X 6.49999971 6.54636884 6.50000000 -2 H 6.50000000 7.09480000 7.26880000 1 H 6.50000000 7.09480000 5.73120000 1 O 6.50000000 6.50000000 6.50000000 6 Here is the output of the program $ ./dipole_moment.awk waterX.xyz Dipole Moment x y z | D.Moment | [ Debye ] -0.000000 1 .869302 0 .000000 1 .869302 This example should be easy to alter, so you can calculate ( if you want ) the center of the mass of the molecule, the geometrical center, radius of gyration ... Files dipole_moment.awk waterX.xyz Dipole moment of an array of charges \u21a9","title":"Dipole moment example"},{"location":"Case_studies/Discrete_histogram/","text":"Discrete histogram This is an improved version of the script that was mentioned in the introduction of the task to count the countries in the coin.txt file. The usual histogram is a graphical representation of the distribution of numerical data. That also implies that you have regular intervals for the range of the data. What if you want statistics over Months, days or any text - yes it is possible - remember the coins? It uses an undocumented feature that is rather useful. Here is the code, always check with the attached file which should contain the latest version. #!/usr/bin/awk -f BEGIN { if ( ! col ) col = 1 } { counts [ $ ( col )] ++ ; total ++ ; } END { for ( v in counts ) { printf \"%s %.0f %f \\n\" , v , counts [ v ], counts [ v ] / 0.01 / total ; } } Simply running the script over the file with the data (coins.txt) will calculate the distribution over the first column. $ ./histogram-discrete.awk coins.txt silver 4 30 .769231 gold 9 69 .230769 This finds that there are 4 silver coins and 9 gold in the file. The last column is the percentage. Now, for the trick (special thanks to my colleague Douglas Scofield for introducing me to this trick). If you add col=4 before the name of the input file it will be interpreted as variable assignment (providing that you do not have a file with this name ;-)). Here is the result - nothing is changed in the script. $ ./histogram-discrete.awk col = 4 coins.txt Switzerland 1 7 .692308 Canada 1 7 .692308 Austria-Hungary 1 7 .692308 PRC 1 7 .692308 RSA 2 15 .384615 USA 7 53 .846154 The official documentation says that you should write -v col=4 or --assign=col=4 so, keep this in mind. Notice that the output has no particular order. In the latest awk versions there is a way to force a particular order to the output but this is something I leave to you. You can always pipe the output via sort. Files coins.txt histogram-discrete.awk","title":"Discrete histogram"},{"location":"Case_studies/Discrete_histogram/#discrete-histogram","text":"This is an improved version of the script that was mentioned in the introduction of the task to count the countries in the coin.txt file. The usual histogram is a graphical representation of the distribution of numerical data. That also implies that you have regular intervals for the range of the data. What if you want statistics over Months, days or any text - yes it is possible - remember the coins? It uses an undocumented feature that is rather useful. Here is the code, always check with the attached file which should contain the latest version. #!/usr/bin/awk -f BEGIN { if ( ! col ) col = 1 } { counts [ $ ( col )] ++ ; total ++ ; } END { for ( v in counts ) { printf \"%s %.0f %f \\n\" , v , counts [ v ], counts [ v ] / 0.01 / total ; } } Simply running the script over the file with the data (coins.txt) will calculate the distribution over the first column. $ ./histogram-discrete.awk coins.txt silver 4 30 .769231 gold 9 69 .230769 This finds that there are 4 silver coins and 9 gold in the file. The last column is the percentage. Now, for the trick (special thanks to my colleague Douglas Scofield for introducing me to this trick). If you add col=4 before the name of the input file it will be interpreted as variable assignment (providing that you do not have a file with this name ;-)). Here is the result - nothing is changed in the script. $ ./histogram-discrete.awk col = 4 coins.txt Switzerland 1 7 .692308 Canada 1 7 .692308 Austria-Hungary 1 7 .692308 PRC 1 7 .692308 RSA 2 15 .384615 USA 7 53 .846154 The official documentation says that you should write -v col=4 or --assign=col=4 so, keep this in mind. Notice that the output has no particular order. In the latest awk versions there is a way to force a particular order to the output but this is something I leave to you. You can always pipe the output via sort. Files coins.txt histogram-discrete.awk","title":"Discrete histogram"},{"location":"Case_studies/Fasta_tips/","text":"Fasta file format tips With multi-fasta files, it is often required to extract few fasta sequences which contain the keyword/s of interest. One fast way to do this, is by awk. data.fa >Chr1 ATCTGCTGCTCGGGCTGCTCTAT... >Chr2 GTACGTCGTAGGACATGCATCG... >MT1 TACGATCGATCAGCTCAGCATC... >MT2 CGCCATGGATCAGCTACATGTA... $ awk 'BEGIN {RS=\">\"} /Chr2/ {print \">\"$0}' data . fa Note that in the BEGIN section of the script, we have redefined the internal variable for the record separator RS = \">\" which by default is \"new line\". This way, awk will treat the whole fasta (multi-line) record as one record. output: >Chr2 GTACGTCGTAGGACATGCATCG... example taken from: link","title":"Fasta file format tips"},{"location":"Case_studies/Fasta_tips/#fasta-file-format-tips","text":"With multi-fasta files, it is often required to extract few fasta sequences which contain the keyword/s of interest. One fast way to do this, is by awk. data.fa >Chr1 ATCTGCTGCTCGGGCTGCTCTAT... >Chr2 GTACGTCGTAGGACATGCATCG... >MT1 TACGATCGATCAGCTCAGCATC... >MT2 CGCCATGGATCAGCTACATGTA... $ awk 'BEGIN {RS=\">\"} /Chr2/ {print \">\"$0}' data . fa Note that in the BEGIN section of the script, we have redefined the internal variable for the record separator RS = \">\" which by default is \"new line\". This way, awk will treat the whole fasta (multi-line) record as one record. output: >Chr2 GTACGTCGTAGGACATGCATCG... example taken from: link","title":"Fasta file format tips"},{"location":"Case_studies/Gaussian-extract-geometry/","text":"Gaussian - extract geometry from output .log file It is rather common problem that one wants to extract the final geometry from a Gaussian calculation. It is possible to do it with GaussView, molden... But when you have hundreds of geometries... Here is a solution using Open Babel $ obabel -ig09 gaussian_output.log -oxyz 8 gaussian_output.log Energy: -190290.8876477 O 1 .48397 0 .74575 -0.00894 H 0 .54487 0 .98777 0 .10858 O 1 .36176 -0.69934 -0.11156 H 1 .80921 -0.98067 0 .69811 O -1.48420 -0.74573 0 .00817 H -0.54547 -0.98830 -0.11124 O -1.36140 0 .69926 0 .11245 H -1.80957 0 .98168 -0.69645 1 molecule converted Which will find the last geometry (not sure what happens if the point is not stationary) and will write out the standard orientation of the molecule. Here is a solution in awk, which gives you the option to select which orientation you want: Input or Standard orientation By default the script will print the last geometry in the output.log file in \"Standard orientation\". $ GAUSSIAN-log2xyz.awk gaussian_output.log 8 XYZ -Stationary point- [ Standard orientation ] extracted from: gaussian_output.log O 1 .48396700 0 .74574700 -0.00893700 H 0 .54487000 0 .98777200 0 .10857600 O 1 .36176000 -0.69934000 -0.11155800 H 1 .80920900 -0.98067200 0 .69811200 O -1.48420500 -0.74573000 0 .00817100 H -0.54547200 -0.98829500 -0.11124500 O -1.36140100 0 .69926200 0 .11245000 H -1.80957500 0 .98167900 -0.69645200 To print in the Input orientation add outf=0 before the output.log i.e. $ GAUSSIAN-log2xyz.awk outf = 0 gaussian_output.log 8 XYZ -Stationary point- [ Input orientation ] extracted from: gaussian_output.log O -0.70140300 -0.20578800 0 .09097700 H -1.11738400 0 .67789200 0 .07209400 O 0 .69843900 0 .17126700 -0.01863500 H 0 .91917500 -0.18830400 -0.88864000 O 0 .23362800 2 .98135600 0 .03930000 H 0 .65037800 2 .09806100 0 .06000100 O -1.16628300 2 .60379100 0 .14722400 H -1.38832100 2 .96390800 1 .01668100 GAUSSIAN-log2xyz.awk #!/bin/awk -f BEGIN { ss = \"H,He,Li,Be,B,C,N,O,F,Ne,Na,Mg,Al,Si,P,S,Cl,Ar,K,Ca,Sc,Ti,V,Cr,Mn,Fe,Co,Ni,Cu,Zn,Ga,Ge,As,Se,Br,Kr,Rb,Sr,Y,Zr,Nb,Mo,Tc,Ru,Rh,Pd,Ag,Cd,In,Sn,Sb,Te,I,Xe,Cs,Ba,La,Ce,Pr,Nd,Pm,Sm,Eu,Gd,Tb,Dy,Ho,Er,Tm,Yb,Lu,Hf,Ta,W,Re,Os,Ir,Pt,Au,Hg,Tl,Pb,Bi,Po,At,Rn,Fr,Ra,Ac,Th,Pa,U,Np,Pu,Am,Cm,Bk,Cf,Es,Fm,Md,No,Lr,Rf,Ha,D\" split ( ss , atsym , \",\" ) outf = 1 # Default output format orient [ 0 ] = \"Input orientation\" orient [ 1 ] = \"Standard orientation\" } /Stationary point found/ { SP = 1 ; # found stationary point SPtxt = \" -Stationary point- \" # add info to the second line } $ 0 ~ orient [ outf ] && ( SP < 2 ) { if ( SP == 1 ) SP = 2 # stop looking for geometry output getline ; getline ; getline ; getline ; getline ; iat = 0 do { iat ++ atn [ iat ] =$ 2 ; x [ iat ] =$ 4 ; y [ iat ] =$ 5 ; z [ iat ] =$ 6 getline ; } while ( NF != 1 ) } END { print iat \"\\n XYZ \" SPtxt \"[\" orient [ outf ] \"] extracted from: \" FILENAME for ( i = 1 ; i <= iat ; i ++ ) printf ( \"%4s %14.8f %14.8f %14.8f\\n\" , atsym [ atn [ i ]], x [ i ], y [ i ], z [ i ]) } Files GAUSSIAN-log2xyz.awk","title":"Gaussian extract geometry"},{"location":"Case_studies/Gaussian-extract-geometry/#gaussian-extract-geometry-from-output-log-file","text":"It is rather common problem that one wants to extract the final geometry from a Gaussian calculation. It is possible to do it with GaussView, molden... But when you have hundreds of geometries... Here is a solution using Open Babel $ obabel -ig09 gaussian_output.log -oxyz 8 gaussian_output.log Energy: -190290.8876477 O 1 .48397 0 .74575 -0.00894 H 0 .54487 0 .98777 0 .10858 O 1 .36176 -0.69934 -0.11156 H 1 .80921 -0.98067 0 .69811 O -1.48420 -0.74573 0 .00817 H -0.54547 -0.98830 -0.11124 O -1.36140 0 .69926 0 .11245 H -1.80957 0 .98168 -0.69645 1 molecule converted Which will find the last geometry (not sure what happens if the point is not stationary) and will write out the standard orientation of the molecule. Here is a solution in awk, which gives you the option to select which orientation you want: Input or Standard orientation By default the script will print the last geometry in the output.log file in \"Standard orientation\". $ GAUSSIAN-log2xyz.awk gaussian_output.log 8 XYZ -Stationary point- [ Standard orientation ] extracted from: gaussian_output.log O 1 .48396700 0 .74574700 -0.00893700 H 0 .54487000 0 .98777200 0 .10857600 O 1 .36176000 -0.69934000 -0.11155800 H 1 .80920900 -0.98067200 0 .69811200 O -1.48420500 -0.74573000 0 .00817100 H -0.54547200 -0.98829500 -0.11124500 O -1.36140100 0 .69926200 0 .11245000 H -1.80957500 0 .98167900 -0.69645200 To print in the Input orientation add outf=0 before the output.log i.e. $ GAUSSIAN-log2xyz.awk outf = 0 gaussian_output.log 8 XYZ -Stationary point- [ Input orientation ] extracted from: gaussian_output.log O -0.70140300 -0.20578800 0 .09097700 H -1.11738400 0 .67789200 0 .07209400 O 0 .69843900 0 .17126700 -0.01863500 H 0 .91917500 -0.18830400 -0.88864000 O 0 .23362800 2 .98135600 0 .03930000 H 0 .65037800 2 .09806100 0 .06000100 O -1.16628300 2 .60379100 0 .14722400 H -1.38832100 2 .96390800 1 .01668100 GAUSSIAN-log2xyz.awk #!/bin/awk -f BEGIN { ss = \"H,He,Li,Be,B,C,N,O,F,Ne,Na,Mg,Al,Si,P,S,Cl,Ar,K,Ca,Sc,Ti,V,Cr,Mn,Fe,Co,Ni,Cu,Zn,Ga,Ge,As,Se,Br,Kr,Rb,Sr,Y,Zr,Nb,Mo,Tc,Ru,Rh,Pd,Ag,Cd,In,Sn,Sb,Te,I,Xe,Cs,Ba,La,Ce,Pr,Nd,Pm,Sm,Eu,Gd,Tb,Dy,Ho,Er,Tm,Yb,Lu,Hf,Ta,W,Re,Os,Ir,Pt,Au,Hg,Tl,Pb,Bi,Po,At,Rn,Fr,Ra,Ac,Th,Pa,U,Np,Pu,Am,Cm,Bk,Cf,Es,Fm,Md,No,Lr,Rf,Ha,D\" split ( ss , atsym , \",\" ) outf = 1 # Default output format orient [ 0 ] = \"Input orientation\" orient [ 1 ] = \"Standard orientation\" } /Stationary point found/ { SP = 1 ; # found stationary point SPtxt = \" -Stationary point- \" # add info to the second line } $ 0 ~ orient [ outf ] && ( SP < 2 ) { if ( SP == 1 ) SP = 2 # stop looking for geometry output getline ; getline ; getline ; getline ; getline ; iat = 0 do { iat ++ atn [ iat ] =$ 2 ; x [ iat ] =$ 4 ; y [ iat ] =$ 5 ; z [ iat ] =$ 6 getline ; } while ( NF != 1 ) } END { print iat \"\\n XYZ \" SPtxt \"[\" orient [ outf ] \"] extracted from: \" FILENAME for ( i = 1 ; i <= iat ; i ++ ) printf ( \"%4s %14.8f %14.8f %14.8f\\n\" , atsym [ atn [ i ]], x [ i ], y [ i ], z [ i ]) } Files GAUSSIAN-log2xyz.awk","title":"Gaussian - extract geometry from output .log file"},{"location":"Case_studies/Gaussian_smearing/","text":"Gaussian smearing Sometimes it is useful or necessary to apply Gaussian smearing on your discrete values. For example if you want to add temperature broadening on theoretically calculated spectra (from Lattice Dynamics, normal mode analysis etc.). Here is a code that shows how it could be done in awk ( illustrating the use of functions as well ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #!/usr/bin/awk -f BEGIN { FWHM = 30 ; # Default smearing if none is provided on the command line FWHM = FWHM / 2.35482 # trick to read parameters from the command line if ( ARGC == 3 ) FWHM = ARGV [ 2 ]; ARGC = 2 ; } ! /#/ { f ++ ; if ( f == 1 ) { fmin =$ 1 ; fmax =$ 1 } freq [ f ] =$ 1 ; if ( fmin > $ 1 ) fmin =$ 1 ; if ( fmax < $ 1 ) fmax =$ 1 ; nfreq = f ; } END { print \"# freq intensity | nfreq: \" nfreq \" fmin: \" fmin \" fmax: \" fmax for ( i = int ( fmin - 5 * FWHM ); i <= int ( fmax + 5 * FWHM ); i ++ ){ for ( f = 1 ; f <= nfreq ; f ++ ){ data [ i ] = data [ i ] + gauss ( freq [ f ], i , FWHM ); } print i , data [ i ] } } function gauss ( x0 , x , c ){ area = 1 ; if (( x - x0 ) ** 2 < 10000 ) { return area * exp ( - ((( x - x0 )) ** 2 ) / ( 2 . * c ** 2 ))} else { return 0.0 } } Here are a couple of interesting ( or not ) points in the code. In the #awk BEGIN section, we define default FWHM value in case none is provided on the command line. There are minimum checks that facilitate the command line input. When the script is executed, the argument values are stored in ARGV array and the number of arguments in ARGC . The zeroth element is the script's name itself. 1 st will have the first parameter and so on. So, if ARGC == 3 (0, 1 and 2 will count as 3) then we set the second argument as FWHM in our script and decrease the ARGC back to 2 to trick the script that we have only 2 parameters on the command line. Otherwise, awk will try to read our 3 rd parameter as a regular file. Then, for each line that is not a comment, the script reads the value in the first column and store it locally, while trying to keep track of the smallest and greatest value. The END section, essentially runs a loop over a range slightly larger than fmax-fmin and in turns for (f=1;f<=nfreq;f++) calculates Gaussian contribution from each peak. The last section in the script is a user defined function for the Gaussian function with small precautions to avoid errors when the numbers become unreasonably small. To run the script: ./Gauss-smearing.awk freq.dat 10 Here is the result from a Gnuplot script that calls the awk script directly, that avoids unnecessary creation of temporary files. All scripts and data files are attached bellow. Files gauss-smear-data.awk freq.dat ps-plot-v01.gnu - Gnuplot script Comment The script requires only small changes to handle intensities as well, can you do it yourself?","title":"Gaussian smearing"},{"location":"Case_studies/Gaussian_smearing/#gaussian-smearing","text":"Sometimes it is useful or necessary to apply Gaussian smearing on your discrete values. For example if you want to add temperature broadening on theoretically calculated spectra (from Lattice Dynamics, normal mode analysis etc.). Here is a code that shows how it could be done in awk ( illustrating the use of functions as well ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #!/usr/bin/awk -f BEGIN { FWHM = 30 ; # Default smearing if none is provided on the command line FWHM = FWHM / 2.35482 # trick to read parameters from the command line if ( ARGC == 3 ) FWHM = ARGV [ 2 ]; ARGC = 2 ; } ! /#/ { f ++ ; if ( f == 1 ) { fmin =$ 1 ; fmax =$ 1 } freq [ f ] =$ 1 ; if ( fmin > $ 1 ) fmin =$ 1 ; if ( fmax < $ 1 ) fmax =$ 1 ; nfreq = f ; } END { print \"# freq intensity | nfreq: \" nfreq \" fmin: \" fmin \" fmax: \" fmax for ( i = int ( fmin - 5 * FWHM ); i <= int ( fmax + 5 * FWHM ); i ++ ){ for ( f = 1 ; f <= nfreq ; f ++ ){ data [ i ] = data [ i ] + gauss ( freq [ f ], i , FWHM ); } print i , data [ i ] } } function gauss ( x0 , x , c ){ area = 1 ; if (( x - x0 ) ** 2 < 10000 ) { return area * exp ( - ((( x - x0 )) ** 2 ) / ( 2 . * c ** 2 ))} else { return 0.0 } } Here are a couple of interesting ( or not ) points in the code. In the #awk BEGIN section, we define default FWHM value in case none is provided on the command line. There are minimum checks that facilitate the command line input. When the script is executed, the argument values are stored in ARGV array and the number of arguments in ARGC . The zeroth element is the script's name itself. 1 st will have the first parameter and so on. So, if ARGC == 3 (0, 1 and 2 will count as 3) then we set the second argument as FWHM in our script and decrease the ARGC back to 2 to trick the script that we have only 2 parameters on the command line. Otherwise, awk will try to read our 3 rd parameter as a regular file. Then, for each line that is not a comment, the script reads the value in the first column and store it locally, while trying to keep track of the smallest and greatest value. The END section, essentially runs a loop over a range slightly larger than fmax-fmin and in turns for (f=1;f<=nfreq;f++) calculates Gaussian contribution from each peak. The last section in the script is a user defined function for the Gaussian function with small precautions to avoid errors when the numbers become unreasonably small. To run the script: ./Gauss-smearing.awk freq.dat 10 Here is the result from a Gnuplot script that calls the awk script directly, that avoids unnecessary creation of temporary files. All scripts and data files are attached bellow. Files gauss-smear-data.awk freq.dat ps-plot-v01.gnu - Gnuplot script Comment The script requires only small changes to handle intensities as well, can you do it yourself?","title":"Gaussian smearing"},{"location":"Case_studies/Linear_interpolation/","text":"Linear interpolation Here is a problem that occurs often. One have a set of data files that contain 2 columns (x and y) for some calculated property. Here I have selected only two such sets. I want to be able to add them, subtract them or make an average. So, what is the problem? The problem is, that the data is sampled in the same region but on different grids i.e. the number of points is different. 1.dat 0.00 0.151576740 6.51 0.157566880 13.03 0.175107240 19.54 0.202843140 ... 3967.59 -0.000073200 3974.10 -0.000082352 3980.62 -0.000095120 3987.13 -0.000107504 2.dat 0.00 0.060664982 7.41 0.063726254 14.83 0.072136290 22.24 0.083837971 ... 3965.71 -0.000059534 3973.12 -0.000055689 3980.53 -0.000048746 3987.94 -0.000041886 One needs to have the values on the same grid to make some simple calculations. I am interested in the region x in [1500:4000] and with a small awk script I can do on-the-fly linear interpolation (which is good enough in this particular case). Here is how the interpolated data compares to the original. 1_new.dat 1500 0.00488383 1501 0.0050328 1502 0.00518177 1503 0.00533075 ... 2_new.dat 1500 0.00410784 1501 0.00411207 1502 0.0041163 1503 0.00412054 ... The script is running over integer numbers for easier understanding, but could be easily modified to handle real numbers for the grid values. interpolate-on-regular-grid.awk #!/usr/bin/awk -f BEGIN { minx = 1500 ; # Range: lower limit maxx = 4000 ; # upper limit dx = 1 ; # increment } { # check if the lower limit has been reached if (( $ 1 > minx ) && ( $ 1 < maxx )){ x2 =$ 1 ; y2 =$ 2 ; # for the defined range for ( xi = minx ; xi <= maxx ; xi = xi + dx ){ # step forward if the current grid point is outside the [x1:x2] region if ( xi > x2 ) { while (( $ 1 < xi ) && ( getline != 0 ) ) { x1 = x2 ; y1 = y2 ; x2 =$ 1 ; y2 =$ 2 } } yi = ( y2 - y1 ) / ( x2 - x1 ) * ( xi - x1 ) + y1 ; # linear interpolation print xi , yi } } x1 = $ 1 ; y1 =$ 2 } Here is the result: Warning I have written the script for my purposes and tested it against my data. It works for all of my cases, but could fail in some unforeseen situations so, please, always make sure that your results are reasonable. Do not blindly use the scripts provided on this site. Files interpolate-on-regular-grid.awk 1.dat 2.dat 1_new.dat 2_new.dat","title":"Linear interpolation"},{"location":"Case_studies/Linear_interpolation/#linear-interpolation","text":"Here is a problem that occurs often. One have a set of data files that contain 2 columns (x and y) for some calculated property. Here I have selected only two such sets. I want to be able to add them, subtract them or make an average. So, what is the problem? The problem is, that the data is sampled in the same region but on different grids i.e. the number of points is different. 1.dat 0.00 0.151576740 6.51 0.157566880 13.03 0.175107240 19.54 0.202843140 ... 3967.59 -0.000073200 3974.10 -0.000082352 3980.62 -0.000095120 3987.13 -0.000107504 2.dat 0.00 0.060664982 7.41 0.063726254 14.83 0.072136290 22.24 0.083837971 ... 3965.71 -0.000059534 3973.12 -0.000055689 3980.53 -0.000048746 3987.94 -0.000041886 One needs to have the values on the same grid to make some simple calculations. I am interested in the region x in [1500:4000] and with a small awk script I can do on-the-fly linear interpolation (which is good enough in this particular case). Here is how the interpolated data compares to the original. 1_new.dat 1500 0.00488383 1501 0.0050328 1502 0.00518177 1503 0.00533075 ... 2_new.dat 1500 0.00410784 1501 0.00411207 1502 0.0041163 1503 0.00412054 ... The script is running over integer numbers for easier understanding, but could be easily modified to handle real numbers for the grid values. interpolate-on-regular-grid.awk #!/usr/bin/awk -f BEGIN { minx = 1500 ; # Range: lower limit maxx = 4000 ; # upper limit dx = 1 ; # increment } { # check if the lower limit has been reached if (( $ 1 > minx ) && ( $ 1 < maxx )){ x2 =$ 1 ; y2 =$ 2 ; # for the defined range for ( xi = minx ; xi <= maxx ; xi = xi + dx ){ # step forward if the current grid point is outside the [x1:x2] region if ( xi > x2 ) { while (( $ 1 < xi ) && ( getline != 0 ) ) { x1 = x2 ; y1 = y2 ; x2 =$ 1 ; y2 =$ 2 } } yi = ( y2 - y1 ) / ( x2 - x1 ) * ( xi - x1 ) + y1 ; # linear interpolation print xi , yi } } x1 = $ 1 ; y1 =$ 2 } Here is the result: Warning I have written the script for my purposes and tested it against my data. It works for all of my cases, but could fail in some unforeseen situations so, please, always make sure that your results are reasonable. Do not blindly use the scripts provided on this site. Files interpolate-on-regular-grid.awk 1.dat 2.dat 1_new.dat 2_new.dat","title":"Linear interpolation"},{"location":"Case_studies/List/","text":"Case studies Here is a collection of mine and contributed awk scripts. General topic Awk and Jmol awk is using input data to write java script code to plot vectors in Jmol Multiple input files - first approach awk collects and assembles data from multiple files in memory Multiple input files - second approach awk collects data from multiple files but picks only the necessary data to save on memory Multiple output files MUST KNOW feature - covered during the workshop Color output with custom keywords use simple awk script to highlight keywords in your output Bioinformatics oriented bioawk Bioawk is an extension to Brian Kernighan's awk, adding the support of several common biological data formats, including optionally gzip'ed BED, GFF, SAM, VCF, FASTA/Q and TAB-delimited formats with column names. Fasta file format tips worth to know if working often with files in multi-fasta format Multiline fasta to single line fasta single cryptic-looking line that will decriphered during the workshop Sequence clustering with awk apllication of the multiple files approach - contribution by Mart\u00edn Gonz\u00e1lez Buitr\u00f3n Substitute scientific with common species names in a phylogenetic tree file Statistics on very large columns of values Manipulating and getting statistics for .vcf and .gff files Math oriented Discrete histogram very handy discrete histogram awk code Gaussian smearing trivial task done with awk - example how to use functions Linear interpolation use linear interpolation to resample your date on different grid Physics oriented Dipole moment example simple calulations should not be difficult to code - here is an example Multiple files - VASP CHGCAR difference an simplified example on how to read multiple files (bzip-ed) line-by-line simultaneously to save memory POSCAR: reorder atom types simple task creates programing nightmare Primarily used as reference Awk and Gnuplot outdated problem but shows hot to send data to another program and read back the results Awk writes Python collecting your data might be so tedious to program, that you might wamt to use awk to write python instead Gaussian - extract geometry from output .log file example ProLiant Status check simple check on some values, collected and mailed Awk and Networking awk have some advance network protocol capabilities...","title":"= Case Studies ="},{"location":"Case_studies/List/#case-studies","text":"Here is a collection of mine and contributed awk scripts.","title":"Case studies"},{"location":"Case_studies/List/#general-topic","text":"Awk and Jmol awk is using input data to write java script code to plot vectors in Jmol Multiple input files - first approach awk collects and assembles data from multiple files in memory Multiple input files - second approach awk collects data from multiple files but picks only the necessary data to save on memory Multiple output files MUST KNOW feature - covered during the workshop Color output with custom keywords use simple awk script to highlight keywords in your output","title":"General topic"},{"location":"Case_studies/List/#bioinformatics-oriented","text":"bioawk Bioawk is an extension to Brian Kernighan's awk, adding the support of several common biological data formats, including optionally gzip'ed BED, GFF, SAM, VCF, FASTA/Q and TAB-delimited formats with column names. Fasta file format tips worth to know if working often with files in multi-fasta format Multiline fasta to single line fasta single cryptic-looking line that will decriphered during the workshop Sequence clustering with awk apllication of the multiple files approach - contribution by Mart\u00edn Gonz\u00e1lez Buitr\u00f3n Substitute scientific with common species names in a phylogenetic tree file Statistics on very large columns of values Manipulating and getting statistics for .vcf and .gff files","title":"Bioinformatics oriented"},{"location":"Case_studies/List/#math-oriented","text":"Discrete histogram very handy discrete histogram awk code Gaussian smearing trivial task done with awk - example how to use functions Linear interpolation use linear interpolation to resample your date on different grid","title":"Math oriented"},{"location":"Case_studies/List/#physics-oriented","text":"Dipole moment example simple calulations should not be difficult to code - here is an example Multiple files - VASP CHGCAR difference an simplified example on how to read multiple files (bzip-ed) line-by-line simultaneously to save memory POSCAR: reorder atom types simple task creates programing nightmare","title":"Physics oriented"},{"location":"Case_studies/List/#primarily-used-as-reference","text":"Awk and Gnuplot outdated problem but shows hot to send data to another program and read back the results Awk writes Python collecting your data might be so tedious to program, that you might wamt to use awk to write python instead Gaussian - extract geometry from output .log file example ProLiant Status check simple check on some values, collected and mailed Awk and Networking awk have some advance network protocol capabilities...","title":"Primarily used as reference"},{"location":"Case_studies/Multi2single_fasta/","text":"Multiline Fasta To Single Line Fasta I got the question, solved it, then I realized it is rather common problem. So, I googled for it and the solution, I have to admit, was better than mine. Here is the solution and link to the original source. I have a fasta file with following format >gi|321257144|ref|XP_003193485.1| flap endonuclease [Cryptococcus gattii WM276] MGIKGLTGLLSENAPKCMKDHEMKTLFGRKVAIDASMSIYQFLIAVRQQDGQMLMNESGDVTSHLMGFFY RTIRMVDHGIKPCYIFDGKPPELKGSVLAKRFARREEAKEGEEEAKETGTAEDVDKLARRQVRVTREHNE ECKKLLSLMGIPVVTAPGEAEAQCAELARAGKVYAAGSEDMDTLTFHSPILLRHLTFSEAKKMPISEIHL DVALRDLEMSMDQFIELCILLGCDYLEPCKGIGPKTALKLMREHGTLGKVVEHIRGKMAEKAEEIKAAAD EEAEAEAEAEKYDSDPENEEGGETMINSDGEEVPAPSKPKSPKKKAPAKKKKIASSGMQIPEFWPWEEAK QLFLKPDVVNGDDLVLEWKQPDTEGLVEFLCRDKGFNEDRVRAGAAKLSKMLAAKQQGRLDGFFTVKPKE PAAKDAGKGKGKDTKGEKRKAEEKGAAKKKTKK >gi|321473340|gb|EFX84308.1| hypothetical protein DAPPUDRAFT_47502 [Daphnia pulex] MGIKGLTQVIGDTAPTAIKENEIKNYFGRKVAIDASMSIYQFLIAVRSEGAMLTSADGETTSHLMGIFYR TIRMVDNGIKPVYVFDGKPPDMKGGELTKRAEKREEASKQLVLATDAGDAVEMEKMNKRLVKVNKGHTDE CKQLLTLMGIPYVEAPCEAEAQCAALVKAGKVYATATEDMDSLTFGSNVLLRYLTYSEAKKMPIKEFHLD KILDGLSYTMDEFIDLCIMLGCDYCDTIKGIGAKRAKELIDKHRCIEKVIENLDTKKYTVPENWPYQEAR RLFKTPDVADAETLDLKWTQPDEEGLVKFMCGDKNFNEERIRSGAKKLCKAKTGQTQGRLDSFFKVLPSS KPSTPSTPASKRKVGCIIYLFLYF but I wanna to look sequence in a single line, not in many line as they are. any quick method?? using awk: awk '/^>/ {printf(\"\\n%s\\n\",$0);next; } { printf(\"%s\",$0);} END {printf(\"\\n\");}' < file . fa > gi | 321257144 | ref | XP_003193485 . 1 | flap endonuclease [ Cryptococcus gattii WM276 ] MGIKGLTGLLSENAPKCMKDHEMKTLFGRKVAIDASMSIYQFLIAVRQQDGQMLMNESGDVTSHLMGFFYRTIRMVDHGIKPCYIFDGKPPELKGSVLAKRFARREEAKEGEEEAKETGTAEDVDKLARRQVRVTREHNEECKKLLSLMGIPVVTAPGEAEAQCAELARAGKVYAAGSEDMDTLTFHSPILLRHLTFSEAKKMPISEIHLDVALRDLEMSMDQFIELCILLGCDYLEPCKGIGPKTALKLMREHGTLGKVVEHIRGKMAEKAEEIKAAADEEAEAEAEAEKYDSDPENEEGGETMINSDGEEVPAPSKPKSPKKKAPAKKKKIASSGMQIPEFWPWEEAKQLFLKPDVVNGDDLVLEWKQPDTEGLVEFLCRDKGFNEDRVRAGAAKLSKMLAAKQQGRLDGFFTVKPKEPAAKDAGKGKGKDTKGEKRKAEEKGAAKKKTKK > gi | 321473340 | gb | EFX84308 . 1 | hypothetical protein DAPPUDRAFT_47502 [ Daphnia pulex ] MGIKGLTQVIGDTAPTAIKENEIKNYFGRKVAIDASMSIYQFLIAVRSEGAMLTSADGETTSHLMGIFYRTIRMVDNGIKPVYVFDGKPPDMKGGELTKRAEKREEASKQLVLATDAGDAVEMEKMNKRLVKVNKGHTDECKQLLTLMGIPYVEAPCEAEAQCAALVKAGKVYATATEDMDSLTFGSNVLLRYLTYSEAKKMPIKEFHLDKILDGLSYTMDEFIDLCIMLGCDYCDTIKGIGAKRAKELIDKHRCIEKVIENLDTKKYTVPENWPYQEARRLFKTPDVADAETLDLKWTQPDEEGLVKFMCGDKNFNEERIRSGAKKLCKAKTGQTQGRLDSFFKVLPSSKPSTPSTPASKRKVGCIIYLFLYF Note Note that there is a blank line at the beginning of the output - this is further discusses in the original forum post. This example case will be discussed during the workshop","title":"Multiline Fasta To Single Line Fasta"},{"location":"Case_studies/Multi2single_fasta/#multiline-fasta-to-single-line-fasta","text":"I got the question, solved it, then I realized it is rather common problem. So, I googled for it and the solution, I have to admit, was better than mine. Here is the solution and link to the original source. I have a fasta file with following format >gi|321257144|ref|XP_003193485.1| flap endonuclease [Cryptococcus gattii WM276] MGIKGLTGLLSENAPKCMKDHEMKTLFGRKVAIDASMSIYQFLIAVRQQDGQMLMNESGDVTSHLMGFFY RTIRMVDHGIKPCYIFDGKPPELKGSVLAKRFARREEAKEGEEEAKETGTAEDVDKLARRQVRVTREHNE ECKKLLSLMGIPVVTAPGEAEAQCAELARAGKVYAAGSEDMDTLTFHSPILLRHLTFSEAKKMPISEIHL DVALRDLEMSMDQFIELCILLGCDYLEPCKGIGPKTALKLMREHGTLGKVVEHIRGKMAEKAEEIKAAAD EEAEAEAEAEKYDSDPENEEGGETMINSDGEEVPAPSKPKSPKKKAPAKKKKIASSGMQIPEFWPWEEAK QLFLKPDVVNGDDLVLEWKQPDTEGLVEFLCRDKGFNEDRVRAGAAKLSKMLAAKQQGRLDGFFTVKPKE PAAKDAGKGKGKDTKGEKRKAEEKGAAKKKTKK >gi|321473340|gb|EFX84308.1| hypothetical protein DAPPUDRAFT_47502 [Daphnia pulex] MGIKGLTQVIGDTAPTAIKENEIKNYFGRKVAIDASMSIYQFLIAVRSEGAMLTSADGETTSHLMGIFYR TIRMVDNGIKPVYVFDGKPPDMKGGELTKRAEKREEASKQLVLATDAGDAVEMEKMNKRLVKVNKGHTDE CKQLLTLMGIPYVEAPCEAEAQCAALVKAGKVYATATEDMDSLTFGSNVLLRYLTYSEAKKMPIKEFHLD KILDGLSYTMDEFIDLCIMLGCDYCDTIKGIGAKRAKELIDKHRCIEKVIENLDTKKYTVPENWPYQEAR RLFKTPDVADAETLDLKWTQPDEEGLVKFMCGDKNFNEERIRSGAKKLCKAKTGQTQGRLDSFFKVLPSS KPSTPSTPASKRKVGCIIYLFLYF but I wanna to look sequence in a single line, not in many line as they are. any quick method?? using awk: awk '/^>/ {printf(\"\\n%s\\n\",$0);next; } { printf(\"%s\",$0);} END {printf(\"\\n\");}' < file . fa > gi | 321257144 | ref | XP_003193485 . 1 | flap endonuclease [ Cryptococcus gattii WM276 ] MGIKGLTGLLSENAPKCMKDHEMKTLFGRKVAIDASMSIYQFLIAVRQQDGQMLMNESGDVTSHLMGFFYRTIRMVDHGIKPCYIFDGKPPELKGSVLAKRFARREEAKEGEEEAKETGTAEDVDKLARRQVRVTREHNEECKKLLSLMGIPVVTAPGEAEAQCAELARAGKVYAAGSEDMDTLTFHSPILLRHLTFSEAKKMPISEIHLDVALRDLEMSMDQFIELCILLGCDYLEPCKGIGPKTALKLMREHGTLGKVVEHIRGKMAEKAEEIKAAADEEAEAEAEAEKYDSDPENEEGGETMINSDGEEVPAPSKPKSPKKKAPAKKKKIASSGMQIPEFWPWEEAKQLFLKPDVVNGDDLVLEWKQPDTEGLVEFLCRDKGFNEDRVRAGAAKLSKMLAAKQQGRLDGFFTVKPKEPAAKDAGKGKGKDTKGEKRKAEEKGAAKKKTKK > gi | 321473340 | gb | EFX84308 . 1 | hypothetical protein DAPPUDRAFT_47502 [ Daphnia pulex ] MGIKGLTQVIGDTAPTAIKENEIKNYFGRKVAIDASMSIYQFLIAVRSEGAMLTSADGETTSHLMGIFYRTIRMVDNGIKPVYVFDGKPPDMKGGELTKRAEKREEASKQLVLATDAGDAVEMEKMNKRLVKVNKGHTDECKQLLTLMGIPYVEAPCEAEAQCAALVKAGKVYATATEDMDSLTFGSNVLLRYLTYSEAKKMPIKEFHLDKILDGLSYTMDEFIDLCIMLGCDYCDTIKGIGAKRAKELIDKHRCIEKVIENLDTKKYTVPENWPYQEARRLFKTPDVADAETLDLKWTQPDEEGLVKFMCGDKNFNEERIRSGAKKLCKAKTGQTQGRLDSFFKVLPSSKPSTPSTPASKRKVGCIIYLFLYF Note Note that there is a blank line at the beginning of the output - this is further discusses in the original forum post. This example case will be discussed during the workshop","title":"Multiline Fasta To Single Line Fasta"},{"location":"Case_studies/Multiple_output_files/","text":"Multiple output files Here is a type of problem particularly suited for awk. Let's use the example with the coins again and assume that the file is much larger and you want to print the data in separate files, named after the country and the year, for example \"USA.1986.txt\". This will require to write into multiple files in different order. Most languages will require to open files (in append mode), assign file handles and closing it. If you want to avoid multiple opening/closing you need to handle the list of open files yourself. This puts some extra overhead that could be easily avoided in awk ( and all shells [bash, csh, tcsh] as long as I know ). It is as easy as writing: print \"whatever you want\" > country \".\" year \".txt\" print \"whatever you want\" >> country \".\" year \".txt\" Here is an one line variant of it... $ awk '{print $0 > $4\".\"$3\".txt\"}' coins.txt $ wc -l *.txt 1 Austria-Hungary.1908.txt 1 Canada.1988.txt 13 coins.txt 1 PRC.1986.txt 1 RSA.1979.txt 1 RSA.1981.txt 1 Switzerland.1984.txt 1 USA.1981.txt 4 USA.1986.txt 2 USA.1987.txt 26 total Awk can keep up to 1024 open files at the same time. Warning The example above is not working under OS X . A simple fix is to store the file name in a variable before printing into it i.e. OS X version. It will work with Gnu Awk as well $ awk '{f=$4\".\"$3\".txt\"; print $0 > f }' coins . txt Credits to Ding He for the tip original source: http://stackoverflow.com/questions/7980325/splitting-a-file-using-awk-on-mac-os-x","title":"Multiple output files"},{"location":"Case_studies/Multiple_output_files/#multiple-output-files","text":"Here is a type of problem particularly suited for awk. Let's use the example with the coins again and assume that the file is much larger and you want to print the data in separate files, named after the country and the year, for example \"USA.1986.txt\". This will require to write into multiple files in different order. Most languages will require to open files (in append mode), assign file handles and closing it. If you want to avoid multiple opening/closing you need to handle the list of open files yourself. This puts some extra overhead that could be easily avoided in awk ( and all shells [bash, csh, tcsh] as long as I know ). It is as easy as writing: print \"whatever you want\" > country \".\" year \".txt\" print \"whatever you want\" >> country \".\" year \".txt\" Here is an one line variant of it... $ awk '{print $0 > $4\".\"$3\".txt\"}' coins.txt $ wc -l *.txt 1 Austria-Hungary.1908.txt 1 Canada.1988.txt 13 coins.txt 1 PRC.1986.txt 1 RSA.1979.txt 1 RSA.1981.txt 1 Switzerland.1984.txt 1 USA.1981.txt 4 USA.1986.txt 2 USA.1987.txt 26 total Awk can keep up to 1024 open files at the same time. Warning The example above is not working under OS X . A simple fix is to store the file name in a variable before printing into it i.e. OS X version. It will work with Gnu Awk as well $ awk '{f=$4\".\"$3\".txt\"; print $0 > f }' coins . txt Credits to Ding He for the tip original source: http://stackoverflow.com/questions/7980325/splitting-a-file-using-awk-on-mac-os-x","title":"Multiple output files"},{"location":"Case_studies/POSCAR_reorder/","text":"POSCAR: reorder atom types This study case is designed to illustrate a combination of awk features while solving an easy to describe problem but painfully annoying to program. It contains only the basic functionality and does not check for error, does not handle some exceptions in the File format etc. POSCAR is an text format structure input file for the VASP computational code. It defines the lattice geometry and the ionic positions. A limitation of the format is that the positions of the ions cannot be provided in random order, which makes the task of reordering them particularly unpleasant. Nowadays one can find tools to do this, but here we pick the case for the purpose of the workshop. POSCAR 1 2 3 4 5 6 7 8 9 10 11 12 13 MgOH2 1.000000000000000 3.18083395988771 0.00000000000000 0.00000000000000 -1.59041697994386 2.75468301448513 0.00000000000000 0.00000000000000 0.00000000000000 4.76789525772412 Mg O H 1 2 2 Direct 0.0000000000000000 0.0000000000000000 0.000000000000 0.3333333333300033 0.6666666666599994 0.219129582305 0.6666666666599994 0.3333333333300033 -0.219129582305 0.3333333333300033 0.6666666666599994 0.423195382917 0.6666666666599994 0.3333333333300033 -0.423195382917 The lines below Direct are the positions of the ions in order specified on line 6 - Mg O H . Yet, this is not enough, since on line 7 one gets the number of each ion type... The example above has 3 types and 5 ions in total and it is easier to manually fix the desired order. When this file contains more than 100 ions and several types, this simple routine becomes extremely dangerous, since any error is very difficult to spot. Here we have a code that we can easily build interactively as exercise, but below each block is briefly described. #!/usr/bin/awk -f BEGIN { argc = ARGC ; ARGC = 2 # just read the first file } NR <= 5 { print } # print the part that does not change NR == 6 { Ntypes = NF ; # Keep the number of different types for ( i = 1 ; i <= NF ; i ++ ){ element [ i ] =$ i } # Make list with the elements } NR == 7 { for ( i = 1 ; i <= NF ; i ++ ){ elementN [ element [ i ]] =$ i } } # count the number of elements /Direct/ { for ( i = 1 ; i <= Ntypes ; i ++ ){ # loop over the different types for ( j = 1 ; j <= elementN [ element [ i ]]; j ++ ){ # loop over the number of atoms for each type getline # get one line line [ element [ i ], j ] = $ 0 # keep the whole line,[atom name, N] } } } END { for ( i = 1 ; i <= argc - 2 ; i ++ ){ printf ( \"%s \" , ARGV [ i + 1 ]) # print the new order } print \"\" # print new line for ( i = 1 ; i <= argc - 2 ; i ++ ){ printf ( \"%s \" , elementN [ ARGV [ i + 1 ]]) # print the type numbers in the order } print \"\" # print new line print \"Direct\" for ( i = 1 ; i <= argc - 2 ; i ++ ){ atom = ARGV [ i + 1 ] for ( j = 1 ; j <= elementN [ atom ]; j ++ ){ print line [ atom , j ] } } } We will put a code that will read the provided file as first parameter and use the remaining to specify the new order of ions. BEGIN {...} we need the original number of arguments, that is why we first argc = ARGC ; Then we change the number of parameters to trick awk to read only the first argument ARGC = 2 . NR <= 5 { print } lines 1 to 5 does not need to be changed - so we print them. NR == 6 {...} We need to know the number of different types Ntypes= NF and then make a index list with the labels of the elements. NR == 7 {...) We match the number of elements for each type, so it becomes elementN [ \"Mg\" ] = 1 ; elementN [ \"O\" ] = 2 ; elementN [ \"H\" ] = 2 ; /Direct/ {...} We have all the information to read the following lines with a double loop over each type and corresponding number. Done. We have each line indexed in a way that we can access them in a random order. The first line will look like line [ \"Mg\" , 1 ] = 0.0000000000000000 0.0000000000000000 0.000000000000 . Note, we do not need to do anything but reorder the lines, so we keep them intact. END {...} Time to fetch the new order from the command line and print in loops by calling the collected data. Output ./POSCAR-reorder.awk POSCAR H O Mg MgOH2 1 .000000000000000 3 .18083395988771 0 .00000000000000 0 .00000000000000 -1.59041697994386 2 .75468301448513 0 .00000000000000 0 .00000000000000 0 .00000000000000 4 .76789525772412 H O Mg 2 2 1 Direct 0 .3333333333300033 0 .6666666666599994 0 .423195382917 0 .6666666666599994 0 .3333333333300033 -0.423195382917 0 .3333333333300033 0 .6666666666599994 0 .219129582305 0 .6666666666599994 0 .3333333333300033 -0.219129582305 0 .0000000000000000 0 .0000000000000000 0 .000000000000 Easy or not? This might look complicated but it mostly contains for-loops that collect the data. What could be an advantage then? If you look, each block matches and operates only on the targeted lines - makes it a bit easier to split the problem to separate tasks.","title":"POSCAR: reorder atom types"},{"location":"Case_studies/POSCAR_reorder/#poscar-reorder-atom-types","text":"This study case is designed to illustrate a combination of awk features while solving an easy to describe problem but painfully annoying to program. It contains only the basic functionality and does not check for error, does not handle some exceptions in the File format etc. POSCAR is an text format structure input file for the VASP computational code. It defines the lattice geometry and the ionic positions. A limitation of the format is that the positions of the ions cannot be provided in random order, which makes the task of reordering them particularly unpleasant. Nowadays one can find tools to do this, but here we pick the case for the purpose of the workshop. POSCAR 1 2 3 4 5 6 7 8 9 10 11 12 13 MgOH2 1.000000000000000 3.18083395988771 0.00000000000000 0.00000000000000 -1.59041697994386 2.75468301448513 0.00000000000000 0.00000000000000 0.00000000000000 4.76789525772412 Mg O H 1 2 2 Direct 0.0000000000000000 0.0000000000000000 0.000000000000 0.3333333333300033 0.6666666666599994 0.219129582305 0.6666666666599994 0.3333333333300033 -0.219129582305 0.3333333333300033 0.6666666666599994 0.423195382917 0.6666666666599994 0.3333333333300033 -0.423195382917 The lines below Direct are the positions of the ions in order specified on line 6 - Mg O H . Yet, this is not enough, since on line 7 one gets the number of each ion type... The example above has 3 types and 5 ions in total and it is easier to manually fix the desired order. When this file contains more than 100 ions and several types, this simple routine becomes extremely dangerous, since any error is very difficult to spot. Here we have a code that we can easily build interactively as exercise, but below each block is briefly described. #!/usr/bin/awk -f BEGIN { argc = ARGC ; ARGC = 2 # just read the first file } NR <= 5 { print } # print the part that does not change NR == 6 { Ntypes = NF ; # Keep the number of different types for ( i = 1 ; i <= NF ; i ++ ){ element [ i ] =$ i } # Make list with the elements } NR == 7 { for ( i = 1 ; i <= NF ; i ++ ){ elementN [ element [ i ]] =$ i } } # count the number of elements /Direct/ { for ( i = 1 ; i <= Ntypes ; i ++ ){ # loop over the different types for ( j = 1 ; j <= elementN [ element [ i ]]; j ++ ){ # loop over the number of atoms for each type getline # get one line line [ element [ i ], j ] = $ 0 # keep the whole line,[atom name, N] } } } END { for ( i = 1 ; i <= argc - 2 ; i ++ ){ printf ( \"%s \" , ARGV [ i + 1 ]) # print the new order } print \"\" # print new line for ( i = 1 ; i <= argc - 2 ; i ++ ){ printf ( \"%s \" , elementN [ ARGV [ i + 1 ]]) # print the type numbers in the order } print \"\" # print new line print \"Direct\" for ( i = 1 ; i <= argc - 2 ; i ++ ){ atom = ARGV [ i + 1 ] for ( j = 1 ; j <= elementN [ atom ]; j ++ ){ print line [ atom , j ] } } } We will put a code that will read the provided file as first parameter and use the remaining to specify the new order of ions. BEGIN {...} we need the original number of arguments, that is why we first argc = ARGC ; Then we change the number of parameters to trick awk to read only the first argument ARGC = 2 . NR <= 5 { print } lines 1 to 5 does not need to be changed - so we print them. NR == 6 {...} We need to know the number of different types Ntypes= NF and then make a index list with the labels of the elements. NR == 7 {...) We match the number of elements for each type, so it becomes elementN [ \"Mg\" ] = 1 ; elementN [ \"O\" ] = 2 ; elementN [ \"H\" ] = 2 ; /Direct/ {...} We have all the information to read the following lines with a double loop over each type and corresponding number. Done. We have each line indexed in a way that we can access them in a random order. The first line will look like line [ \"Mg\" , 1 ] = 0.0000000000000000 0.0000000000000000 0.000000000000 . Note, we do not need to do anything but reorder the lines, so we keep them intact. END {...} Time to fetch the new order from the command line and print in loops by calling the collected data. Output ./POSCAR-reorder.awk POSCAR H O Mg MgOH2 1 .000000000000000 3 .18083395988771 0 .00000000000000 0 .00000000000000 -1.59041697994386 2 .75468301448513 0 .00000000000000 0 .00000000000000 0 .00000000000000 4 .76789525772412 H O Mg 2 2 1 Direct 0 .3333333333300033 0 .6666666666599994 0 .423195382917 0 .6666666666599994 0 .3333333333300033 -0.423195382917 0 .3333333333300033 0 .6666666666599994 0 .219129582305 0 .6666666666599994 0 .3333333333300033 -0.219129582305 0 .0000000000000000 0 .0000000000000000 0 .000000000000 Easy or not? This might look complicated but it mostly contains for-loops that collect the data. What could be an advantage then? If you look, each block matches and operates only on the targeted lines - makes it a bit easier to split the problem to separate tasks.","title":"POSCAR: reorder atom types"},{"location":"Case_studies/ProLiant_status_check/","text":"ProLiant Status check Here is another example of a common problem solved by awk script. If you are running an HP ProLiant server, you might want to receive mail with the system health every day. In general that is easy, but what I want is, to check the report before delivery and make the subject reflect the general result i.e. \"\" [ProLiant Status]: OK - if everything is fine or [ProLiant Status]: !!!WARNING!!! followed by the problematic line/s So, awk makes system calls and checks for \"hot\" keywords (FAIL, nok) indicating failures and checks the reported temperature values against the thresholds. ProLiant_Status.awk #!/usr/bin/awk -f # This script will execute the commands bellow, collect the output # then check for failures and mail the status # The subject of the mail will be: # [ProLiant Status]: OK - if everything is fine or # [ProLiant Status]: !!!WARNING!!! followed by the problematic line/s BEGIN { cmd = \"/usr/sbin/hpacucli controller all show status\" ; line [ l ++ ] = \"> \" cmd ; get_output ( cmd ); cmd = \"/usr/sbin/hpacucli controller slot=0 logicaldrive all show status\" ; line [ l ++ ] = \"> \" cmd ; get_output ( cmd ); cmd = \"/usr/sbin/hpacucli controller slot=0 physicaldrive all show status\" ; line [ l ++ ] = \"> \" cmd ; get_output ( cmd ); cmd = \"/sbin/hpasmcli -s \\\"SHOW ASR\\\"\" ; line [ l ++ ] = \"> \" cmd ; get_output ( cmd ); cmd = \"/sbin/hpasmcli -s \\\"SHOW FANS\\\"\" ; line [ l ++ ] = \"> \" cmd ; get_output ( cmd ); cmd = \"/sbin/hpasmcli -s \\\"SHOW TEMP\\\"\" ; line [ l ++ ] = \"> \" cmd ; get_output ( cmd ); # Compose a mail containing the output if ( ! SUBJ ) SUBJ = \"[ProLiant Status - allium]: OK\" ; else SUBJ = \"[ProLiant Status - allium]: !!!WARNING!!! \" SUBJ cmd = \"mail -s \\\"\" SUBJ \"\\\" root\" # cmd=\"cat\" # print on screen instead mailing for ( i = 0 ; i <= l ; i ++ ) { print line [ i ] | cmd } close ( cmd ); } # Collects the output and checks for failures function get_output ( command ) { while ( cmd | getline ) { line [ l ++ ] =$ 0 ; # check the line for NOK or FAIL if ( tolower ( $ 0 ) ~ \"nok|fail\" ) SUBJ = SUBJ \" \" $ 0 # if the line contains temperature info - check the threshold if ( $ 0 ~ \"C.*F.*C.*F\" ) { gsub ( \"C/\" , \" \" , $ 0 ); # trick to get easily only the numbers if ( $ 3 >= $ 5 ) SUBJ = SUBJ \" \" $ 0 } } close ( cmd ) } Here is how the report looks like: Subject : [ ProLiant Status - aberlour ] : OK > / usr / sbin / hpacucli controller all show status Smart Array P212 in Slot 1 Controller Status : OK Cache Status : OK Battery / Capacitor Status : OK > / usr / sbin / hpacucli controller slot = 1 logicaldrive all show status logicaldrive 1 ( 40.0 GB , 6 ): OK logicaldrive 2 ( 16.3 TB , 6 ): OK > / usr / sbin / hpacucli controller slot = 1 physicaldrive all show status physicaldrive 1 I : 1 : 1 ( port 1 I : box 1 : bay 1 , 3 TB ): OK physicaldrive 1 I : 1 : 2 ( port 1 I : box 1 : bay 2 , 3 TB ): OK physicaldrive 1 I : 1 : 3 ( port 1 I : box 1 : bay 3 , 3 TB ): OK physicaldrive 1 I : 1 : 4 ( port 1 I : box 1 : bay 4 , 3 TB ): OK physicaldrive 1 I : 1 : 5 ( port 1 I : box 1 : bay 5 , 3 TB ): OK physicaldrive 1 I : 1 : 6 ( port 1 I : box 1 : bay 6 , 3 TB ): OK physicaldrive 1 I : 1 : 7 ( port 1 I : box 1 : bay 7 , 3 TB ): OK physicaldrive 1 I : 1 : 8 ( port 1 I : box 1 : bay 8 , 3 TB ): OK > / sbin / hpasmcli - s \"SHOW ASR\" ASR timeout is 10 minutes . ASR is currently enabled . > / sbin / hpasmcli - s \"SHOW FANS\" Fan Location Present Speed of max Redundant Partner Hot - pluggable --- -------- ------- ----- ------ --------- ------- ------------- # 1 SYSTEM Yes NORMAL 55 % N / A N / A No # 2 SYSTEM Yes NORMAL 55 % N / A N / A No # 3 SYSTEM Yes NORMAL 59 % N / A N / A No # 4 SYSTEM Yes NORMAL 53 % N / A N / A No > / sbin / hpasmcli - s \"SHOW TEMP\" Sensor Location Temp Threshold ------ -------- ---- --------- # 1 MEMORY_BD 33 C / 91 F 87 C / 188 F # 2 MEMORY_BD - 87 C / 188 F # 3 MEMORY_BD 33 C / 91 F 87 C / 188 F # 4 MEMORY_BD - 87 C / 188 F # 5 MEMORY_BD - 87 C / 188 F # 6 MEMORY_BD - 87 C / 188 F # 7 MEMORY_BD 40 C / 104 F 95 C / 203 F # 8 MEMORY_BD - 87 C / 188 F # 9 MEMORY_BD - 87 C / 188 F # 10 MEMORY_BD - 87 C / 188 F # 11 MEMORY_BD - 87 C / 188 F # 12 MEMORY_BD - 87 C / 188 F # 13 MEMORY_BD - 87 C / 188 F # 14 MEMORY_BD - 95 C / 203 F # 15 SYSTEM_BD - 60 C / 140 F # 16 SYSTEM_BD - 60 C / 140 F # 17 AMBIENT 30 C / 86 F 60 C / 140 F # 18 AMBIENT 38 C / 100 F 70 C / 158 F # 19 SYSTEM_BD 17 C / 62 F 112 C / 233 F # 20 SYSTEM_BD 41 C / 105 F 79 C / 174 F # 21 SYSTEM_BD 31 C / 87 F 60 C / 140 F Files ProLiant_Status.awk","title":"ProLiant Status check"},{"location":"Case_studies/ProLiant_status_check/#proliant-status-check","text":"Here is another example of a common problem solved by awk script. If you are running an HP ProLiant server, you might want to receive mail with the system health every day. In general that is easy, but what I want is, to check the report before delivery and make the subject reflect the general result i.e. \"\" [ProLiant Status]: OK - if everything is fine or [ProLiant Status]: !!!WARNING!!! followed by the problematic line/s So, awk makes system calls and checks for \"hot\" keywords (FAIL, nok) indicating failures and checks the reported temperature values against the thresholds. ProLiant_Status.awk #!/usr/bin/awk -f # This script will execute the commands bellow, collect the output # then check for failures and mail the status # The subject of the mail will be: # [ProLiant Status]: OK - if everything is fine or # [ProLiant Status]: !!!WARNING!!! followed by the problematic line/s BEGIN { cmd = \"/usr/sbin/hpacucli controller all show status\" ; line [ l ++ ] = \"> \" cmd ; get_output ( cmd ); cmd = \"/usr/sbin/hpacucli controller slot=0 logicaldrive all show status\" ; line [ l ++ ] = \"> \" cmd ; get_output ( cmd ); cmd = \"/usr/sbin/hpacucli controller slot=0 physicaldrive all show status\" ; line [ l ++ ] = \"> \" cmd ; get_output ( cmd ); cmd = \"/sbin/hpasmcli -s \\\"SHOW ASR\\\"\" ; line [ l ++ ] = \"> \" cmd ; get_output ( cmd ); cmd = \"/sbin/hpasmcli -s \\\"SHOW FANS\\\"\" ; line [ l ++ ] = \"> \" cmd ; get_output ( cmd ); cmd = \"/sbin/hpasmcli -s \\\"SHOW TEMP\\\"\" ; line [ l ++ ] = \"> \" cmd ; get_output ( cmd ); # Compose a mail containing the output if ( ! SUBJ ) SUBJ = \"[ProLiant Status - allium]: OK\" ; else SUBJ = \"[ProLiant Status - allium]: !!!WARNING!!! \" SUBJ cmd = \"mail -s \\\"\" SUBJ \"\\\" root\" # cmd=\"cat\" # print on screen instead mailing for ( i = 0 ; i <= l ; i ++ ) { print line [ i ] | cmd } close ( cmd ); } # Collects the output and checks for failures function get_output ( command ) { while ( cmd | getline ) { line [ l ++ ] =$ 0 ; # check the line for NOK or FAIL if ( tolower ( $ 0 ) ~ \"nok|fail\" ) SUBJ = SUBJ \" \" $ 0 # if the line contains temperature info - check the threshold if ( $ 0 ~ \"C.*F.*C.*F\" ) { gsub ( \"C/\" , \" \" , $ 0 ); # trick to get easily only the numbers if ( $ 3 >= $ 5 ) SUBJ = SUBJ \" \" $ 0 } } close ( cmd ) } Here is how the report looks like: Subject : [ ProLiant Status - aberlour ] : OK > / usr / sbin / hpacucli controller all show status Smart Array P212 in Slot 1 Controller Status : OK Cache Status : OK Battery / Capacitor Status : OK > / usr / sbin / hpacucli controller slot = 1 logicaldrive all show status logicaldrive 1 ( 40.0 GB , 6 ): OK logicaldrive 2 ( 16.3 TB , 6 ): OK > / usr / sbin / hpacucli controller slot = 1 physicaldrive all show status physicaldrive 1 I : 1 : 1 ( port 1 I : box 1 : bay 1 , 3 TB ): OK physicaldrive 1 I : 1 : 2 ( port 1 I : box 1 : bay 2 , 3 TB ): OK physicaldrive 1 I : 1 : 3 ( port 1 I : box 1 : bay 3 , 3 TB ): OK physicaldrive 1 I : 1 : 4 ( port 1 I : box 1 : bay 4 , 3 TB ): OK physicaldrive 1 I : 1 : 5 ( port 1 I : box 1 : bay 5 , 3 TB ): OK physicaldrive 1 I : 1 : 6 ( port 1 I : box 1 : bay 6 , 3 TB ): OK physicaldrive 1 I : 1 : 7 ( port 1 I : box 1 : bay 7 , 3 TB ): OK physicaldrive 1 I : 1 : 8 ( port 1 I : box 1 : bay 8 , 3 TB ): OK > / sbin / hpasmcli - s \"SHOW ASR\" ASR timeout is 10 minutes . ASR is currently enabled . > / sbin / hpasmcli - s \"SHOW FANS\" Fan Location Present Speed of max Redundant Partner Hot - pluggable --- -------- ------- ----- ------ --------- ------- ------------- # 1 SYSTEM Yes NORMAL 55 % N / A N / A No # 2 SYSTEM Yes NORMAL 55 % N / A N / A No # 3 SYSTEM Yes NORMAL 59 % N / A N / A No # 4 SYSTEM Yes NORMAL 53 % N / A N / A No > / sbin / hpasmcli - s \"SHOW TEMP\" Sensor Location Temp Threshold ------ -------- ---- --------- # 1 MEMORY_BD 33 C / 91 F 87 C / 188 F # 2 MEMORY_BD - 87 C / 188 F # 3 MEMORY_BD 33 C / 91 F 87 C / 188 F # 4 MEMORY_BD - 87 C / 188 F # 5 MEMORY_BD - 87 C / 188 F # 6 MEMORY_BD - 87 C / 188 F # 7 MEMORY_BD 40 C / 104 F 95 C / 203 F # 8 MEMORY_BD - 87 C / 188 F # 9 MEMORY_BD - 87 C / 188 F # 10 MEMORY_BD - 87 C / 188 F # 11 MEMORY_BD - 87 C / 188 F # 12 MEMORY_BD - 87 C / 188 F # 13 MEMORY_BD - 87 C / 188 F # 14 MEMORY_BD - 95 C / 203 F # 15 SYSTEM_BD - 60 C / 140 F # 16 SYSTEM_BD - 60 C / 140 F # 17 AMBIENT 30 C / 86 F 60 C / 140 F # 18 AMBIENT 38 C / 100 F 70 C / 158 F # 19 SYSTEM_BD 17 C / 62 F 112 C / 233 F # 20 SYSTEM_BD 41 C / 105 F 79 C / 174 F # 21 SYSTEM_BD 31 C / 87 F 60 C / 140 F Files ProLiant_Status.awk","title":"ProLiant Status check"},{"location":"Case_studies/Sequence_clustering/","text":"Sequence clustering with awk Contribution by Mart\u00edn Gonz\u00e1lez Buitr\u00f3n, National University of Quilmes, Argentina, exchange PhD student - Stockholm University 2020.04 CDHit is a software that allows users to perform sequence clustering. An important thing about clustering is to evaluate clustering conditions, i.e: identity, coverage, etc. One main problem about different conditions is to know what happen with each sequence. Lucky us, CDHit standard output has few patterns that anyone could exploit using AWK. These are, how clusters are written and how sequence lines belong in one cluster begins. Suppose someone has a non-redundant dataset of hundred or thousands of sequences and performed two clustering using CDHit. Moreover, suppose that earlier dataset is diverse. A problem appears, which configuration is better for your clustering goal? Knowing that, the following script process both standard CDHit clustering files (ended in *.clstr) and checks in which cluster it contains each sequence. If the above is done by hand, it could not be so good for the health. So, the script output has these fields in CSV format: query_code, query_length, cluster_number_in_file_1, cluster_size_in_file_1, target_code, target_length, cluster_number_in_file_2, cluster_size_in_file_2, cluster_size_diff, same_cluster Where same_cluster is a boolean column and has information about the changing location of that sequence, i.e: same_cluster = Yes ------> that sequence didn't change of cluster same_cluster = No ------> that sequence change to other cluster (new or already existed) Where cluster_size_diff has information about the difference in size from each cluster (cluster_size_in_file_2 - cluster_size_in_file_1): cluster_size_diff = 0 ------> that sequence is not in a bigger or smaller cluster cluster_size_diff > 0 ------> that sequence is in a bigger cluster (new or already existed) cluster_size_diff < 0 ------> that sequence is in a smaller cluster (new or already existed) Warnings! Could be that cluster_size_diff is different than 0 but the cluster continue being the same (e.g: look sequences in Cluster_0). That happen due how CDHit works and of course, the values in clustering conditions what were used to compare. For a deeper and accurate way to understand what is happening, you should then check these three columns, same_cluster and both \"cluster_size\" for any sequence, easily you can check column $4, $8 and $10. Cloud be that same_cluster is \"No\" but the cluster continue being pretty the same (e.g: look sequences in Cluster_1 to Cluster_4). That happen due how CDHit works and of course, the values in clustering conditions what were used to compare. For a deeper and accurate way to understand what is happening in this hard case, you should then check sequence neighbours and/or these two \" cluster_size \" columns for any sequence, easily you can check column $4, $8. For example, for bigger clusters in File 1 is more probable that those clusters leak/add some sequence in other CDHit condition but not changing to much in size (do size percentage study if necessary). There are others possible results that you may need to study on detail, like: Creation of new clusters (\"border sequences\" in different clusters form this new cluster or just sequences that were alone are now together) Complete destruction of clusters (this happen frequently in small clusters that have \"border sequences\") Funny destruction of clusters (they split on half creating two \"new ones\" or something like that) To run the program $ ./get_seq_jumping_description_from_CDHIT_clstr_info.awk 2020 -03-13_cdhit_ide_1.00_cov_0.98.clstr 2020 -03-13_cdhit_ide_0.98_cov_0.90.clstr Example output from the program File1,,,,File2,,,, query_code, query_length, cluster_number_in_file_1, cluster_size_in_file_1, target_code,target_length, cluster_number_in_file_2, cluster_size_in_file_2, cluster_size_diff, same_cluster 1FJG:A,1522,0,271,1FJG:A,1522,0,378,107,Yes 1HNW:A,1522,0,271,1HNW:A,1522,0,378,107,Yes 1HNX:A,1522,0,271,1HNX:A,1522,0,378,107,Yes 1HNZ:A,1522,0,271,1HNZ:A,1522,0,378,107,Yes 1HR0:A,1522,0,271,1HR0:A,1522,0,378,107,Yes 1I94:A,1514,0,271,1I94:A,1514,0,378,107,Yes 1IBK:A,1522,0,271,1IBK:A,1522,0,378,107,Yes 1IBL:A,1522,0,271,1IBL:A,1522,0,378,107,Yes ... 6UCQ:2a,1521,0,271,6UCQ:2a,1521,0,378,107,Yes 6UO1:1a,1521,0,271,6UO1:1a,1521,0,378,107,Yes 6UO1:2a,1521,0,271,6UO1:2a,1521,0,378,107,Yes 1VY4:AW,76,1,209,1VY4:AW,76,4,213,4,No 1VY4:AY,76,1,209,1VY4:AY,76,4,213,4,No 1VY4:CW,76,1,209,1VY4:CW,76,4,213,4,No 1VY4:CY,76,1,209,1VY4:CY,76,4,213,4,No ... For details, look into the code available for download at the bottom of this page. The role of each line is commented for easier understanding. Example files get_seq_jumping_description_from_CDHIT_clstr_info.awk 2020-03-13_cdhit_ide_0.98_cov_0.90.clstr 2020-03-13_cdhit_ide_1.00_cov_0.98.clstr","title":"Sequence clustering with awk"},{"location":"Case_studies/Sequence_clustering/#sequence-clustering-with-awk","text":"Contribution by Mart\u00edn Gonz\u00e1lez Buitr\u00f3n, National University of Quilmes, Argentina, exchange PhD student - Stockholm University 2020.04 CDHit is a software that allows users to perform sequence clustering. An important thing about clustering is to evaluate clustering conditions, i.e: identity, coverage, etc. One main problem about different conditions is to know what happen with each sequence. Lucky us, CDHit standard output has few patterns that anyone could exploit using AWK. These are, how clusters are written and how sequence lines belong in one cluster begins. Suppose someone has a non-redundant dataset of hundred or thousands of sequences and performed two clustering using CDHit. Moreover, suppose that earlier dataset is diverse. A problem appears, which configuration is better for your clustering goal? Knowing that, the following script process both standard CDHit clustering files (ended in *.clstr) and checks in which cluster it contains each sequence. If the above is done by hand, it could not be so good for the health. So, the script output has these fields in CSV format: query_code, query_length, cluster_number_in_file_1, cluster_size_in_file_1, target_code, target_length, cluster_number_in_file_2, cluster_size_in_file_2, cluster_size_diff, same_cluster Where same_cluster is a boolean column and has information about the changing location of that sequence, i.e: same_cluster = Yes ------> that sequence didn't change of cluster same_cluster = No ------> that sequence change to other cluster (new or already existed) Where cluster_size_diff has information about the difference in size from each cluster (cluster_size_in_file_2 - cluster_size_in_file_1): cluster_size_diff = 0 ------> that sequence is not in a bigger or smaller cluster cluster_size_diff > 0 ------> that sequence is in a bigger cluster (new or already existed) cluster_size_diff < 0 ------> that sequence is in a smaller cluster (new or already existed) Warnings! Could be that cluster_size_diff is different than 0 but the cluster continue being the same (e.g: look sequences in Cluster_0). That happen due how CDHit works and of course, the values in clustering conditions what were used to compare. For a deeper and accurate way to understand what is happening, you should then check these three columns, same_cluster and both \"cluster_size\" for any sequence, easily you can check column $4, $8 and $10. Cloud be that same_cluster is \"No\" but the cluster continue being pretty the same (e.g: look sequences in Cluster_1 to Cluster_4). That happen due how CDHit works and of course, the values in clustering conditions what were used to compare. For a deeper and accurate way to understand what is happening in this hard case, you should then check sequence neighbours and/or these two \" cluster_size \" columns for any sequence, easily you can check column $4, $8. For example, for bigger clusters in File 1 is more probable that those clusters leak/add some sequence in other CDHit condition but not changing to much in size (do size percentage study if necessary). There are others possible results that you may need to study on detail, like: Creation of new clusters (\"border sequences\" in different clusters form this new cluster or just sequences that were alone are now together) Complete destruction of clusters (this happen frequently in small clusters that have \"border sequences\") Funny destruction of clusters (they split on half creating two \"new ones\" or something like that) To run the program $ ./get_seq_jumping_description_from_CDHIT_clstr_info.awk 2020 -03-13_cdhit_ide_1.00_cov_0.98.clstr 2020 -03-13_cdhit_ide_0.98_cov_0.90.clstr Example output from the program File1,,,,File2,,,, query_code, query_length, cluster_number_in_file_1, cluster_size_in_file_1, target_code,target_length, cluster_number_in_file_2, cluster_size_in_file_2, cluster_size_diff, same_cluster 1FJG:A,1522,0,271,1FJG:A,1522,0,378,107,Yes 1HNW:A,1522,0,271,1HNW:A,1522,0,378,107,Yes 1HNX:A,1522,0,271,1HNX:A,1522,0,378,107,Yes 1HNZ:A,1522,0,271,1HNZ:A,1522,0,378,107,Yes 1HR0:A,1522,0,271,1HR0:A,1522,0,378,107,Yes 1I94:A,1514,0,271,1I94:A,1514,0,378,107,Yes 1IBK:A,1522,0,271,1IBK:A,1522,0,378,107,Yes 1IBL:A,1522,0,271,1IBL:A,1522,0,378,107,Yes ... 6UCQ:2a,1521,0,271,6UCQ:2a,1521,0,378,107,Yes 6UO1:1a,1521,0,271,6UO1:1a,1521,0,378,107,Yes 6UO1:2a,1521,0,271,6UO1:2a,1521,0,378,107,Yes 1VY4:AW,76,1,209,1VY4:AW,76,4,213,4,No 1VY4:AY,76,1,209,1VY4:AY,76,4,213,4,No 1VY4:CW,76,1,209,1VY4:CW,76,4,213,4,No 1VY4:CY,76,1,209,1VY4:CY,76,4,213,4,No ... For details, look into the code available for download at the bottom of this page. The role of each line is commented for easier understanding. Example files get_seq_jumping_description_from_CDHIT_clstr_info.awk 2020-03-13_cdhit_ide_0.98_cov_0.90.clstr 2020-03-13_cdhit_ide_1.00_cov_0.98.clstr","title":"Sequence clustering with awk"},{"location":"Case_studies/awk-jmol/","text":"Awk and Jmol I have a structure that I can easily visualize with Jmol and I want to plot vectors at each atom. Here I am giving an example with Jmol, but the concept is the same for any other program. The command to plot such vectors with Jmol is draw ID vector ( atomno = 1 ) { x , y , z } For larger molecules this quickly becomes quite a tedious work to type all this commands... so let awk write it for us. The output is printed to the sceen and saved in file vectors.spt that will later run in Jmol. $ awk '{i++;printf (\"draw v%i vector (atomno=%i) {%f,%f,%f}\\n\",i,i,$1,$2,$3)}' vectors . dat | tee vectors . spt draw v1 vector ( atomno = 1 ) { - 0.500000 , 0.700000 , 0.700000 } draw v2 vector ( atomno = 2 ) { 0.500000 , - 1.000000 , 0.900000 } draw v3 vector ( atomno = 3 ) { 0.500000 , - 0.500000 , 0.900000 } where vectors.dat contains our vectors (x,y,z) vectors.dat -0.5 0.7 0.7 0.5 -1.0 0.9 0.5 -0.5 0.9 The Jmol script that creates the figure above looks like this (in red is the line that loads the awk generated commands) # load the molecule load \"1.xyz\" # save the status , orianation etc . center { 6.285864 6.6071978 6.971991 }; moveto - 1.0 { 0 0 1 0 } 100.0 0.0 0.0 { 6.285864 6.6071978 6.971991 } 2.1566827 { 0 0 0 } 0 0 0 3.0 0.0 0.0 ; save orientation \"default\" ; moveto 0.0 { - 488 825 - 286 58.44 } 100.0 0.0 0.0 { 6.285864 6.6071978 6.971991 } 2.1566827 { 0 0 0 } 0 0 0 3.0 0.0 0.0 ;; wireframe 0.1 set perspectiveDepth FALSE # load the file generated with awk script \"vectors.spt\" write image 200 200 png \"molecule.png\" Here are few more plots generated the same way. Of course, you can use the same \"trick\" to change colors, sizes, or any other properties by generating instructions with simple awk scripts.","title":"Awk and Jmol"},{"location":"Case_studies/awk-jmol/#awk-and-jmol","text":"I have a structure that I can easily visualize with Jmol and I want to plot vectors at each atom. Here I am giving an example with Jmol, but the concept is the same for any other program. The command to plot such vectors with Jmol is draw ID vector ( atomno = 1 ) { x , y , z } For larger molecules this quickly becomes quite a tedious work to type all this commands... so let awk write it for us. The output is printed to the sceen and saved in file vectors.spt that will later run in Jmol. $ awk '{i++;printf (\"draw v%i vector (atomno=%i) {%f,%f,%f}\\n\",i,i,$1,$2,$3)}' vectors . dat | tee vectors . spt draw v1 vector ( atomno = 1 ) { - 0.500000 , 0.700000 , 0.700000 } draw v2 vector ( atomno = 2 ) { 0.500000 , - 1.000000 , 0.900000 } draw v3 vector ( atomno = 3 ) { 0.500000 , - 0.500000 , 0.900000 } where vectors.dat contains our vectors (x,y,z) vectors.dat -0.5 0.7 0.7 0.5 -1.0 0.9 0.5 -0.5 0.9 The Jmol script that creates the figure above looks like this (in red is the line that loads the awk generated commands) # load the molecule load \"1.xyz\" # save the status , orianation etc . center { 6.285864 6.6071978 6.971991 }; moveto - 1.0 { 0 0 1 0 } 100.0 0.0 0.0 { 6.285864 6.6071978 6.971991 } 2.1566827 { 0 0 0 } 0 0 0 3.0 0.0 0.0 ; save orientation \"default\" ; moveto 0.0 { - 488 825 - 286 58.44 } 100.0 0.0 0.0 { 6.285864 6.6071978 6.971991 } 2.1566827 { 0 0 0 } 0 0 0 3.0 0.0 0.0 ;; wireframe 0.1 set perspectiveDepth FALSE # load the file generated with awk script \"vectors.spt\" write image 200 200 png \"molecule.png\" Here are few more plots generated the same way. Of course, you can use the same \"trick\" to change colors, sizes, or any other properties by generating instructions with simple awk scripts.","title":"Awk and Jmol"},{"location":"Case_studies/awk_gnuplot/","text":"Awk and Gnuplot Using awk to fit Birch\u2013Murnaghan equation of state and show the results. I have written this script a long time ago, before Gnuplot had the options to print its own variables on the plot. Nowadays, it is possible to make the fit entirely from Gnuplot, although it will be still tricky to make some decisions if you want to align some labels. Perhaps the most valueable part is the demonstartion of simultaneous output/input to external program (Gnuplot in this case) while (( gnu | & getline ) > 0 ) and for future reference. #!/usr/bin/awk -f ############################################################################# # # # GNU License - Author: Pavlin Mitev # # Version 0.3 - Original code from A. Papaconstantopoulos # # http://cst-www.nrl.navy.mil/bind/static/index.html # # # # Fits Birch equation to obtain equilibrion energy, equilibrium lattice # # constant, bulk modulus, and pressure derivative of the bulk modulus. # # # # REMARK: FCC version; affects the estimation of eq.latt. constant # # # # Format of the input file: # # Atomic_Volume_in_Angstroms Energy_per_atom_in_eV # # # ############################################################################# BEGIN { if ( ARGC > 2 ) { gnu = \"(gnuplot -persist >& /dev/stdout)\" ; vfw = ARGV [ 2 ]; print \"t(vo,v) = (vo/v)**(2./3.) - 1.0;\" | & gnu print \"e(eo,vo,ko,kop,v) = eo + 1.125*ko*vo*t(vo,v)*t(vo,v)* (1.0 + 0.5*(kop-4.0)*t(vo,v));\" | & gnu ; print \"ef=1; vf=\" vfw \"; kf=0.1; kfp=4;\" | & gnu ; print \"fit [\" ARGV [ 3 ] \":\" ARGV [ 4 ] \"] e(ef,vf,kf,kfp,x) \\\"\" ARGV [ 1 ] \"\\\" via ef,vf,kf,kfp;\" | & gnu ; print \"print \\\"Results of 3rd order Birch fit:\\\";\" | & gnu ; print \"print \\\"E_0 = \\\",ef,\\\" Ev\\\";\" | & gnu ; print \"print \\\"V_0 = \\\",vf,\\\" Angstrom**3\\\";\" | & gnu ; print \"af = (4.0*vf)**(1./3.);\" | & gnu ; print \"print \\\"a_0 = \\\",af,\\\" Angstrom\\\";\" | & gnu ; print \"kfx = 160.21765*kf;\" | & gnu ; print \"print \\\"B_0 = \\\",kfx,\\\" GPa\\\";\" | & gnu ; print \"print \\\"B_0d pressure derivative= \\\",kfp;\" | & gnu ; # close(gnu,\"to\"); while (( gnu | & getline ) > 0 ) { print $ 0 ; if ( $ 1 == \"E_0\" ) { print \"set label \\\"E_0= \" $ 3 \" [eV]\\\" at screen 0.5, 0.78 center\" | & gnu ;} if ( $ 1 == \"V_0\" ) { vfw = $ 3 ;} if ( $ 1 == \"a_0\" ) { print \"set label \\\"a_0= \" $ 3 \" [A]\\\" at screen 0.5, 0.74 center\" | & gnu ;} if ( $ 1 == \"B_0\" ) { print \"set label \\\"Bulk Modulus= \" $ 3 \" [GPa]\\\" at screen 0.5, 0.70 center\" | & gnu ;} if ( $ 1 == \"B_0d\" ) { print \"set label \\\"B_0 dp= \" $ 4 \"\\\" at screen 0.5, 0.66 center\" | & gnu ; print \"set label \\\"\" vfw \"\\\" at first \" vfw \", ef*0.95 center\" | & gnu ; print \"set arrow from \" vfw \",ef*0.96 \\ to \" vfw \",ef\" | & gnu ; print \"plot \\\"\" ARGV [ 1 ] \"\\\" title \\\"Original data\\\" w p,\\ e(ef,vf,kf,kfp,x) title \\\"Birtch fit\\\"\" | & gnu ; close ( gnu ); exit ( 0 ) } } } else { print \"Syntax:\" ; print \"Bulk datafile_name atomic_volume_near_equilibrium [lower_limit [upper_limit]]\" print \"Read the source code for details\" ; print \"\" ; } } Without parameters, it prints the syntax... $ ./Bulk-Modulus-Birch-FCC-Vol-A-eV.awk Syntax: Bulk datafile_name atomic_volume_near_equilibrium [ lower_limit [ upper_limit ]] Read the source code for details Here how it works. ./Bulk-Modulus-Birch-FCC-Vol-A-eV.awk bulkm.r-e.dat 4 Files Bulk-Modulus-Birch-FCC-Vol-A-eV.awk bulkm.r-e.dat","title":"Awk and Gnuplot"},{"location":"Case_studies/awk_gnuplot/#awk-and-gnuplot","text":"","title":"Awk and Gnuplot"},{"location":"Case_studies/awk_gnuplot/#using-awk-to-fit-birchmurnaghan-equation-of-state-and-show-the-results","text":"I have written this script a long time ago, before Gnuplot had the options to print its own variables on the plot. Nowadays, it is possible to make the fit entirely from Gnuplot, although it will be still tricky to make some decisions if you want to align some labels. Perhaps the most valueable part is the demonstartion of simultaneous output/input to external program (Gnuplot in this case) while (( gnu | & getline ) > 0 ) and for future reference. #!/usr/bin/awk -f ############################################################################# # # # GNU License - Author: Pavlin Mitev # # Version 0.3 - Original code from A. Papaconstantopoulos # # http://cst-www.nrl.navy.mil/bind/static/index.html # # # # Fits Birch equation to obtain equilibrion energy, equilibrium lattice # # constant, bulk modulus, and pressure derivative of the bulk modulus. # # # # REMARK: FCC version; affects the estimation of eq.latt. constant # # # # Format of the input file: # # Atomic_Volume_in_Angstroms Energy_per_atom_in_eV # # # ############################################################################# BEGIN { if ( ARGC > 2 ) { gnu = \"(gnuplot -persist >& /dev/stdout)\" ; vfw = ARGV [ 2 ]; print \"t(vo,v) = (vo/v)**(2./3.) - 1.0;\" | & gnu print \"e(eo,vo,ko,kop,v) = eo + 1.125*ko*vo*t(vo,v)*t(vo,v)* (1.0 + 0.5*(kop-4.0)*t(vo,v));\" | & gnu ; print \"ef=1; vf=\" vfw \"; kf=0.1; kfp=4;\" | & gnu ; print \"fit [\" ARGV [ 3 ] \":\" ARGV [ 4 ] \"] e(ef,vf,kf,kfp,x) \\\"\" ARGV [ 1 ] \"\\\" via ef,vf,kf,kfp;\" | & gnu ; print \"print \\\"Results of 3rd order Birch fit:\\\";\" | & gnu ; print \"print \\\"E_0 = \\\",ef,\\\" Ev\\\";\" | & gnu ; print \"print \\\"V_0 = \\\",vf,\\\" Angstrom**3\\\";\" | & gnu ; print \"af = (4.0*vf)**(1./3.);\" | & gnu ; print \"print \\\"a_0 = \\\",af,\\\" Angstrom\\\";\" | & gnu ; print \"kfx = 160.21765*kf;\" | & gnu ; print \"print \\\"B_0 = \\\",kfx,\\\" GPa\\\";\" | & gnu ; print \"print \\\"B_0d pressure derivative= \\\",kfp;\" | & gnu ; # close(gnu,\"to\"); while (( gnu | & getline ) > 0 ) { print $ 0 ; if ( $ 1 == \"E_0\" ) { print \"set label \\\"E_0= \" $ 3 \" [eV]\\\" at screen 0.5, 0.78 center\" | & gnu ;} if ( $ 1 == \"V_0\" ) { vfw = $ 3 ;} if ( $ 1 == \"a_0\" ) { print \"set label \\\"a_0= \" $ 3 \" [A]\\\" at screen 0.5, 0.74 center\" | & gnu ;} if ( $ 1 == \"B_0\" ) { print \"set label \\\"Bulk Modulus= \" $ 3 \" [GPa]\\\" at screen 0.5, 0.70 center\" | & gnu ;} if ( $ 1 == \"B_0d\" ) { print \"set label \\\"B_0 dp= \" $ 4 \"\\\" at screen 0.5, 0.66 center\" | & gnu ; print \"set label \\\"\" vfw \"\\\" at first \" vfw \", ef*0.95 center\" | & gnu ; print \"set arrow from \" vfw \",ef*0.96 \\ to \" vfw \",ef\" | & gnu ; print \"plot \\\"\" ARGV [ 1 ] \"\\\" title \\\"Original data\\\" w p,\\ e(ef,vf,kf,kfp,x) title \\\"Birtch fit\\\"\" | & gnu ; close ( gnu ); exit ( 0 ) } } } else { print \"Syntax:\" ; print \"Bulk datafile_name atomic_volume_near_equilibrium [lower_limit [upper_limit]]\" print \"Read the source code for details\" ; print \"\" ; } } Without parameters, it prints the syntax... $ ./Bulk-Modulus-Birch-FCC-Vol-A-eV.awk Syntax: Bulk datafile_name atomic_volume_near_equilibrium [ lower_limit [ upper_limit ]] Read the source code for details Here how it works. ./Bulk-Modulus-Birch-FCC-Vol-A-eV.awk bulkm.r-e.dat 4 Files Bulk-Modulus-Birch-FCC-Vol-A-eV.awk bulkm.r-e.dat","title":"Using awk to fit Birch\u2013Murnaghan equation of state and show the results."},{"location":"Case_studies/awk_network/","text":"Awk and networking Here is just an example that illustrates the awk ability to communicate over common network protocols. Please, read the awk documentation for detailed explanation of the commands. On this Internet address http://130.238.141.28/inet_obs.htm one can find some real-time weather data. The script below extracts the raw values and print them on the screen (or uses the old Linux notification tool to show it on the desktop / commented out in this example /). #!/usr/bin/awk -f BEGIN { RS = ORS = \"\\r\\n\" http = \"/inet/tcp/0/130.238.141.28/80\" print \"GET http://130.238.141.28/inet_obs.htm\" | & http while (( http | & getline ) > 0 ) { if ( match ( $ 0 , \"Temperatur:\" )) ss = sprintf ( \" Temp :%6g\u00b0C \\n\" , $ 2 ) if ( match ( $ 0 , \"Luftfuktighet:\" )) ss = ss sprintf ( \"Rel. humidity :%6g% \\n\" , $ 2 ) if ( match ( $ 0 , \"Vindhastighet:\" )) ss = ss sprintf ( \" Wind speed :%6gm/s\\n\" , $ 2 ) if ( match ( $ 0 , \"Lufttryck:\" )) ss = ss sprintf ( \"Atm. pressure :%6ghPa\\n\" , $ 2 ) } close ( http ) #cmd=\"echo -e '\"ss\"' | \"osd\" &\"; system(cmd); close(cmd); print ss } Temp : 11.1\u00b0C Rel. humidity : 21% Wind speed : 1.3m/s Atm. pressure :1013.9hPa How about simple web server? http://rosettacode.org/wiki/Hello_world/Web_server#AWK","title":"Awk and networking"},{"location":"Case_studies/awk_network/#awk-and-networking","text":"Here is just an example that illustrates the awk ability to communicate over common network protocols. Please, read the awk documentation for detailed explanation of the commands. On this Internet address http://130.238.141.28/inet_obs.htm one can find some real-time weather data. The script below extracts the raw values and print them on the screen (or uses the old Linux notification tool to show it on the desktop / commented out in this example /). #!/usr/bin/awk -f BEGIN { RS = ORS = \"\\r\\n\" http = \"/inet/tcp/0/130.238.141.28/80\" print \"GET http://130.238.141.28/inet_obs.htm\" | & http while (( http | & getline ) > 0 ) { if ( match ( $ 0 , \"Temperatur:\" )) ss = sprintf ( \" Temp :%6g\u00b0C \\n\" , $ 2 ) if ( match ( $ 0 , \"Luftfuktighet:\" )) ss = ss sprintf ( \"Rel. humidity :%6g% \\n\" , $ 2 ) if ( match ( $ 0 , \"Vindhastighet:\" )) ss = ss sprintf ( \" Wind speed :%6gm/s\\n\" , $ 2 ) if ( match ( $ 0 , \"Lufttryck:\" )) ss = ss sprintf ( \"Atm. pressure :%6ghPa\\n\" , $ 2 ) } close ( http ) #cmd=\"echo -e '\"ss\"' | \"osd\" &\"; system(cmd); close(cmd); print ss } Temp : 11.1\u00b0C Rel. humidity : 21% Wind speed : 1.3m/s Atm. pressure :1013.9hPa How about simple web server? http://rosettacode.org/wiki/Hello_world/Web_server#AWK","title":"Awk and networking"},{"location":"Case_studies/awk_writes_python/","text":"Awk writes Python This sounds strange. Why would one use awk to write Python code? Well, the reason is the same as before, \"the overheads of more sophisticated approaches are not worth the bother\" . My analysis program runs in Python but collecting my data from the output of another program (GULP in this case) requires some tedious work. The result is a small awk script that extracts the relevant values and writes a Python structure that takes a couple of lines to load. The code looks large and complicated, but if you just look closely, each search section is independent from the rest and addresses a single property of interest. With time, I kept adding more and more patterns to make the script more robust and generic. If you are still not convinced, just look how the GULP output looks like. In the file section, as usual, there is a complete set as an example. GULP-out2python.awk #!/usr/bin/awk -f BEGIN { print \"import numpy as np\\n\\nclass GULP:\\n\\tdef __init__(self):\\n\\t\\tself.ver=\\\"2015.04.15\\\"\" } /Dimensionality =/ { if ( $ 3 == 3 ) print \"GULP.pbc= np.array([ True, True, True], dtype=bool)\" if ( $ 3 == 2 ) print \"GULP.pbc= np.array([ True, True, False], dtype=bool)\" if ( $ 3 == 1 ) print \"GULP.pbc= np.array([ True, False, False], dtype=bool)\" if ( $ 3 == 0 ) print \"GULP.pbc= np.array([ False, False, False], dtype=bool)\" } /Cartesian lattice vectors \\(Angstroms\\) :/ { getline ; getline ; printf \"GULP.cell= np.array([\\n\\t[%g,%g,%g],\\n\" , $ 1 , $ 2 , $ 3 getline ; printf \"\\t[%g,%g,%g],\\n\" , $ 1 , $ 2 , $ 3 getline ; printf \"\\t[%g,%g,%g]])\\n\" , $ 1 , $ 2 , $ 3 } /Fractional coordinates of asymmetric unit/ { for ( i = 1 ; i <= 6 ; i ++ ) getline while ( NF > 1 ){ scoord [ $ 1 ] = \"[\" $ 4 \",\" $ 5 \",\" $ 6 \"]\" ; charge [ $ 1 ] =$ 7 ; if ( $ 2 == \"X1\" ) $ 2 = \"X\" symbol [ $ 1 ] =$ 2 ; natoms =$ 1 ; getline ; } print \"GULP.scaled_positions= np.array([\" ; for ( i = 1 ; i <= natoms - 1 ; i ++ ){ print \"\\t\" scoord [ i ] \",\" }; print \"\\t\" scoord [ i ] \"])\" printf \"GULP.charges= np.array([\" ; for ( i = 1 ; i <= natoms - 1 ; i ++ ){ print \"\\t\" charge [ i ] \",\" }; print \"\\t\" , charge [ natoms ] \"])\" ; printf \"GULP.symbols= np.array([\" ; for ( i = 1 ; i <= natoms - 1 ; i ++ ){ printf \"'\" symbol [ i ] \"',\" }; print \"'\" symbol [ natoms ] \"'])\" ; } /Initial Cartesian coordinates :/ { for ( i = 1 ; i <= 6 ; i ++ ) getline while ( NF > 1 ){ coord [ $ 1 ] = \"[\" $ 4 \",\" $ 5 \",\" $ 6 \"]\" charge [ $ 1 ] =$ 7 symbol [ $ 1 ] =$ 2 natoms =$ 1 getline ; } print \"GULP.positions= np.array([\" ; for ( i = 1 ; i <= natoms - 1 ; i ++ ){ print \"\\t\" coord [ i ] \",\" }; print \"\\t\" coord [ i ] \"])\" } /Electrostatic potential at atomic positions :/ { for ( i = 1 ; i <= 6 ; i ++ ) getline while ( NF > 1 ){ gsub ( \"-\" , \" -\" , $ 0 ); split ( $ 0 , data ) aPOT [ $ 1 ] = data [ 4 ] + 0 aEF [ $ 1 ] = \"[\" data [ 5 ] + 0 \",\" data [ 6 ] + 0 \",\" data [ 7 ] + 0 \"]\" getline ; } printf \"GULP.EPOT= np.array([\" ; for ( i = 1 ; i <= natoms - 1 ; i ++ ){ print \"\\t\" aPOT [ i ] \",\" }; print \"\\t\" aPOT [ natoms ] \"])\" ; print \"GULP.EPOTgradient= np.array([\" ; for ( i = 1 ; i <= natoms - 1 ; i ++ ){ print \"\\t\" aEF [ i ] \",\" }; print \"\\t\" aEF [ i ] \"])\" } The output from the script looks like this \"data.py\" import numpy as np class GULP : def __init__ ( self ): self . ver = \"2014.01.28\" GULP . pbc = np . array ([ True , True , True ], dtype = bool ) GULP . cell = np . array ([ [ 8.93076 , 0 , 0 ], [ 0 , 5.95384 , 0 ], [ 0 , 0 , 23.42 ]]) GULP . scaled_positions = np . array ([ [ 0.578633 , 0.302123 , 0.218589 ], [ 0.762789 , 0.553684 , 0.312622 ], [ 0.078391 , 0.553354 , 0.313708 ], [ 0.242805 , 0.801983 , 0.220219 ], [ 0.078509 , 0.053341 , 0.313727 ], [ 0.578346 , 0.802044 , 0.218543 ], [ 0.764646 , 0.052150 , 0.315978 ], ... and here is the code that loads the data in my Python program # Load the data from GULP #========================================== from data import GULP structure = Atoms ( symbols = GULP . symbols , cell = GULP . cell , #positions= GULP.positions, scaled_positions = GULP . scaled_positions , pbc = GULP . pbc ) structure . charges = GULP . charges ; structure . EPOTgradient = GULP . EPOTgradient ; #========================================== Files GULP-out2python.awk data.py EF.gout - GULP program output","title":"Awk writes Python"},{"location":"Case_studies/awk_writes_python/#awk-writes-python","text":"This sounds strange. Why would one use awk to write Python code? Well, the reason is the same as before, \"the overheads of more sophisticated approaches are not worth the bother\" . My analysis program runs in Python but collecting my data from the output of another program (GULP in this case) requires some tedious work. The result is a small awk script that extracts the relevant values and writes a Python structure that takes a couple of lines to load. The code looks large and complicated, but if you just look closely, each search section is independent from the rest and addresses a single property of interest. With time, I kept adding more and more patterns to make the script more robust and generic. If you are still not convinced, just look how the GULP output looks like. In the file section, as usual, there is a complete set as an example. GULP-out2python.awk #!/usr/bin/awk -f BEGIN { print \"import numpy as np\\n\\nclass GULP:\\n\\tdef __init__(self):\\n\\t\\tself.ver=\\\"2015.04.15\\\"\" } /Dimensionality =/ { if ( $ 3 == 3 ) print \"GULP.pbc= np.array([ True, True, True], dtype=bool)\" if ( $ 3 == 2 ) print \"GULP.pbc= np.array([ True, True, False], dtype=bool)\" if ( $ 3 == 1 ) print \"GULP.pbc= np.array([ True, False, False], dtype=bool)\" if ( $ 3 == 0 ) print \"GULP.pbc= np.array([ False, False, False], dtype=bool)\" } /Cartesian lattice vectors \\(Angstroms\\) :/ { getline ; getline ; printf \"GULP.cell= np.array([\\n\\t[%g,%g,%g],\\n\" , $ 1 , $ 2 , $ 3 getline ; printf \"\\t[%g,%g,%g],\\n\" , $ 1 , $ 2 , $ 3 getline ; printf \"\\t[%g,%g,%g]])\\n\" , $ 1 , $ 2 , $ 3 } /Fractional coordinates of asymmetric unit/ { for ( i = 1 ; i <= 6 ; i ++ ) getline while ( NF > 1 ){ scoord [ $ 1 ] = \"[\" $ 4 \",\" $ 5 \",\" $ 6 \"]\" ; charge [ $ 1 ] =$ 7 ; if ( $ 2 == \"X1\" ) $ 2 = \"X\" symbol [ $ 1 ] =$ 2 ; natoms =$ 1 ; getline ; } print \"GULP.scaled_positions= np.array([\" ; for ( i = 1 ; i <= natoms - 1 ; i ++ ){ print \"\\t\" scoord [ i ] \",\" }; print \"\\t\" scoord [ i ] \"])\" printf \"GULP.charges= np.array([\" ; for ( i = 1 ; i <= natoms - 1 ; i ++ ){ print \"\\t\" charge [ i ] \",\" }; print \"\\t\" , charge [ natoms ] \"])\" ; printf \"GULP.symbols= np.array([\" ; for ( i = 1 ; i <= natoms - 1 ; i ++ ){ printf \"'\" symbol [ i ] \"',\" }; print \"'\" symbol [ natoms ] \"'])\" ; } /Initial Cartesian coordinates :/ { for ( i = 1 ; i <= 6 ; i ++ ) getline while ( NF > 1 ){ coord [ $ 1 ] = \"[\" $ 4 \",\" $ 5 \",\" $ 6 \"]\" charge [ $ 1 ] =$ 7 symbol [ $ 1 ] =$ 2 natoms =$ 1 getline ; } print \"GULP.positions= np.array([\" ; for ( i = 1 ; i <= natoms - 1 ; i ++ ){ print \"\\t\" coord [ i ] \",\" }; print \"\\t\" coord [ i ] \"])\" } /Electrostatic potential at atomic positions :/ { for ( i = 1 ; i <= 6 ; i ++ ) getline while ( NF > 1 ){ gsub ( \"-\" , \" -\" , $ 0 ); split ( $ 0 , data ) aPOT [ $ 1 ] = data [ 4 ] + 0 aEF [ $ 1 ] = \"[\" data [ 5 ] + 0 \",\" data [ 6 ] + 0 \",\" data [ 7 ] + 0 \"]\" getline ; } printf \"GULP.EPOT= np.array([\" ; for ( i = 1 ; i <= natoms - 1 ; i ++ ){ print \"\\t\" aPOT [ i ] \",\" }; print \"\\t\" aPOT [ natoms ] \"])\" ; print \"GULP.EPOTgradient= np.array([\" ; for ( i = 1 ; i <= natoms - 1 ; i ++ ){ print \"\\t\" aEF [ i ] \",\" }; print \"\\t\" aEF [ i ] \"])\" } The output from the script looks like this \"data.py\" import numpy as np class GULP : def __init__ ( self ): self . ver = \"2014.01.28\" GULP . pbc = np . array ([ True , True , True ], dtype = bool ) GULP . cell = np . array ([ [ 8.93076 , 0 , 0 ], [ 0 , 5.95384 , 0 ], [ 0 , 0 , 23.42 ]]) GULP . scaled_positions = np . array ([ [ 0.578633 , 0.302123 , 0.218589 ], [ 0.762789 , 0.553684 , 0.312622 ], [ 0.078391 , 0.553354 , 0.313708 ], [ 0.242805 , 0.801983 , 0.220219 ], [ 0.078509 , 0.053341 , 0.313727 ], [ 0.578346 , 0.802044 , 0.218543 ], [ 0.764646 , 0.052150 , 0.315978 ], ... and here is the code that loads the data in my Python program # Load the data from GULP #========================================== from data import GULP structure = Atoms ( symbols = GULP . symbols , cell = GULP . cell , #positions= GULP.positions, scaled_positions = GULP . scaled_positions , pbc = GULP . pbc ) structure . charges = GULP . charges ; structure . EPOTgradient = GULP . EPOTgradient ; #========================================== Files GULP-out2python.awk data.py EF.gout - GULP program output","title":"Awk writes Python"},{"location":"Case_studies/colors/","text":"Color output with custom keywords There are few tools to highlight different language syntax with colors and they are excellent tools for what they are designed. Some time I would like to use my own keywords for highlighting and this always turns to be a difficult task. Here is a small script that highlights provided keywords (or simple regular expressions) on the fly. For convenience, I made it possible to choose between some predefined color schemes. freq-dvr.out 3300.42 H2O_H-bonded DVR/snapshot-000100-281-59.xyz_H281-O279.dvr 3436.45 H2O_H-bonded DVR/snapshot-000150-280-59.xyz_H280-O279.dvr 3382.03 H2O_H-bonded DVR/snapshot-000200-280-59.xyz_H280-O279.dvr 3360.05 H2O_H-bonded DVR/snapshot-000350-280-59.xyz_H280-O279.dvr 3709.4 NOT_H-bonded DVR/snapshot-002850-181.xyz_H181-O180.dvr 3647.54 NOT_H-bonded DVR/snapshot-002900-181.xyz_H181-O180.dvr 3683.93 NOT_H-bonded DVR/snapshot-003100-191-181-199.xyz_H181-O180.dvr 3703.93 NOT_H-bonded DVR/snapshot-004100-224.xyz_H224-O222.dvr 3670.12 NOT_H-bonded DVR/snapshot-004150-224.xyz_H224-O222.dvr 3654.3 NOT_H-bonded DVR/snapshot-004200-224.xyz_H224-O222.dvr 3710.58 NOT_H-bonded DVR/snapshot-004250-224.xyz_H224-O222.dvr 3643.06 CO2_H-bonded DVR/snapshot-004350-134-199.xyz_H134-O132.dvr 3535.59 H2O_H-bonded DVR/snapshot-004500-133.xyz_H133-O132.dvr 3668.44 NOT_H-bonded DVR/snapshot-004600-134-199.xyz_H134-O132.dvr 3638.55 NOT_H-bonded DVR/snapshot-004650-118-134-199.xyz_H134-O132.dvr 3576.47 H2O_H-bonded DVR/snapshot-004750-233-133.xyz_H133-O132.dvr 3735.93 NOT_H-bonded DVR/snapshot-004750-233-133.xyz_H233-O231.dvr 3625.43 H2O_H-bonded DVR/snapshot-004800-233-133-118.xyz_H133-O132.dvr 3740.8 NOT_H-bonded DVR/snapshot-004800-233-133-118.xyz_H233-O231.dvr 3673.06 H2O_H-bonded DVR/snapshot-004850-233-199.xyz_H233-O231.dvr 3723.15 NOT_H-bonded DVR/snapshot-004900-233-199.xyz_H233-O231.dvr 3703.71 NOT_H-bonded DVR/snapshot-004950-200-233.xyz_H233-O231.dvr 3682.4 CO2_H-bonded DVR/snapshot-005000-200-233.xyz_H233-O231.dvr 3693.42 NOT_H-bonded DVR/snapshot-005050-233.xyz_H233-O231.dvr 3551.17 H2O_H-bonded DVR/snapshot-005950-233.xyz_H233-O231.dvr 3704.16 NOT_H-bonded DVR/snapshot-006350-200-62.xyz_H062-O060.dvr 3732.31 NOT_H-bonded DVR/snapshot-006400-200-62.xyz_H062-O060.dvr 3705.08 NOT_H-bonded DVR/snapshot-006450-62.xyz_H062-O060.dvr 3724.14 NOT_H-bonded DVR/snapshot-006500-62.xyz_H062-O060.dvr 3658.92 NOT_H-bonded DVR/snapshot-006550-62.xyz_H062-O060.dvr 3700.88 H2O_H-bonded DVR/snapshot-006600-62.xyz_H062-O060.dvr 3707.23 H2O_H-bonded DVR/snapshot-006650-62.xyz_H062-O060.dvr 3645.98 H2O_H-bonded DVR/snapshot-006700-200-62.xyz_H062-O060.dvr 3709.66 NOT_H-bonded DVR/snapshot-006750-200-62.xyz_H062-O060.dvr 3681.21 NOT_H-bonded DVR/snapshot-006800-200-62.xyz_H062-O060.dvr 3682.88 NOT_H-bonded DVR/snapshot-006850-200-62.xyz_H062-O060.dvr 3704.9 H2O_H-bonded DVR/snapshot-006900-200-62.xyz_H062-O060.dvr 3600.96 NOT_H-bonded DVR/snapshot-007250-272.xyz_H272-O270.dvr 3663.54 NOT_H-bonded DVR/snapshot-007350-62.xyz_H062-O060.dvr 3710.52 NOT_H-bonded DVR/snapshot-007400-200-62.xyz_H062-O060.dvr 3708.11 NOT_H-bonded DVR/snapshot-007450-62.xyz_H062-O060.dvr 3717.28 NOT_H-bonded DVR/snapshot-007500-62.xyz_H062-O060.dvr 3751 NOT_H-bonded DVR/snapshot-007550-62.xyz_H062-O060.dvr 3722 NOT_H-bonded DVR/snapshot-007900-223.xyz_H223-O222.dvr 3703.76 CO2_H-bonded DVR/snapshot-007950-223.xyz_H223-O222.dvr 3805.29 CO2_H-bonded DVR/snapshot-008000-223.xyz_H223-O222.dvr 3651.37 CO2_H-bonded DVR/snapshot-008050-223.xyz_H223-O222.dvr 3766.86 CO2_H-bonded DVR/snapshot-008100-223.xyz_H223-O222.dvr 3698.48 CO2_H-bonded DVR/snapshot-008150-223.xyz_H223-O222.dvr 3692.41 CO2_H-bonded DVR/snapshot-008200-223.xyz_H223-O222.dvr $ cat freq-dvr.out | color.awk 2H2O_H-bonded 1NOT_H-bonded 3CO2_H-bonded color.awk #!/usr/bin/awk -f BEGIN { nARGC = ARGC ; ARGC = 1 # Trick the command line to ignore the files and use them as options c [ 0 ] = \"\\033[97;41m\" # white-red c [ 1 ] = \"\\033[31;1m\" # red c [ 2 ] = \"\\033[32;1m\" # green c [ 3 ] = \"\\033[33;1m\" # yellow c [ 4 ] = \"\\033[34;1m\" # blue c [ 5 ] = \"\\033[35;1m\" # magenta c [ 6 ] = \"\\033[36;1m\" # cyan c [ 7 ] = \"\\033[93;41m\" # yellow-red cn = \"\\033[0m\" # reset } { for ( i = 1 ; i < nARGC ; i ++ ){ ci = ARGV [ i ] + 0 # color index m = ARGV [ i ] # match string sub ( ci , \"\" , m ) # remove color index gsub ( m , c [ ci ] \"&\" cn ) # insert color codes } print $ 0 } The color scheme is selected by number before the matching string, which limits a bit the functionality, but it keeps the script simple... Here is an alternative script that has some general keywords and matching criteria predefined. It is straight forward to add your own patterns or remove the unnecessary ones. #!/usr/bin/awk -f BEGIN { c [ 0 ] = \"\\033[97;41m\" # white-red c [ 1 ] = \"\\033[31;1m\" # red c [ 2 ] = \"\\033[32;1m\" # green c [ 3 ] = \"\\033[33;1m\" # yellow c [ 4 ] = \"\\033[34;1m\" # blue c [ 5 ] = \"\\033[35;1m\" # magenta c [ 6 ] = \"\\033[36;1m\" # cyan c [ 7 ] = \"\\033[93;41m\" # yellow-red cn = \"\\033[0m\" # reset } { gsub ( /WARNING|Warning|warning/ , c [ 2 ] \"&\" cn ) # Warning gsub ( /ERROR|Error|error/ , c [ 0 ] \"&\" cn ) # Error gsub ( /FAIL|Fail|fail|FAILED|Failed|failed/ , c [ 0 ] \"&\" cn ) # Failed gsub ( /([0-9]{1,3}\\.){3}[0-9]{1,3}/ , c [ 2 ] \"&\" cn ) # IP4 address gsub ( /([0-9a-fA-F]{2}[:-]){5}[0-9a-fA-F]{2}/ , c [ 4 ] \"&\" cn ) # MAC gsub ( /[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}/ , c [ 6 ] \"&\" cn ) # e-mail gsub ( /https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)/ , c [ 4 ] \"&\" cn ) # web address $ 0 = gensub ( /(^|[[ ]){1}(OK|Ok)([] ]|$){1}/ , \"\\\\1\" c [ 2 ] \"\\\\2\" cn \"\\\\3\" , \"g\" ) # OK - special case gsub ( /[&@][A-Za-z_]+/ , c [ 3 ] \"&\" cn ) # @& gsub ( /^#SBATCH.*$/ , c [ 6 ] \"&\" cn ) # SBATCH gsub ( /^#.*$/ , c [ 4 ] \"&\" cn ) # comment print $ 0 }","title":"Color output with custom keywords"},{"location":"Case_studies/colors/#color-output-with-custom-keywords","text":"There are few tools to highlight different language syntax with colors and they are excellent tools for what they are designed. Some time I would like to use my own keywords for highlighting and this always turns to be a difficult task. Here is a small script that highlights provided keywords (or simple regular expressions) on the fly. For convenience, I made it possible to choose between some predefined color schemes. freq-dvr.out 3300.42 H2O_H-bonded DVR/snapshot-000100-281-59.xyz_H281-O279.dvr 3436.45 H2O_H-bonded DVR/snapshot-000150-280-59.xyz_H280-O279.dvr 3382.03 H2O_H-bonded DVR/snapshot-000200-280-59.xyz_H280-O279.dvr 3360.05 H2O_H-bonded DVR/snapshot-000350-280-59.xyz_H280-O279.dvr 3709.4 NOT_H-bonded DVR/snapshot-002850-181.xyz_H181-O180.dvr 3647.54 NOT_H-bonded DVR/snapshot-002900-181.xyz_H181-O180.dvr 3683.93 NOT_H-bonded DVR/snapshot-003100-191-181-199.xyz_H181-O180.dvr 3703.93 NOT_H-bonded DVR/snapshot-004100-224.xyz_H224-O222.dvr 3670.12 NOT_H-bonded DVR/snapshot-004150-224.xyz_H224-O222.dvr 3654.3 NOT_H-bonded DVR/snapshot-004200-224.xyz_H224-O222.dvr 3710.58 NOT_H-bonded DVR/snapshot-004250-224.xyz_H224-O222.dvr 3643.06 CO2_H-bonded DVR/snapshot-004350-134-199.xyz_H134-O132.dvr 3535.59 H2O_H-bonded DVR/snapshot-004500-133.xyz_H133-O132.dvr 3668.44 NOT_H-bonded DVR/snapshot-004600-134-199.xyz_H134-O132.dvr 3638.55 NOT_H-bonded DVR/snapshot-004650-118-134-199.xyz_H134-O132.dvr 3576.47 H2O_H-bonded DVR/snapshot-004750-233-133.xyz_H133-O132.dvr 3735.93 NOT_H-bonded DVR/snapshot-004750-233-133.xyz_H233-O231.dvr 3625.43 H2O_H-bonded DVR/snapshot-004800-233-133-118.xyz_H133-O132.dvr 3740.8 NOT_H-bonded DVR/snapshot-004800-233-133-118.xyz_H233-O231.dvr 3673.06 H2O_H-bonded DVR/snapshot-004850-233-199.xyz_H233-O231.dvr 3723.15 NOT_H-bonded DVR/snapshot-004900-233-199.xyz_H233-O231.dvr 3703.71 NOT_H-bonded DVR/snapshot-004950-200-233.xyz_H233-O231.dvr 3682.4 CO2_H-bonded DVR/snapshot-005000-200-233.xyz_H233-O231.dvr 3693.42 NOT_H-bonded DVR/snapshot-005050-233.xyz_H233-O231.dvr 3551.17 H2O_H-bonded DVR/snapshot-005950-233.xyz_H233-O231.dvr 3704.16 NOT_H-bonded DVR/snapshot-006350-200-62.xyz_H062-O060.dvr 3732.31 NOT_H-bonded DVR/snapshot-006400-200-62.xyz_H062-O060.dvr 3705.08 NOT_H-bonded DVR/snapshot-006450-62.xyz_H062-O060.dvr 3724.14 NOT_H-bonded DVR/snapshot-006500-62.xyz_H062-O060.dvr 3658.92 NOT_H-bonded DVR/snapshot-006550-62.xyz_H062-O060.dvr 3700.88 H2O_H-bonded DVR/snapshot-006600-62.xyz_H062-O060.dvr 3707.23 H2O_H-bonded DVR/snapshot-006650-62.xyz_H062-O060.dvr 3645.98 H2O_H-bonded DVR/snapshot-006700-200-62.xyz_H062-O060.dvr 3709.66 NOT_H-bonded DVR/snapshot-006750-200-62.xyz_H062-O060.dvr 3681.21 NOT_H-bonded DVR/snapshot-006800-200-62.xyz_H062-O060.dvr 3682.88 NOT_H-bonded DVR/snapshot-006850-200-62.xyz_H062-O060.dvr 3704.9 H2O_H-bonded DVR/snapshot-006900-200-62.xyz_H062-O060.dvr 3600.96 NOT_H-bonded DVR/snapshot-007250-272.xyz_H272-O270.dvr 3663.54 NOT_H-bonded DVR/snapshot-007350-62.xyz_H062-O060.dvr 3710.52 NOT_H-bonded DVR/snapshot-007400-200-62.xyz_H062-O060.dvr 3708.11 NOT_H-bonded DVR/snapshot-007450-62.xyz_H062-O060.dvr 3717.28 NOT_H-bonded DVR/snapshot-007500-62.xyz_H062-O060.dvr 3751 NOT_H-bonded DVR/snapshot-007550-62.xyz_H062-O060.dvr 3722 NOT_H-bonded DVR/snapshot-007900-223.xyz_H223-O222.dvr 3703.76 CO2_H-bonded DVR/snapshot-007950-223.xyz_H223-O222.dvr 3805.29 CO2_H-bonded DVR/snapshot-008000-223.xyz_H223-O222.dvr 3651.37 CO2_H-bonded DVR/snapshot-008050-223.xyz_H223-O222.dvr 3766.86 CO2_H-bonded DVR/snapshot-008100-223.xyz_H223-O222.dvr 3698.48 CO2_H-bonded DVR/snapshot-008150-223.xyz_H223-O222.dvr 3692.41 CO2_H-bonded DVR/snapshot-008200-223.xyz_H223-O222.dvr $ cat freq-dvr.out | color.awk 2H2O_H-bonded 1NOT_H-bonded 3CO2_H-bonded color.awk #!/usr/bin/awk -f BEGIN { nARGC = ARGC ; ARGC = 1 # Trick the command line to ignore the files and use them as options c [ 0 ] = \"\\033[97;41m\" # white-red c [ 1 ] = \"\\033[31;1m\" # red c [ 2 ] = \"\\033[32;1m\" # green c [ 3 ] = \"\\033[33;1m\" # yellow c [ 4 ] = \"\\033[34;1m\" # blue c [ 5 ] = \"\\033[35;1m\" # magenta c [ 6 ] = \"\\033[36;1m\" # cyan c [ 7 ] = \"\\033[93;41m\" # yellow-red cn = \"\\033[0m\" # reset } { for ( i = 1 ; i < nARGC ; i ++ ){ ci = ARGV [ i ] + 0 # color index m = ARGV [ i ] # match string sub ( ci , \"\" , m ) # remove color index gsub ( m , c [ ci ] \"&\" cn ) # insert color codes } print $ 0 } The color scheme is selected by number before the matching string, which limits a bit the functionality, but it keeps the script simple... Here is an alternative script that has some general keywords and matching criteria predefined. It is straight forward to add your own patterns or remove the unnecessary ones. #!/usr/bin/awk -f BEGIN { c [ 0 ] = \"\\033[97;41m\" # white-red c [ 1 ] = \"\\033[31;1m\" # red c [ 2 ] = \"\\033[32;1m\" # green c [ 3 ] = \"\\033[33;1m\" # yellow c [ 4 ] = \"\\033[34;1m\" # blue c [ 5 ] = \"\\033[35;1m\" # magenta c [ 6 ] = \"\\033[36;1m\" # cyan c [ 7 ] = \"\\033[93;41m\" # yellow-red cn = \"\\033[0m\" # reset } { gsub ( /WARNING|Warning|warning/ , c [ 2 ] \"&\" cn ) # Warning gsub ( /ERROR|Error|error/ , c [ 0 ] \"&\" cn ) # Error gsub ( /FAIL|Fail|fail|FAILED|Failed|failed/ , c [ 0 ] \"&\" cn ) # Failed gsub ( /([0-9]{1,3}\\.){3}[0-9]{1,3}/ , c [ 2 ] \"&\" cn ) # IP4 address gsub ( /([0-9a-fA-F]{2}[:-]){5}[0-9a-fA-F]{2}/ , c [ 4 ] \"&\" cn ) # MAC gsub ( /[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,4}/ , c [ 6 ] \"&\" cn ) # e-mail gsub ( /https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)/ , c [ 4 ] \"&\" cn ) # web address $ 0 = gensub ( /(^|[[ ]){1}(OK|Ok)([] ]|$){1}/ , \"\\\\1\" c [ 2 ] \"\\\\2\" cn \"\\\\3\" , \"g\" ) # OK - special case gsub ( /[&@][A-Za-z_]+/ , c [ 3 ] \"&\" cn ) # @& gsub ( /^#SBATCH.*$/ , c [ 6 ] \"&\" cn ) # SBATCH gsub ( /^#.*$/ , c [ 4 ] \"&\" cn ) # comment print $ 0 }","title":"Color output with custom keywords"},{"location":"Case_studies/manipulating_vcf/","text":"Manipulating the output from a genome analysis - vcf and gff Problem formulated an presented at the workshop by Jonas S\u00f6derberg , Department of Cell and Molecular Biology, Molecular Evolution We have a comparison between a number of different fly cell lines. These are found in a huge vcf file (dgrp2.vcf). We also have the annotations in a gff3 file (Drosophila_melanogaster.BDGP6.28.101.chr.gff3) and the genome itself in a fasta file. Getting the RAW files The annotation of the fly genome: Find the GFF3 here . The genome sequence used in this project: The genome is located here . The comparison of all the variants in the cell lines in the freeze project Finally, here is the VCF file. Starting to work with the data Let's say we want to find out all genes that contains a variant and all variants that are located within a gene. What do we want to do first? Take a look at the vcf file. That is the one that contains all the variants. Then look at the gff file, which contains the genes and other annotations. Finally, take a look at the DNA sequence. You will need to combine all three to answer the question. Also, to make this faster, lets just look at chromosome 4 , which means we have to extract that data as well. What do we need to get Fles containing only chromosome 4 Positions for SNPs and INDELs Positions for genes and CDSs Separation of variants (SNPs and INDELs) into two groups, inside and outside genes (and CDSs) Separation of genes/CDSs into those with and without variants (and maybe how many there are per gene) Tips before starting For readability of the vcf file: awk '{print $1\"\\t\"$2\"\\t\"$3\"\\t\"$4\"\\t\"$5\"\\t\"$6\"\\t\"$7\"\\t\"$8\"\\t\"$9}' dgrp2.vcf > dgrp2_trimmed.vcf & If your awk takes too long to run (10h or so) - Make sure you have the latest awk, and also you might want to think about parallelisation. My solutions also work for files with more than one chromosome, so in some cases they are longer than needed for your exercise. I left it so on purpose. SNPs and INDELs are collectively named variants Genes and CoDing Sequences (CDSs) are sometimes just collectively called genes The exercise Identify the steps you need to do and what each step does. Open the hints if you get stuck. Chromosome 4 Hint , what do we need? Extract chr4 from the vcf and the gff and make new files Hint All lines from chromosome 4 start with a 4 Solution awk '/^4/{print $0}' Drosophila_melanogaster.BDGP6.28.101.chr.gff3 > Drosophila_melanogaster.chr4.gff3 awk '/^4/{print $0}' dgrp2_trimmed.vcf > dgrp2_chr4.vcf Follow-up task: Count and sort the different genomic features in chromosome 4 by number. Task result example 1 chromosome 1 snoRNA 2 pre_miRNA 7 pseudogene 11 pseudogenic_transcript 26 ncRNA_gene 31 ncRNA 79 gene 295 mRNA 338 three_prime_UTR 571 five_prime_UTR 2740 CDS 3155 exon SNPs and INDELs Hint , which output do we want? Get distribution of variants and list them in two separate files. For a bonus plot of the lengths of the INDELS, get the length of all INDELS into a third file Hint Remove lines beginning with # (grep) Hint If columns 4 and 5 have different length, it's an INDEL. Otherwise it's a SNP. Hint You want the output to be a file with columns 1, 2, 4 and 5, a classifier (SNP or INDEL) and finally the length of the INDEL (put \"-\" for the SNPs) Solution cat dgrp2_chr4.vcf | grep -v \"#\" | awk '{if (length($4)>1||length($5)>1){a=\"INDEL\";b=length($4)-length($5);cnt[b]+=1;} else {a=\"SNP\";b=\"-\";} printf(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\", $1, $2, a, b, $4, $5) > \"indels_Drosophila_chr4\";}END{for (x in cnt){print x,cnt[x] > \"distr_Drosophila_chr4\"}}' Follow-up task: Print nucleotide substitution that these SNPs introduce sorted by number. Remember the coins... Task result example 1182 C->T 1133 G->A 932 A->G 929 A->T 892 T->A 880 T->C 639 G->T 621 C->A 436 A->C 396 T->G 372 G->C 357 C->G Genes with variants Hint , how do we get those? Compare back and separate the annotation into features that do and don\u2019t have variants. For a bonus, also record the number of variants in each feature Hint Make an index using the previous output to identify positions of variants Hint For each feature in the gff, check all position it covers to see if they are in your index, if so print to one file. If not, print to another. Solution awk 'FNR==NR{a[$1,$2]=\"T\"; next}{ hits=0; for(N=$4; N<=$5; N++) { if (a[$1,N] == \"T\") {hits+=1}} if (hits>0) {print hits \"\\t\" $0 > \"haveSNPINDEL_Drosophila_chr4.gff\"} else {print $0 > \"noSNPINDEL_Drosophila_chr4.gff\"}}' indels_Drosophila_chr4 Drosophila_melanogaster.chr4.gff3 Follow-up task: Count and sort the SNPs (not INDELs) in your output and compare to the output from the first step. Task result example 1 chromosome 1 pre_miRNA 1 snoRNA 6 pseudogene 9 pseudogenic_transcript 22 ncRNA_gene 28 ncRNA 79 gene 264 three_prime_UTR 290 five_prime_UTR 295 mRNA 1798 CDS 2181 exon Genes/CDSs only Hint , what features do we look for? Filter for genes and CDSs before doing the analysis. Hint Only genes and CDSs are interesting to us. Make a gff without the rest of the features. Solution awk '{if ($3==\"gene\" || $3==\"CDS\") print $0}' Drosophila_melanogaster.chr4.gff3 > Drosophila_melanogaster.chr4_genesCDSs.gff3 Final list of variants Hint , how do we classify the variants? Repeat step 3 for the SNPs/INDELs themselves, to see which are actually located inside genes Hint Make an index of all genes/CDSs (from your gff), where start and stop are paired Hint For each feature from your step 2 file, check the position agains the index and print whether or not the variant is inside a gene. Solution awk 'FNR==NR{ingene[$1,$4]=$5; next}{state=\"Not in gene\";for (pair in ingene) {split(pair, t, SUBSEP); if ($1==t[1] && $2>=t[2] && $2<=ingene[t[1],t[2]]) {state=(t[1] \" \" t[2] \" \" ingene[t[1],t[2]])}} print $0, \" \", state }' Drosophila_melanogaster.chr4_genesCDSs.gff3 indels_Drosophila_chr4 > SNPsInGenes_Drosophila_ch4 Final result Here we can see all SNPs and INDELs which are inside a relevant region. We have successfully made two gff:s containing all gene positions for genes with variants and genes without. Along the way we also got a list of all genes that contain variants too. Extra task Re-do the files but this time include the gene ID (i.e. FBtr0089178 from column nine) and translate that into the full gene name found in this file . Solution awk 'FNR==1{++fileidx} fileidx==1{split($9,a,\";|:\");ingene[$1,$4,$5]=a[2]; next} fileidx==2{FS=\"\\t\";name[$3]=$5} fileidx==3{state=\"Not in gene\";for (trip in ingene) {split(trip, t, SUBSEP); if ($1==t[1] && $2>=t[2] && $2<=t[3]) {state=(t[1] \"\\t\" t[2] \"\\t\" t[3] \"\\t\" name[ingene[t[1],t[2],t[3]]])}} print $0, \"\\t\", state }' Drosophila_melanogaster.chr4_genesCDSs.gff3 fbgn_fbtr_fbpp_expanded_fb_2020_06.tsv indels_Drosophila_chr4 > SNPsInNamedGenes_Drosophila_ch4 Look at the distribution of genes: awk -F\"\\t\" '{print $10}' SNPsInNamedGenes_Drosophila_ch4 | sort | uniq -c | sort -n","title":"Manipulating the output from a genome analysis - vcf and gff"},{"location":"Case_studies/manipulating_vcf/#manipulating-the-output-from-a-genome-analysis-vcf-and-gff","text":"Problem formulated an presented at the workshop by Jonas S\u00f6derberg , Department of Cell and Molecular Biology, Molecular Evolution We have a comparison between a number of different fly cell lines. These are found in a huge vcf file (dgrp2.vcf). We also have the annotations in a gff3 file (Drosophila_melanogaster.BDGP6.28.101.chr.gff3) and the genome itself in a fasta file.","title":"Manipulating the output from a genome analysis - vcf and gff"},{"location":"Case_studies/manipulating_vcf/#getting-the-raw-files","text":"","title":"Getting the RAW files"},{"location":"Case_studies/manipulating_vcf/#the-annotation-of-the-fly-genome","text":"Find the GFF3 here .","title":"The annotation of the fly genome:"},{"location":"Case_studies/manipulating_vcf/#the-genome-sequence-used-in-this-project","text":"The genome is located here .","title":"The genome sequence used in this project:"},{"location":"Case_studies/manipulating_vcf/#the-comparison-of-all-the-variants-in-the-cell-lines-in-the-freeze-project","text":"Finally, here is the VCF file.","title":"The comparison of all the variants in the cell lines in the freeze project"},{"location":"Case_studies/manipulating_vcf/#starting-to-work-with-the-data","text":"Let's say we want to find out all genes that contains a variant and all variants that are located within a gene. What do we want to do first? Take a look at the vcf file. That is the one that contains all the variants. Then look at the gff file, which contains the genes and other annotations. Finally, take a look at the DNA sequence. You will need to combine all three to answer the question. Also, to make this faster, lets just look at chromosome 4 , which means we have to extract that data as well.","title":"Starting to work with the data"},{"location":"Case_studies/manipulating_vcf/#what-do-we-need-to-get","text":"Fles containing only chromosome 4 Positions for SNPs and INDELs Positions for genes and CDSs Separation of variants (SNPs and INDELs) into two groups, inside and outside genes (and CDSs) Separation of genes/CDSs into those with and without variants (and maybe how many there are per gene)","title":"What do we need to get"},{"location":"Case_studies/manipulating_vcf/#tips-before-starting","text":"For readability of the vcf file: awk '{print $1\"\\t\"$2\"\\t\"$3\"\\t\"$4\"\\t\"$5\"\\t\"$6\"\\t\"$7\"\\t\"$8\"\\t\"$9}' dgrp2.vcf > dgrp2_trimmed.vcf & If your awk takes too long to run (10h or so) - Make sure you have the latest awk, and also you might want to think about parallelisation. My solutions also work for files with more than one chromosome, so in some cases they are longer than needed for your exercise. I left it so on purpose. SNPs and INDELs are collectively named variants Genes and CoDing Sequences (CDSs) are sometimes just collectively called genes","title":"Tips before starting"},{"location":"Case_studies/manipulating_vcf/#the-exercise","text":"Identify the steps you need to do and what each step does. Open the hints if you get stuck.","title":"The exercise"},{"location":"Case_studies/manipulating_vcf/#chromosome-4","text":"Hint , what do we need? Extract chr4 from the vcf and the gff and make new files Hint All lines from chromosome 4 start with a 4 Solution awk '/^4/{print $0}' Drosophila_melanogaster.BDGP6.28.101.chr.gff3 > Drosophila_melanogaster.chr4.gff3 awk '/^4/{print $0}' dgrp2_trimmed.vcf > dgrp2_chr4.vcf","title":"Chromosome 4"},{"location":"Case_studies/manipulating_vcf/#follow-up-task","text":"Count and sort the different genomic features in chromosome 4 by number. Task result example 1 chromosome 1 snoRNA 2 pre_miRNA 7 pseudogene 11 pseudogenic_transcript 26 ncRNA_gene 31 ncRNA 79 gene 295 mRNA 338 three_prime_UTR 571 five_prime_UTR 2740 CDS 3155 exon","title":"Follow-up task:"},{"location":"Case_studies/manipulating_vcf/#snps-and-indels","text":"Hint , which output do we want? Get distribution of variants and list them in two separate files. For a bonus plot of the lengths of the INDELS, get the length of all INDELS into a third file Hint Remove lines beginning with # (grep) Hint If columns 4 and 5 have different length, it's an INDEL. Otherwise it's a SNP. Hint You want the output to be a file with columns 1, 2, 4 and 5, a classifier (SNP or INDEL) and finally the length of the INDEL (put \"-\" for the SNPs) Solution cat dgrp2_chr4.vcf | grep -v \"#\" | awk '{if (length($4)>1||length($5)>1){a=\"INDEL\";b=length($4)-length($5);cnt[b]+=1;} else {a=\"SNP\";b=\"-\";} printf(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\", $1, $2, a, b, $4, $5) > \"indels_Drosophila_chr4\";}END{for (x in cnt){print x,cnt[x] > \"distr_Drosophila_chr4\"}}'","title":"SNPs and INDELs"},{"location":"Case_studies/manipulating_vcf/#follow-up-task_1","text":"Print nucleotide substitution that these SNPs introduce sorted by number. Remember the coins... Task result example 1182 C->T 1133 G->A 932 A->G 929 A->T 892 T->A 880 T->C 639 G->T 621 C->A 436 A->C 396 T->G 372 G->C 357 C->G","title":"Follow-up task:"},{"location":"Case_studies/manipulating_vcf/#genes-with-variants","text":"Hint , how do we get those? Compare back and separate the annotation into features that do and don\u2019t have variants. For a bonus, also record the number of variants in each feature Hint Make an index using the previous output to identify positions of variants Hint For each feature in the gff, check all position it covers to see if they are in your index, if so print to one file. If not, print to another. Solution awk 'FNR==NR{a[$1,$2]=\"T\"; next}{ hits=0; for(N=$4; N<=$5; N++) { if (a[$1,N] == \"T\") {hits+=1}} if (hits>0) {print hits \"\\t\" $0 > \"haveSNPINDEL_Drosophila_chr4.gff\"} else {print $0 > \"noSNPINDEL_Drosophila_chr4.gff\"}}' indels_Drosophila_chr4 Drosophila_melanogaster.chr4.gff3","title":"Genes with variants"},{"location":"Case_studies/manipulating_vcf/#follow-up-task_2","text":"Count and sort the SNPs (not INDELs) in your output and compare to the output from the first step. Task result example 1 chromosome 1 pre_miRNA 1 snoRNA 6 pseudogene 9 pseudogenic_transcript 22 ncRNA_gene 28 ncRNA 79 gene 264 three_prime_UTR 290 five_prime_UTR 295 mRNA 1798 CDS 2181 exon","title":"Follow-up task:"},{"location":"Case_studies/manipulating_vcf/#genescdss-only","text":"Hint , what features do we look for? Filter for genes and CDSs before doing the analysis. Hint Only genes and CDSs are interesting to us. Make a gff without the rest of the features. Solution awk '{if ($3==\"gene\" || $3==\"CDS\") print $0}' Drosophila_melanogaster.chr4.gff3 > Drosophila_melanogaster.chr4_genesCDSs.gff3","title":"Genes/CDSs only"},{"location":"Case_studies/manipulating_vcf/#final-list-of-variants","text":"Hint , how do we classify the variants? Repeat step 3 for the SNPs/INDELs themselves, to see which are actually located inside genes Hint Make an index of all genes/CDSs (from your gff), where start and stop are paired Hint For each feature from your step 2 file, check the position agains the index and print whether or not the variant is inside a gene. Solution awk 'FNR==NR{ingene[$1,$4]=$5; next}{state=\"Not in gene\";for (pair in ingene) {split(pair, t, SUBSEP); if ($1==t[1] && $2>=t[2] && $2<=ingene[t[1],t[2]]) {state=(t[1] \" \" t[2] \" \" ingene[t[1],t[2]])}} print $0, \" \", state }' Drosophila_melanogaster.chr4_genesCDSs.gff3 indels_Drosophila_chr4 > SNPsInGenes_Drosophila_ch4","title":"Final list of variants"},{"location":"Case_studies/manipulating_vcf/#final-result","text":"Here we can see all SNPs and INDELs which are inside a relevant region. We have successfully made two gff:s containing all gene positions for genes with variants and genes without. Along the way we also got a list of all genes that contain variants too.","title":"Final result"},{"location":"Case_studies/manipulating_vcf/#extra-task","text":"Re-do the files but this time include the gene ID (i.e. FBtr0089178 from column nine) and translate that into the full gene name found in this file . Solution awk 'FNR==1{++fileidx} fileidx==1{split($9,a,\";|:\");ingene[$1,$4,$5]=a[2]; next} fileidx==2{FS=\"\\t\";name[$3]=$5} fileidx==3{state=\"Not in gene\";for (trip in ingene) {split(trip, t, SUBSEP); if ($1==t[1] && $2>=t[2] && $2<=t[3]) {state=(t[1] \"\\t\" t[2] \"\\t\" t[3] \"\\t\" name[ingene[t[1],t[2],t[3]]])}} print $0, \"\\t\", state }' Drosophila_melanogaster.chr4_genesCDSs.gff3 fbgn_fbtr_fbpp_expanded_fb_2020_06.tsv indels_Drosophila_chr4 > SNPsInNamedGenes_Drosophila_ch4 Look at the distribution of genes: awk -F\"\\t\" '{print $10}' SNPsInNamedGenes_Drosophila_ch4 | sort | uniq -c | sort -n","title":"Extra task"},{"location":"Case_studies/multiple_files_I/","text":"Multiple input files - first approach The script will collect the data in sequence and store the data in memory, then print the arranged data at the end. Here is an example problem that is easy to solve with awk. Let's assume that we have collected some data about some persons. It is not systematic, so the data files are not complete and rows are not in the same order... file: 1.dat Daniel 10 Anders 7 Sven 56 Ali 17 Peter 6 file: 2.dat Peter Monday Sven Sunday David Tuesday Let's put them together! If the files had all names with missing data marked - sorting the files and pasting them together will essentially be enough. Below, it is just one possible way to do it. First we need to have a list of all names, collect the data, then try to print whatever we have collected. Warning awk under OS X is not fully compatible with Gawk (GNU awk) and the internal variable ARGIND is not available - the script will not work as intended. script.awk #!/usr/bin/awk -f { names [ $ 1 ] = 1 ; data [ $ 1 ][ ARGIND ] = $ 2 } END { for ( i in names ) print i \"\\t\\t\" data [ i ][ 1 ] \"\\t\\t\" data [ i ][ 2 ] } Run the script like this: ./script.awk 1 .dat 2 .dat ` The script runs over the two files in a row and on each line it uses associative arrays to collect the names from the first column in names [ $ 1 ] . data [ $ 1 ][ ARGIND ] is two dimensional array with indexes [name][ number of current file/argument]. At the end we will have elements like this: ... data [ \"Sven\" ][ 1 ] = 56 data [ \"Sven\" ][ 2 ] = \"Sunday\" ... The END section runs over the collected names and prints the collected data - so we recover as much as possible from both files. Here is the output. Anders 7 Daniel 10 Ali 17 Sven 56 Sunday David Tuesday Peter 6 Monday Alternative, perhaps better solution in this case, might the specially written tool for this purpose i.e. $ join -a1 -a2 -j 1 -o 0 ,1.2,2.2 -e \"NULL\" < ( sort 1 .dat ) < ( sort 2 .dat ) Ali 17 NULL Anders 7 NULL Daniel 10 NULL David NULL Tuesday Peter 6 Monday Sven 56 Sunday Note Note that the files need to be sorted by the field they will be joined, since the program is trying to avoid loading the whole data in the memory. If the data is sorted, awk also can join the data without loading it into the memory. http://unix.stackexchange.com/questions/194968/why-isnt-this-awk-command-doing-a-full-outer-join Credits to Mahesh Panchal for the tip Exercise Copy/paste the text in to two files with the suggested names scientific 2 | Bacteria | Bacteria <bacteria> | scientific name | 29 | Myxococcales | | scientific name | 139 | Borreliella burgdorferi | | scientific name | 161 | Treponema pallidum subsp. pallidum | | scientific name | 168 | Treponema pallidum subsp. pertenue | | scientific name | 356 | Rhizobiales | | scientific name | 638 | Arsenophonus nasoniae | | scientific name | genbank 2 | eubacteria | | genbank common name | 29 | fruiting gliding bacteria | | genbank common name | 139 | Lyme disease spirochete | | genbank common name | 161 | syphilis treponeme | | genbank common name | 168 | yaws treponeme | | genbank common name | 356 | rhizobacteria | | genbank common name | 638 | son-killer infecting Nasonia vitripennis | | genbank common name | Can you join the information from both files to collect the data in better format? ID | scientific name | genbank common name 2 | Bacteria | eubacteria 29 | Myxococcales | fruiting gliding bacteria 139 | Borreliella burgdorferi | Lyme disease spirochete Leave the extra blanks for the first attempt. We will use this problem (cleaning the remaining spaces before and after) to introduce user defined functions. Hint Using FILENAME might come handy. Possible solution #!/usr/bin/awk -f BEGIN { FS = \"|\" } { id [ $ 1 ] = 1 ; data [ $ 1 ][ FILENAME ] = $ 2 } END { for ( i in id ) print trim ( i ) \"|\" trim ( data [ i ][ \"scientific\" ]) \"|\" trim ( data [ i ][ \"genbank\" ]) } function trim ( x ) { sub ( /^[ \\t]*/ , \"\" , x ); sub ( /[ \\t]*$/ , \"\" , x ); return x } Solution usung join uggested by Amrei Binzer-Panchal, 2021.01.18 $ join -a1 -a2 -j 1 -o 0 ,1.2,2.2 -e \"NULL\" -t \"|\" < ( sort scientific ) < ( sort genbank ) 139 | Borreliella burgdorferi | Lyme disease spirochete 161 | Treponema pallidum subsp. pallidum | syphilis treponeme 168 | Treponema pallidum subsp. pertenue | yaws treponeme 2 | Bacteria | eubacteria 29 | Myxococcales | fruiting gliding bacteria 356 | Rhizobiales | rhizobacteria 638 | Arsenophonus nasoniae | son-killer infecting Nasonia vitripennis","title":"Multiple input files - first approach"},{"location":"Case_studies/multiple_files_I/#multiple-input-files-first-approach","text":"The script will collect the data in sequence and store the data in memory, then print the arranged data at the end. Here is an example problem that is easy to solve with awk. Let's assume that we have collected some data about some persons. It is not systematic, so the data files are not complete and rows are not in the same order... file: 1.dat Daniel 10 Anders 7 Sven 56 Ali 17 Peter 6 file: 2.dat Peter Monday Sven Sunday David Tuesday Let's put them together! If the files had all names with missing data marked - sorting the files and pasting them together will essentially be enough. Below, it is just one possible way to do it. First we need to have a list of all names, collect the data, then try to print whatever we have collected. Warning awk under OS X is not fully compatible with Gawk (GNU awk) and the internal variable ARGIND is not available - the script will not work as intended. script.awk #!/usr/bin/awk -f { names [ $ 1 ] = 1 ; data [ $ 1 ][ ARGIND ] = $ 2 } END { for ( i in names ) print i \"\\t\\t\" data [ i ][ 1 ] \"\\t\\t\" data [ i ][ 2 ] } Run the script like this: ./script.awk 1 .dat 2 .dat ` The script runs over the two files in a row and on each line it uses associative arrays to collect the names from the first column in names [ $ 1 ] . data [ $ 1 ][ ARGIND ] is two dimensional array with indexes [name][ number of current file/argument]. At the end we will have elements like this: ... data [ \"Sven\" ][ 1 ] = 56 data [ \"Sven\" ][ 2 ] = \"Sunday\" ... The END section runs over the collected names and prints the collected data - so we recover as much as possible from both files. Here is the output. Anders 7 Daniel 10 Ali 17 Sven 56 Sunday David Tuesday Peter 6 Monday Alternative, perhaps better solution in this case, might the specially written tool for this purpose i.e. $ join -a1 -a2 -j 1 -o 0 ,1.2,2.2 -e \"NULL\" < ( sort 1 .dat ) < ( sort 2 .dat ) Ali 17 NULL Anders 7 NULL Daniel 10 NULL David NULL Tuesday Peter 6 Monday Sven 56 Sunday Note Note that the files need to be sorted by the field they will be joined, since the program is trying to avoid loading the whole data in the memory. If the data is sorted, awk also can join the data without loading it into the memory. http://unix.stackexchange.com/questions/194968/why-isnt-this-awk-command-doing-a-full-outer-join Credits to Mahesh Panchal for the tip","title":"Multiple input files - first approach"},{"location":"Case_studies/multiple_files_I/#exercise","text":"Copy/paste the text in to two files with the suggested names scientific 2 | Bacteria | Bacteria <bacteria> | scientific name | 29 | Myxococcales | | scientific name | 139 | Borreliella burgdorferi | | scientific name | 161 | Treponema pallidum subsp. pallidum | | scientific name | 168 | Treponema pallidum subsp. pertenue | | scientific name | 356 | Rhizobiales | | scientific name | 638 | Arsenophonus nasoniae | | scientific name | genbank 2 | eubacteria | | genbank common name | 29 | fruiting gliding bacteria | | genbank common name | 139 | Lyme disease spirochete | | genbank common name | 161 | syphilis treponeme | | genbank common name | 168 | yaws treponeme | | genbank common name | 356 | rhizobacteria | | genbank common name | 638 | son-killer infecting Nasonia vitripennis | | genbank common name | Can you join the information from both files to collect the data in better format? ID | scientific name | genbank common name 2 | Bacteria | eubacteria 29 | Myxococcales | fruiting gliding bacteria 139 | Borreliella burgdorferi | Lyme disease spirochete Leave the extra blanks for the first attempt. We will use this problem (cleaning the remaining spaces before and after) to introduce user defined functions. Hint Using FILENAME might come handy. Possible solution #!/usr/bin/awk -f BEGIN { FS = \"|\" } { id [ $ 1 ] = 1 ; data [ $ 1 ][ FILENAME ] = $ 2 } END { for ( i in id ) print trim ( i ) \"|\" trim ( data [ i ][ \"scientific\" ]) \"|\" trim ( data [ i ][ \"genbank\" ]) } function trim ( x ) { sub ( /^[ \\t]*/ , \"\" , x ); sub ( /[ \\t]*$/ , \"\" , x ); return x } Solution usung join uggested by Amrei Binzer-Panchal, 2021.01.18 $ join -a1 -a2 -j 1 -o 0 ,1.2,2.2 -e \"NULL\" -t \"|\" < ( sort scientific ) < ( sort genbank ) 139 | Borreliella burgdorferi | Lyme disease spirochete 161 | Treponema pallidum subsp. pallidum | syphilis treponeme 168 | Treponema pallidum subsp. pertenue | yaws treponeme 2 | Bacteria | eubacteria 29 | Myxococcales | fruiting gliding bacteria 356 | Rhizobiales | rhizobacteria 638 | Arsenophonus nasoniae | son-killer infecting Nasonia vitripennis","title":"Exercise"},{"location":"Case_studies/multiple_files_II/","text":"Multiple input files - second approach Let's have the same input data as in the previous case , but this time, for each line in the first file we will read the second file and look for a match. For large files, this will be significantly slower than the first case since we will read multiple times the second file, but this approach reduces the memory requirement, since at no point we need to load both files into memory like in the first case. In fact this approach needs to keep only one line per file in the memory at he same time. file: 1.dat Daniel 10 Anders 7 Sven 56 Ali 17 Peter 6 file: 2.dat Peter Monday Sven Sunday David Tuesday Let's start with simple working example, that we will improve later. script1.awk #!/usr/bin/awk -f NR == FNR { printf ( \"%s \" , $ 0 ) while ( getline line < ARGV [ 2 ]){ split ( line , data ) if ( data [ 1 ] ==$ 1 ) printf ( \"%s \" , data [ 2 ]) } close ( ARGV [ 2 ]) print \"\" } Awk will read both files in order. While FNR will count the line number with respect to the current file, NR will count the concatenated number lines i.e. NR == FNR ensures that awk will run the code block only on the first file. Unfortunately, awk will read the second file anyway - as designed juts to find no match... printf ( \"%s \" , $ 0 ) will print the content of the line from the first file and add 2 spaces without the new line. while ( getline line < ARGV [ 2 ]) loop will be executed on each line for the first file. It will read one line from the second file per loop and store it in line variable. split ( line , data ) this time we need to split it manually in data array variable. if ( data [ 1 ] ==$ 1 ) printf ( data [ 2 ]) we print what we find in second column only if we match the values for the first. close ( ARGV [ 2 ]) we need to close the second file to be able to start reading from the beginning next time. print \"\" regardless if we found match or not, we print the new line character. Output $ . / script1 . awk 1 . dat 2 . dat Daniel 10 Anders 7 Sven 56 Sunday Ali 17 Peter 6 Monday Warning Note that only names from the first file were processed i.e. the data for David will not appear in the output. Second round The above solution works but for large files this is rather inefficient, let's improve a bit the code. script2.awk #!/usr/bin/awk -f FNR != NR { exit 0 } { printf ( \"%s \" , $ 0 ) while ( getline line < ARGV [ 2 ]){ split ( line , data ) if ( data [ 1 ] ==$ 1 ) { printf ( \"%s \" , data [ 2 ]); break } } close ( ARGV [ 2 ]) print \"\" } Only two changes: FNR != NR { exit 0 } serves the same purpose but this time matches when we start reading the next file and exits the program preventing awk from reading the second file. if ( data [ 1 ] ==$ 1 ) { printf ( data [ 2 ]); break } we added the brake statement, when we have found the first match from the second file, the program will exit the while loop i.e. will not read the remaining of the second file. Warning This was made under the assumption that you have only one unique entry for each name in the second file. Several year ago, during the exercises, while gradually building the code, the first version of this solution was able to find multiple entries in the second file (real life problem) which exposed a problem with the data that was not identified before. On small files speed up will not be noticeable, but it will be significant on large files. Note: this problem is rather general, not awk specific. Third round Now, what happens if we have more files? One reference and multiple matching files? Let's make an improved version which will demonstrate some other interesting and handy awk features. script3.awk #!/usr/bin/awk -f BEGIN { argc = ARGC ; ARGC = 2 } { printf ( \"%s \" , $ 0 ) for ( i = 2 ; i <= argc - 1 ; i ++ ){ while ( getline line < ARGV [ i ]){ split ( line , data ) if ( data [ 1 ] ==$ 1 ) { printf ( \"%s \" , data [ 2 ]); break } } close ( ARGV [ i ]) } print \"\" } What awk is doing is storing the name of the program + all the filenames passed as arguments in ARGV array in which element 0 is the program's name itself then all the filenames in order. ARGC will contain the number of elements, so awk will loop reading through the files. The trick is if we change the ARGC to 2, then awk will loop only over the first file and stop. This make it easy to use the rest of the filenames as input parameters - they can be anything you want - they do not need to be real file names. BEGIN { argc = ARGC ; ARGC = 2 } This is the best way to force reading only the first file. Since we need to know how many they were originally, we keep it in a argc before changing ARGC = 2 . for ( i = 2 ; i <= argc - 1 ; i ++ ) we manually loop over the remaining files with ARGV [ i ] . Output $ ./script3.awk 1 .dat 2 .dat 2 .dat 2 .dat Daniel 10 Anders 7 Sven 56 Sunday Sunday Sunday Ali 17 Peter 6 Monday Monday Monday","title":"Multiple input files - second approach"},{"location":"Case_studies/multiple_files_II/#multiple-input-files-second-approach","text":"Let's have the same input data as in the previous case , but this time, for each line in the first file we will read the second file and look for a match. For large files, this will be significantly slower than the first case since we will read multiple times the second file, but this approach reduces the memory requirement, since at no point we need to load both files into memory like in the first case. In fact this approach needs to keep only one line per file in the memory at he same time. file: 1.dat Daniel 10 Anders 7 Sven 56 Ali 17 Peter 6 file: 2.dat Peter Monday Sven Sunday David Tuesday Let's start with simple working example, that we will improve later. script1.awk #!/usr/bin/awk -f NR == FNR { printf ( \"%s \" , $ 0 ) while ( getline line < ARGV [ 2 ]){ split ( line , data ) if ( data [ 1 ] ==$ 1 ) printf ( \"%s \" , data [ 2 ]) } close ( ARGV [ 2 ]) print \"\" } Awk will read both files in order. While FNR will count the line number with respect to the current file, NR will count the concatenated number lines i.e. NR == FNR ensures that awk will run the code block only on the first file. Unfortunately, awk will read the second file anyway - as designed juts to find no match... printf ( \"%s \" , $ 0 ) will print the content of the line from the first file and add 2 spaces without the new line. while ( getline line < ARGV [ 2 ]) loop will be executed on each line for the first file. It will read one line from the second file per loop and store it in line variable. split ( line , data ) this time we need to split it manually in data array variable. if ( data [ 1 ] ==$ 1 ) printf ( data [ 2 ]) we print what we find in second column only if we match the values for the first. close ( ARGV [ 2 ]) we need to close the second file to be able to start reading from the beginning next time. print \"\" regardless if we found match or not, we print the new line character. Output $ . / script1 . awk 1 . dat 2 . dat Daniel 10 Anders 7 Sven 56 Sunday Ali 17 Peter 6 Monday Warning Note that only names from the first file were processed i.e. the data for David will not appear in the output.","title":"Multiple input files - second approach"},{"location":"Case_studies/multiple_files_II/#second-round","text":"The above solution works but for large files this is rather inefficient, let's improve a bit the code. script2.awk #!/usr/bin/awk -f FNR != NR { exit 0 } { printf ( \"%s \" , $ 0 ) while ( getline line < ARGV [ 2 ]){ split ( line , data ) if ( data [ 1 ] ==$ 1 ) { printf ( \"%s \" , data [ 2 ]); break } } close ( ARGV [ 2 ]) print \"\" } Only two changes: FNR != NR { exit 0 } serves the same purpose but this time matches when we start reading the next file and exits the program preventing awk from reading the second file. if ( data [ 1 ] ==$ 1 ) { printf ( data [ 2 ]); break } we added the brake statement, when we have found the first match from the second file, the program will exit the while loop i.e. will not read the remaining of the second file. Warning This was made under the assumption that you have only one unique entry for each name in the second file. Several year ago, during the exercises, while gradually building the code, the first version of this solution was able to find multiple entries in the second file (real life problem) which exposed a problem with the data that was not identified before. On small files speed up will not be noticeable, but it will be significant on large files. Note: this problem is rather general, not awk specific.","title":"Second round"},{"location":"Case_studies/multiple_files_II/#third-round","text":"Now, what happens if we have more files? One reference and multiple matching files? Let's make an improved version which will demonstrate some other interesting and handy awk features. script3.awk #!/usr/bin/awk -f BEGIN { argc = ARGC ; ARGC = 2 } { printf ( \"%s \" , $ 0 ) for ( i = 2 ; i <= argc - 1 ; i ++ ){ while ( getline line < ARGV [ i ]){ split ( line , data ) if ( data [ 1 ] ==$ 1 ) { printf ( \"%s \" , data [ 2 ]); break } } close ( ARGV [ i ]) } print \"\" } What awk is doing is storing the name of the program + all the filenames passed as arguments in ARGV array in which element 0 is the program's name itself then all the filenames in order. ARGC will contain the number of elements, so awk will loop reading through the files. The trick is if we change the ARGC to 2, then awk will loop only over the first file and stop. This make it easy to use the rest of the filenames as input parameters - they can be anything you want - they do not need to be real file names. BEGIN { argc = ARGC ; ARGC = 2 } This is the best way to force reading only the first file. Since we need to know how many they were originally, we keep it in a argc before changing ARGC = 2 . for ( i = 2 ; i <= argc - 1 ; i ++ ) we manually loop over the remaining files with ARGV [ i ] . Output $ ./script3.awk 1 .dat 2 .dat 2 .dat 2 .dat Daniel 10 Anders 7 Sven 56 Sunday Sunday Sunday Ali 17 Peter 6 Monday Monday Monday","title":"Third round"},{"location":"Exercises/01.Simple_arithmetic/","text":"01.Simple arithmetic * Task 1 List the files in your home folder: ls -l The output should look something like this: $ ls -l total 88608 drwxr-sr-x 6 user uu_mkem 2048 Dec 17 2007 acml4.0.1 drwxr-sr-x 7 user uu_mkem 10240 Jan 21 2014 bin drwxr-sr-x 2 user uu_mkem 2048 Mar 19 2010 bin64 drwxr-sr-x 9 user uu_mkem 2048 Oct 18 2007 castep drwxrwsr-x 2 user uu_mkem 2048 Jan 18 2013 ddt drwxr-sr-x 2 user uu_mkem 2048 Dec 20 2006 _del drwxr-sr-x 2 user uu_mkem 2048 Mar 26 2013 Desktop -rw-r--r-- 1 user uu_mkem 319954 Aug 15 2012 dftbp_1.2_src.tar.gz drwxr-sr-x 2 user uu_mkem 2048 Mar 26 2013 Documents drwxr-sr-x 2 user uu_mkem 2048 Mar 26 2013 Downloads -rw-r----- 1 user uu_mkem 20730896 Aug 8 2011 espresso-4.3.2-examples.tar.gz -rw-r----- 1 user uu_mkem 15116712 Aug 8 2011 espresso-4.3.2.tar.gz -rw-r----- 1 user uu_mkem 8101479 Jul 5 2011 etc.tgz Write a one-line awk script to calculate the occupied space (5 th column) in this folder. ( * ) Modify the script to count only the files in your home directory (hopefully you have some in your home folder) ( hint: directories begin with \u201dd\u201d in the access field ) ( * ) Task 2 The table below contains coordinates of atomic positions in Angstrom. Copy/paste the text in a file coord.dat on your computer. coord.dat 0.8997508593245822 0.2048785172349154 0.0717195259190714 0.9490486395984686 0.9146565172390164 0.7689034265935394 0.8752470144992109 0.8523867265774260 0.8626469653565847 0.8640857283636117 0.9207732244914749 0.1291593060111536 0.9392794897450530 0.7856183128719006 0.1151153726165936 0.3493798807138794 0.1485424348042750 0.4300844752418486 0.3877406330134096 0.1931874740146635 0.5746869696493597 0.4376693404820234 0.9063674424632875 0.2740320628086707 Write a one-line awk script to add 1 to each number in the second column. ( * ) Modify the script to multiply all numbers by factor of 1.8897 . ( * ) Possible solutions Task 1. 1 ) ls - l | awk ' {s=s+$5} END {print s}' 2 ) ls \u2013 l | awk ' !/^d/ {s=s+$5} END {print s}' Task 2. 1 ) awk '{print $1,$2+1.,$3}' coord . dat or awk '{$2=$2+1; print}' coord . dat 2 ) awk 'BEGIN{a2b=1.8897261} {print $1*a2b,$2*a2b,$3*a2b}' coord . dat","title":"01.Simple arithmetic *"},{"location":"Exercises/01.Simple_arithmetic/#01simple-arithmetic","text":"","title":"01.Simple arithmetic *"},{"location":"Exercises/01.Simple_arithmetic/#task-1","text":"List the files in your home folder: ls -l The output should look something like this: $ ls -l total 88608 drwxr-sr-x 6 user uu_mkem 2048 Dec 17 2007 acml4.0.1 drwxr-sr-x 7 user uu_mkem 10240 Jan 21 2014 bin drwxr-sr-x 2 user uu_mkem 2048 Mar 19 2010 bin64 drwxr-sr-x 9 user uu_mkem 2048 Oct 18 2007 castep drwxrwsr-x 2 user uu_mkem 2048 Jan 18 2013 ddt drwxr-sr-x 2 user uu_mkem 2048 Dec 20 2006 _del drwxr-sr-x 2 user uu_mkem 2048 Mar 26 2013 Desktop -rw-r--r-- 1 user uu_mkem 319954 Aug 15 2012 dftbp_1.2_src.tar.gz drwxr-sr-x 2 user uu_mkem 2048 Mar 26 2013 Documents drwxr-sr-x 2 user uu_mkem 2048 Mar 26 2013 Downloads -rw-r----- 1 user uu_mkem 20730896 Aug 8 2011 espresso-4.3.2-examples.tar.gz -rw-r----- 1 user uu_mkem 15116712 Aug 8 2011 espresso-4.3.2.tar.gz -rw-r----- 1 user uu_mkem 8101479 Jul 5 2011 etc.tgz Write a one-line awk script to calculate the occupied space (5 th column) in this folder. ( * ) Modify the script to count only the files in your home directory (hopefully you have some in your home folder) ( hint: directories begin with \u201dd\u201d in the access field ) ( * )","title":"Task 1"},{"location":"Exercises/01.Simple_arithmetic/#task-2","text":"The table below contains coordinates of atomic positions in Angstrom. Copy/paste the text in a file coord.dat on your computer. coord.dat 0.8997508593245822 0.2048785172349154 0.0717195259190714 0.9490486395984686 0.9146565172390164 0.7689034265935394 0.8752470144992109 0.8523867265774260 0.8626469653565847 0.8640857283636117 0.9207732244914749 0.1291593060111536 0.9392794897450530 0.7856183128719006 0.1151153726165936 0.3493798807138794 0.1485424348042750 0.4300844752418486 0.3877406330134096 0.1931874740146635 0.5746869696493597 0.4376693404820234 0.9063674424632875 0.2740320628086707 Write a one-line awk script to add 1 to each number in the second column. ( * ) Modify the script to multiply all numbers by factor of 1.8897 . ( * ) Possible solutions Task 1. 1 ) ls - l | awk ' {s=s+$5} END {print s}' 2 ) ls \u2013 l | awk ' !/^d/ {s=s+$5} END {print s}' Task 2. 1 ) awk '{print $1,$2+1.,$3}' coord . dat or awk '{$2=$2+1; print}' coord . dat 2 ) awk 'BEGIN{a2b=1.8897261} {print $1*a2b,$2*a2b,$3*a2b}' coord . dat","title":"Task 2"},{"location":"Exercises/02.Data_extraction/","text":"02.Data extraction * Here is a file containing some data data.dat 23 34 567 3 4556 345 22 45 6 34 5 677 787 234 124 5 5 47 1 34 98 45 24 333 345 121 17 100 345 48 65 90 345 665 12 55 73 34 33 23 25 234 17 19 1) Write a script to print the largest number from each line. output: 4556 787 345 665 234 Possible solution awk '{max=$1; for (i=2;i<=NF;i++) { if ($i>max) max=$i } ; print max}' data . dat 2) ... to print the sum of all numbers on each line. output: 5601 1918 1018 1670 513 Possible solution awk '{sum=0; for (i=1;i<=NF;i++) { sum=sum+$i } ; print sum}' data . dat 3) ... to print new data in which all numbers smaller than 55 are replaced with 0 (zero). output: 0 0 567 0 4556 345 0 0 0 0 0 677 787 234 124 0 0 0 0 0 98 0 0 333 345 121 0 100 345 0 65 90 345 665 0 55 73 0 0 0 0 234 0 0 Possible solution inspired by Jessica De Loma awk '{ for(i=1;i<=NF;i++){if($i < 55) $i=0 } print $0 }' data . dat","title":"02.Data extraction *"},{"location":"Exercises/02.Data_extraction/#02data-extraction","text":"Here is a file containing some data data.dat 23 34 567 3 4556 345 22 45 6 34 5 677 787 234 124 5 5 47 1 34 98 45 24 333 345 121 17 100 345 48 65 90 345 665 12 55 73 34 33 23 25 234 17 19 1) Write a script to print the largest number from each line. output: 4556 787 345 665 234 Possible solution awk '{max=$1; for (i=2;i<=NF;i++) { if ($i>max) max=$i } ; print max}' data . dat 2) ... to print the sum of all numbers on each line. output: 5601 1918 1018 1670 513 Possible solution awk '{sum=0; for (i=1;i<=NF;i++) { sum=sum+$i } ; print sum}' data . dat 3) ... to print new data in which all numbers smaller than 55 are replaced with 0 (zero). output: 0 0 567 0 4556 345 0 0 0 0 0 677 787 234 124 0 0 0 0 0 98 0 0 333 345 121 0 100 345 0 65 90 345 665 0 55 73 0 0 0 0 234 0 0 Possible solution inspired by Jessica De Loma awk '{ for(i=1;i<=NF;i++){if($i < 55) $i=0 } print $0 }' data . dat","title":"02.Data extraction *"},{"location":"Exercises/03.Data_extraction/","text":"03.Data extraction ** The output from a molecular dynamics (MD) program looks like the one bellow. Although it is easy to read, it is rather inconvenient to analyse the data. md.out MD step: 0 Pressure: 0.332328E-03 au 0.977743E+10 Pa Potential Energy: -1039.8363386148 H -28295.3864 eV MD Kinetic Energy: 1.0930263164 H 29.7428 eV Total MD Energy: -1038.7433122983 H -28265.6437 eV MD Temperature: 0.0009500446 au 300.0000 K MD step: 10 Pressure: 0.335165E-03 au 0.986088E+10 Pa Potential Energy: -1039.9197980557 H -28297.6575 eV MD Kinetic Energy: 1.1714204733 H 31.8760 eV Total MD Energy: -1038.7483775824 H -28265.7815 eV MD Temperature: 0.0010181838 au 321.5166 K MD step: 20 Pressure: 0.348121E-03 au 0.102421E+11 Pa Potential Energy: -1039.9631365733 H -28298.8368 eV MD Kinetic Energy: 1.2021764632 H 32.7129 eV Total MD Energy: -1038.7609601101 H -28266.1239 eV MD Temperature: 0.0010449165 au 329.9581 K a) Write an Awk script to collect the \"MD temperature\" in one column vs. the \"MD step\" i.e. which is much nicer format to plot, analyze or read by other programs. 0 300.0000 10 321.5166 20 329.9581 b) Can you tabulate all values against the \"MD step\" Output: 0 0.977743E+10 -28295.3864 29.7428 -28265.6437 300.0000 10 0.986088E+10 -28297.6575 31.8760 -28265.7815 321.5166 20 0.102421E+11 -28298.8368 32.7129 -28266.1239 329.9581 Possible solutions: a) $ awk '/MD step/{step= $3} /MD Temperature:/ {print step, $5 }' md . out b) here is an easy solution: $ awk '/MD step/{step= $3; getline; press= $4; getline; pe= $5; getline; ke= $6; getline; te= $6; getline; t=$5; print step,press,pe,ke,te,t} ' md . out here is an alternative solution using \"multi-line records\". Each \"MD step\" ends with space,\"K\", then new line. Note that there is no other line ending the same way, otherwise this will not work. So, we define \"Record separator\" RS=\" K\\n\" then each record will have 33 fields (print NF to see if it is correct). $ awk 'BEGIN {RS=\" K\\n\"} {print $3, $7, $13, $20, $27, $33}' md . out","title":"03.Data extraction **"},{"location":"Exercises/03.Data_extraction/#03data-extraction","text":"The output from a molecular dynamics (MD) program looks like the one bellow. Although it is easy to read, it is rather inconvenient to analyse the data. md.out MD step: 0 Pressure: 0.332328E-03 au 0.977743E+10 Pa Potential Energy: -1039.8363386148 H -28295.3864 eV MD Kinetic Energy: 1.0930263164 H 29.7428 eV Total MD Energy: -1038.7433122983 H -28265.6437 eV MD Temperature: 0.0009500446 au 300.0000 K MD step: 10 Pressure: 0.335165E-03 au 0.986088E+10 Pa Potential Energy: -1039.9197980557 H -28297.6575 eV MD Kinetic Energy: 1.1714204733 H 31.8760 eV Total MD Energy: -1038.7483775824 H -28265.7815 eV MD Temperature: 0.0010181838 au 321.5166 K MD step: 20 Pressure: 0.348121E-03 au 0.102421E+11 Pa Potential Energy: -1039.9631365733 H -28298.8368 eV MD Kinetic Energy: 1.2021764632 H 32.7129 eV Total MD Energy: -1038.7609601101 H -28266.1239 eV MD Temperature: 0.0010449165 au 329.9581 K a) Write an Awk script to collect the \"MD temperature\" in one column vs. the \"MD step\" i.e. which is much nicer format to plot, analyze or read by other programs. 0 300.0000 10 321.5166 20 329.9581 b) Can you tabulate all values against the \"MD step\" Output: 0 0.977743E+10 -28295.3864 29.7428 -28265.6437 300.0000 10 0.986088E+10 -28297.6575 31.8760 -28265.7815 321.5166 20 0.102421E+11 -28298.8368 32.7129 -28266.1239 329.9581 Possible solutions: a) $ awk '/MD step/{step= $3} /MD Temperature:/ {print step, $5 }' md . out b) here is an easy solution: $ awk '/MD step/{step= $3; getline; press= $4; getline; pe= $5; getline; ke= $6; getline; te= $6; getline; t=$5; print step,press,pe,ke,te,t} ' md . out here is an alternative solution using \"multi-line records\". Each \"MD step\" ends with space,\"K\", then new line. Note that there is no other line ending the same way, otherwise this will not work. So, we define \"Record separator\" RS=\" K\\n\" then each record will have 33 fields (print NF to see if it is correct). $ awk 'BEGIN {RS=\" K\\n\"} {print $3, $7, $13, $20, $27, $33}' md . out","title":"03.Data extraction **"},{"location":"Exercises/04.Data_manipulation/","text":"04.Data manipulation ** You have 2 files containing results from two similar experiments. You want to calculate the difference between the numbers in the second columns. 1.dat 0.0 0.00 0.1 1.23 0.2 1.34 0.3 1.67 0.4 2.34 0.5 3.17 2.dat 0.0 0.00 0.1 1.25 0.2 1.24 0.3 1.61 0.4 2.44 0.5 3.27 Try to use awk and any other tools to produce a file that has the results from both files and the difference between the second column in last column in the new file i.e. 0.0 0.00 0.00 0 0.1 1.23 1.25 -0.02 0.2 1.34 1.24 0.1 0.3 1.67 1.61 0.06 0.4 2.34 2.44 -0.1 0.5 3.17 3.27 -0.1 Possible solutions: $ paste 1 . dat 2 . dat | awk '{print $1,$2,$4,$2-$4}' Note: This will not work under OS X $ awk 'ARGIND==1 {x1=$1;y1=$2; getline < ARGV[2]; printf(\"%g %g %g %g %g\\n\",x1,y1,$1,$2,y1-$2);}' 1 . dat 2 . dat Note: This might work under OS X $ awk 'FILENAME==\"1.dat\" {x1=$1;y1=$2; getline < ARGV[2]; printf(\"%g %g %g %g %g\\n\",x1,y1,$1,$2,y1-$2);}' 1 . dat 2 . dat $ awk 'BEGIN{ while (getline < ARGV[1]) {x1=$1;y1=$2; getline < ARGV[2]; printf(\"%g %g %g %g %g\\n\",x1,y1,$1,$2,y1-$2);}}' 1 . dat 2 . dat # contribution by Johan Zvrskovec 2020.08.28 awk 'NR==FNR{n1[$1]=$2;} NR!=FNR{print ($1,n1[$1],$2,$2-n1[$1])}' 1 . dat 2 . dat","title":"04.Data manipulation **"},{"location":"Exercises/04.Data_manipulation/#04data-manipulation","text":"You have 2 files containing results from two similar experiments. You want to calculate the difference between the numbers in the second columns. 1.dat 0.0 0.00 0.1 1.23 0.2 1.34 0.3 1.67 0.4 2.34 0.5 3.17 2.dat 0.0 0.00 0.1 1.25 0.2 1.24 0.3 1.61 0.4 2.44 0.5 3.27 Try to use awk and any other tools to produce a file that has the results from both files and the difference between the second column in last column in the new file i.e. 0.0 0.00 0.00 0 0.1 1.23 1.25 -0.02 0.2 1.34 1.24 0.1 0.3 1.67 1.61 0.06 0.4 2.34 2.44 -0.1 0.5 3.17 3.27 -0.1 Possible solutions: $ paste 1 . dat 2 . dat | awk '{print $1,$2,$4,$2-$4}' Note: This will not work under OS X $ awk 'ARGIND==1 {x1=$1;y1=$2; getline < ARGV[2]; printf(\"%g %g %g %g %g\\n\",x1,y1,$1,$2,y1-$2);}' 1 . dat 2 . dat Note: This might work under OS X $ awk 'FILENAME==\"1.dat\" {x1=$1;y1=$2; getline < ARGV[2]; printf(\"%g %g %g %g %g\\n\",x1,y1,$1,$2,y1-$2);}' 1 . dat 2 . dat $ awk 'BEGIN{ while (getline < ARGV[1]) {x1=$1;y1=$2; getline < ARGV[2]; printf(\"%g %g %g %g %g\\n\",x1,y1,$1,$2,y1-$2);}}' 1 . dat 2 . dat # contribution by Johan Zvrskovec 2020.08.28 awk 'NR==FNR{n1[$1]=$2;} NR!=FNR{print ($1,n1[$1],$2,$2-n1[$1])}' 1 . dat 2 . dat","title":"04.Data manipulation **"},{"location":"Exercises/05.Easy_tricks/","text":"05.Easy tricks * This exercise will illustrate another convenient feature of Awk - generating data from simple functions, simple list or even some more advanced data sets which does not require files to read/analyse. If you use only the BEGIN block, awk will not try to read any file. One can simply write, which will simply print to the terminal: $ awk 'BEGIN{print \"Hello, world!\"}' Hello , world ! Task 1. a) print numbers from 1 to 7 i.e. produce such output 1 2 3 4 5 6 7 Solution... $ awk 'BEGIN{ for(i=1;i<=7;i=i+1) print i }' b) print the same numbers on a single line i.e. 1 2 3 4 5 6 7 Solution... Note that here we use printf which does not print the new line. $ awk 'BEGIN{ for(i=1; i<=7; i=i+1) printf i\" \"}' c) print the numbers from 1 to 7 in reverse order d) print every other number from 1 to 7 i.e. 1 3 5 7 e) print the numbers from 1 to 2 with increments of 0.1 i.e 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 Solution... $ awk 'BEGIN{ for(i=1; i<=2.01; i=i+0.1) print i}' Yes, Awk allow for fractional increments... Note the upper limit 2.01 ! f) can you add on each line the square of the number, exp(), sin() /the argument is in radians, don't worry/ 1 1 2.71828 0.841471 1.1 1.21 3.00417 0.891207 1.2 1.44 3.32012 0.932039 1.3 1.69 3.6693 0.963558 1.4 1.96 4.0552 0.98545 1.5 2.25 4.48169 0.997495 1.6 2.56 4.95303 0.999574 1.7 2.89 5.47395 0.991665 1.8 3.24 6.04965 0.973848 1.9 3.61 6.68589 0.9463 2 4 7.38906 0.909297 Solution... $ awk 'BEGIN{ for(i=1.; i<=2.01; i=i+0.1) print i,i**2,exp(i),sin(i)}' g) make awk script that prints such output i.e. 2 students in each group. (**) Group1 => Student1, Student2 Group2 => Student3, Student4 Group3 => Student5, Student6 Group4 => Student7, Student8 Group5 => Student9, Student10 Group6 => Student11, Student12 Group7 => Student13, Student14 Solution... $ awk 'BEGIN{ for(i=1;i<=7;i++) print \"Group\"i\" => Student\"(i-1)*2+1\", Student\"(i-1)*2+2 }' solution suggested by Ageo Meier de Andrade $ awk 'BEGIN{f=1;for (i=1; i<=7; i++){print \"Group\"i\" => Student\"f++\",Student\"f++}}'","title":"05.Easy tricks *"},{"location":"Exercises/05.Easy_tricks/#05easy-tricks","text":"This exercise will illustrate another convenient feature of Awk - generating data from simple functions, simple list or even some more advanced data sets which does not require files to read/analyse. If you use only the BEGIN block, awk will not try to read any file. One can simply write, which will simply print to the terminal: $ awk 'BEGIN{print \"Hello, world!\"}' Hello , world !","title":"05.Easy tricks *"},{"location":"Exercises/05.Easy_tricks/#task-1","text":"","title":"Task 1."},{"location":"Exercises/05.Easy_tricks/#a-print-numbers-from-1-to-7-ie-produce-such-output","text":"1 2 3 4 5 6 7 Solution... $ awk 'BEGIN{ for(i=1;i<=7;i=i+1) print i }'","title":"a) print numbers from 1 to 7 i.e. produce such output"},{"location":"Exercises/05.Easy_tricks/#b-print-the-same-numbers-on-a-single-line-ie","text":"1 2 3 4 5 6 7 Solution... Note that here we use printf which does not print the new line. $ awk 'BEGIN{ for(i=1; i<=7; i=i+1) printf i\" \"}'","title":"b) print the same numbers on a single line i.e."},{"location":"Exercises/05.Easy_tricks/#c-print-the-numbers-from-1-to-7-in-reverse-order","text":"","title":"c) print the numbers from 1 to 7 in reverse order"},{"location":"Exercises/05.Easy_tricks/#d-print-every-other-number-from-1-to-7-ie","text":"1 3 5 7","title":"d) print every other number from 1 to 7 i.e."},{"location":"Exercises/05.Easy_tricks/#e-print-the-numbers-from-1-to-2-with-increments-of-01-ie","text":"1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 Solution... $ awk 'BEGIN{ for(i=1; i<=2.01; i=i+0.1) print i}' Yes, Awk allow for fractional increments... Note the upper limit 2.01 !","title":"e) print the numbers from 1 to 2 with increments of 0.1 i.e"},{"location":"Exercises/05.Easy_tricks/#f-can-you-add-on-each-line-the-square-of-the-number-exp-sin-the-argument-is-in-radians-dont-worry","text":"1 1 2.71828 0.841471 1.1 1.21 3.00417 0.891207 1.2 1.44 3.32012 0.932039 1.3 1.69 3.6693 0.963558 1.4 1.96 4.0552 0.98545 1.5 2.25 4.48169 0.997495 1.6 2.56 4.95303 0.999574 1.7 2.89 5.47395 0.991665 1.8 3.24 6.04965 0.973848 1.9 3.61 6.68589 0.9463 2 4 7.38906 0.909297 Solution... $ awk 'BEGIN{ for(i=1.; i<=2.01; i=i+0.1) print i,i**2,exp(i),sin(i)}'","title":"f) can you add on each line the square of the number, exp(), sin() /the argument is in radians, don't worry/"},{"location":"Exercises/05.Easy_tricks/#g-make-awk-script-that-prints-such-output-ie-2-students-in-each-group","text":"Group1 => Student1, Student2 Group2 => Student3, Student4 Group3 => Student5, Student6 Group4 => Student7, Student8 Group5 => Student9, Student10 Group6 => Student11, Student12 Group7 => Student13, Student14 Solution... $ awk 'BEGIN{ for(i=1;i<=7;i++) print \"Group\"i\" => Student\"(i-1)*2+1\", Student\"(i-1)*2+2 }' solution suggested by Ageo Meier de Andrade $ awk 'BEGIN{f=1;for (i=1; i<=7; i++){print \"Group\"i\" => Student\"f++\",Student\"f++}}'","title":"g) make awk script that prints such output i.e. 2 students in each group. (**)"},{"location":"Exercises/06.Bioinformaticians_corner/","text":"06.Bioinformaticians corner * Right, at this point I always feel that I am failing to address the bioinformatician community. The examples until this point are absolutely generic, but I am convinced that the best way to explain or illustrate something is explaining it in terms of your own field of interest. Here I found an excellent tutorial, essentially doing the same things or slightly different but on \"transcriptome\" / whatever that means / data file. If you are an bioinformatician, I strongly recommend following the tutorial, which might be more beneficial for you rather than jumping in less common situations that I am trying to present here. Just a teaser for the tutorial as an introduction to the material on the page. AWK GTF! How to Analyze a Transcriptome Like a Pro - Part 1 It is really nice exercise for the rest as well /the ones that do not know what is transcriptome/. If you want to experience, how the bioinformatician feels doing the rest of the exercises on this page /i.e. unaddressed/ - go for it!","title":"06.Bioinformaticians corner *"},{"location":"Exercises/06.Bioinformaticians_corner/#06bioinformaticians-corner","text":"Right, at this point I always feel that I am failing to address the bioinformatician community. The examples until this point are absolutely generic, but I am convinced that the best way to explain or illustrate something is explaining it in terms of your own field of interest. Here I found an excellent tutorial, essentially doing the same things or slightly different but on \"transcriptome\" / whatever that means / data file. If you are an bioinformatician, I strongly recommend following the tutorial, which might be more beneficial for you rather than jumping in less common situations that I am trying to present here. Just a teaser for the tutorial as an introduction to the material on the page. AWK GTF! How to Analyze a Transcriptome Like a Pro - Part 1 It is really nice exercise for the rest as well /the ones that do not know what is transcriptome/. If you want to experience, how the bioinformatician feels doing the rest of the exercises on this page /i.e. unaddressed/ - go for it!","title":"06.Bioinformaticians corner *"},{"location":"Exercises/Advanced_data_analysis/","text":"Advanced data analisys **** You are given a file with numbers on each row - 5 in this case. data1 1 2 3 4 5 1 3 5 7 9 1 2 4 5 0 2 6 7 8 9 Then you are given 5 numbers (let's say \"1, 3, 5, 6 and 7\") and you want to find how many of these numbers are matching a number on each line - think like you are about to check your lottery tickets ;-) The solution bellow is using an \"assicative arrays\" trick to make it easier to loop over the reference numbers. Possible solution Not very elegant but illustrates nicely a convenient use of associated arrays as list - if ($i in n) : awk 'BEGIN{n[1]=n[3]=n[5]=n[6]=n[7]=1} {count=0; for (i=1;i<=NF;i++){if ($i in n) count++} print count} ' data1 Can you improve the script so it could pick up the numbers from the first line, i.e. the winning numbers are on the first line? data2 1 3 5 6 7 1 2 3 4 5 1 3 5 7 9 1 2 4 5 0 2 6 7 8 9 Possible solution awk 'NR==1 {for (i=1;i<=NF;i++) n[$i]=1} NR>1 {count=0; for (i=1;i<=NF;i++){if ($i in n) count++} print count} ' data2 a bit more readable in a script #!/bin/awk -f NR == 1 { for ( i = 1 ; i <= NF ; i ++ ) n [ $ i ] = 1 } NR > 1 { count = 0 ; for ( i = 1 ; i <= NF ; i ++ ) { if ( $ i in n ) count ++ } print count }","title":"Advanced data analysis ****"},{"location":"Exercises/Advanced_data_analysis/#advanced-data-analisys","text":"You are given a file with numbers on each row - 5 in this case. data1 1 2 3 4 5 1 3 5 7 9 1 2 4 5 0 2 6 7 8 9 Then you are given 5 numbers (let's say \"1, 3, 5, 6 and 7\") and you want to find how many of these numbers are matching a number on each line - think like you are about to check your lottery tickets ;-) The solution bellow is using an \"assicative arrays\" trick to make it easier to loop over the reference numbers. Possible solution Not very elegant but illustrates nicely a convenient use of associated arrays as list - if ($i in n) : awk 'BEGIN{n[1]=n[3]=n[5]=n[6]=n[7]=1} {count=0; for (i=1;i<=NF;i++){if ($i in n) count++} print count} ' data1 Can you improve the script so it could pick up the numbers from the first line, i.e. the winning numbers are on the first line? data2 1 3 5 6 7 1 2 3 4 5 1 3 5 7 9 1 2 4 5 0 2 6 7 8 9 Possible solution awk 'NR==1 {for (i=1;i<=NF;i++) n[$i]=1} NR>1 {count=0; for (i=1;i<=NF;i++){if ($i in n) count++} print count} ' data2 a bit more readable in a script #!/bin/awk -f NR == 1 { for ( i = 1 ; i <= NF ; i ++ ) n [ $ i ] = 1 } NR > 1 { count = 0 ; for ( i = 1 ; i <= NF ; i ++ ) { if ( $ i in n ) count ++ } print count }","title":"Advanced data analisys ****"},{"location":"Exercises/Carpool/","text":"Carpool *** Some friends have organized carpool them-self. At the beginning of each accounting period, they write down the numbers of the car's odometer . Every time somebody drives the car, she/he writes down the numbers when the car is returned and hers/his name. carpool.dat 17000 start 17100 Daniel 17220 Sara 17310 David 17410 Daniel 17550 Sara 17800 David Try to write an awk script that calculates what distance has everyone traveled with the car. Hints: Differences between numbers in two consecutive lines might be a good start. Associative arrays might come handy. Answer for the data above: Sara: 260 Daniel: 200 David: 340 The problem is rather easy to solve with awk ( it is my humble opinion ). Can it be solved easier with some of your favorite tools? ( I am interested of alternative approaches ) Possible solution: awk '{dist[$2]=dist[$2]+($1-prev); prev=$1} END{ for (i in dist) if (i !=\"start\") printf(\"%15s: %g\\n\",i,dist[i]) }' carpool . dat solution in Python (credits to Mihai Croicu) import pandas as pd frame = pd . read_csv ( 'odometer' , sep = ' ' , header = None ) frame [ 'driven' ] = frame [ 0 ] - frame [ 0 ] . shift ( 1 ) frame . groupby ([ 1 ])[ 'driven' ] . sum ()","title":"Carpool ***"},{"location":"Exercises/Carpool/#carpool","text":"Some friends have organized carpool them-self. At the beginning of each accounting period, they write down the numbers of the car's odometer . Every time somebody drives the car, she/he writes down the numbers when the car is returned and hers/his name. carpool.dat 17000 start 17100 Daniel 17220 Sara 17310 David 17410 Daniel 17550 Sara 17800 David Try to write an awk script that calculates what distance has everyone traveled with the car. Hints: Differences between numbers in two consecutive lines might be a good start. Associative arrays might come handy. Answer for the data above: Sara: 260 Daniel: 200 David: 340 The problem is rather easy to solve with awk ( it is my humble opinion ). Can it be solved easier with some of your favorite tools? ( I am interested of alternative approaches ) Possible solution: awk '{dist[$2]=dist[$2]+($1-prev); prev=$1} END{ for (i in dist) if (i !=\"start\") printf(\"%15s: %g\\n\",i,dist[i]) }' carpool . dat solution in Python (credits to Mihai Croicu) import pandas as pd frame = pd . read_csv ( 'odometer' , sep = ' ' , header = None ) frame [ 'driven' ] = frame [ 0 ] - frame [ 0 ] . shift ( 1 ) frame . groupby ([ 1 ])[ 'driven' ] . sum ()","title":"Carpool ***"},{"location":"Exercises/Difficult_data/","text":"Difficult to extract data **** Let's assume that we get such an output from a Python program. [ 2 , 261 , 262 ] is a list variable that is easy to print but kind off difficult to deal with in this form... 1 [ 2 , 261 , 262 ] 4.22143226361 2 [ 96 , 447 , 448 ] 3.9916056595 3 [ 103 , 461 , 462 ] 2.94525993079 Use the data above and try to produce output like this ( color images are for guidance ) O002.H261 O002.H262 O096.H447 O096.H448 O103.H461 O103.H462 Posible solutions: awk - F '[][,]' '{printf(\"O%03d.H%03d O%03d.H%03d\\n\",$2,$3,$2,$4)}' data or just by removing the problematic characters... awk '{gsub(\",\",\" \",$0); gsub(\"\\\\[\",\" \",$0); gsub(\"\\\\]\",\" \",$0); printf(\"O%03d.H%03d O%03d.H%03d\\n\",$2,$3,$2,$4)}' data # or with single gsub command awk '{gsub(\",\\\\|\\\\]\\\\|\\\\[\",\" \",$0); printf(\"O%03d.H%03d O%03d.H%03d\\n\",$2,$3,$2,$4)}' data Credits to Jonas S\u00f6derberg for the solution bellow: awk ' { red = substr ( $ 2 , 2 , length ( $ 2 ) - 2 ); green = substr ( $ 3 , 1 , length ( $ 3 ) - 1 ); blue = substr ( $ 4 , 1 , length ( $ 4 ) - 1 ); printf ( \"O%03d.H%03d O%03d.H%03d\\n\" , red , green , red , blue )} data","title":"Difficult to extract data ****"},{"location":"Exercises/Difficult_data/#difficult-to-extract-data","text":"Let's assume that we get such an output from a Python program. [ 2 , 261 , 262 ] is a list variable that is easy to print but kind off difficult to deal with in this form... 1 [ 2 , 261 , 262 ] 4.22143226361 2 [ 96 , 447 , 448 ] 3.9916056595 3 [ 103 , 461 , 462 ] 2.94525993079 Use the data above and try to produce output like this ( color images are for guidance ) O002.H261 O002.H262 O096.H447 O096.H448 O103.H461 O103.H462 Posible solutions: awk - F '[][,]' '{printf(\"O%03d.H%03d O%03d.H%03d\\n\",$2,$3,$2,$4)}' data or just by removing the problematic characters... awk '{gsub(\",\",\" \",$0); gsub(\"\\\\[\",\" \",$0); gsub(\"\\\\]\",\" \",$0); printf(\"O%03d.H%03d O%03d.H%03d\\n\",$2,$3,$2,$4)}' data # or with single gsub command awk '{gsub(\",\\\\|\\\\]\\\\|\\\\[\",\" \",$0); printf(\"O%03d.H%03d O%03d.H%03d\\n\",$2,$3,$2,$4)}' data Credits to Jonas S\u00f6derberg for the solution bellow: awk ' { red = substr ( $ 2 , 2 , length ( $ 2 ) - 2 ); green = substr ( $ 3 , 1 , length ( $ 3 ) - 1 ); blue = substr ( $ 4 , 1 , length ( $ 4 ) - 1 ); printf ( \"O%03d.H%03d O%03d.H%03d\\n\" , red , green , red , blue )} data","title":"Difficult to extract data ****"},{"location":"Exercises/Exercises/","text":"Exercises - warming up Task 1. Make an awk script to calculate: a) sum of the values b) the average i.e arithmetic mean value (can you write the script so it is independent from the number of rows) c) find the maximum value d) calculate the difference between numbers in the first column, i.e 2 nd -1 st , 3 rd -2 nd etc. Copy/paste the numbers in a file for convenience. num.dat 2 4 8 9 7 3 Possible solutions 1.a) $ awk '{sum= sum+$1} END{print sum}' num . dat 33 1.b) $ awk '{sum= sum+$1} END{print sum/NR}' num . dat 5.5 1.c) NR==1 {max=$1} makes sure that you have reasonable initial value. What could go wrong if you skip it? $ awk 'NR==1 {max=$1} {if (max < $1) max=$1} END{print max}' num . dat 9 1.d) Possible solution (note that the first number is the value of the first column): $ awk '{print $1-prev;prev=$1}' num . dat 2 2 4 1 - 2 - 4 Can you improve the script to avoid the problem with the first line? Task 2. Providing you have the following data 10.dat 67 4 33 53 21 99 88 69 79 8 Can you write a script to print the cumulative sum i.e. on each row, next to the original value, you print the sum of all above values? Output: 67 67 4 71 33 104 53 157 21 178 99 277 88 365 69 434 79 513 8 521 Possible solution $ awk '{sum=sum+$1; print $1,sum}' 10 . dat","title":"Warming up"},{"location":"Exercises/Exercises/#exercises-warming-up","text":"","title":"Exercises - warming up"},{"location":"Exercises/Exercises/#task-1-make-an-awk-script-to-calculate","text":"a) sum of the values b) the average i.e arithmetic mean value (can you write the script so it is independent from the number of rows) c) find the maximum value d) calculate the difference between numbers in the first column, i.e 2 nd -1 st , 3 rd -2 nd etc. Copy/paste the numbers in a file for convenience. num.dat 2 4 8 9 7 3 Possible solutions 1.a) $ awk '{sum= sum+$1} END{print sum}' num . dat 33 1.b) $ awk '{sum= sum+$1} END{print sum/NR}' num . dat 5.5 1.c) NR==1 {max=$1} makes sure that you have reasonable initial value. What could go wrong if you skip it? $ awk 'NR==1 {max=$1} {if (max < $1) max=$1} END{print max}' num . dat 9 1.d) Possible solution (note that the first number is the value of the first column): $ awk '{print $1-prev;prev=$1}' num . dat 2 2 4 1 - 2 - 4 Can you improve the script to avoid the problem with the first line?","title":"Task 1. Make an awk script to calculate:"},{"location":"Exercises/Exercises/#task-2-providing-you-have-the-following-data","text":"10.dat 67 4 33 53 21 99 88 69 79 8 Can you write a script to print the cumulative sum i.e. on each row, next to the original value, you print the sum of all above values? Output: 67 67 4 71 33 104 53 157 21 178 99 277 88 365 69 434 79 513 8 521 Possible solution $ awk '{sum=sum+$1; print $1,sum}' 10 . dat","title":"Task 2. Providing you have the following data"},{"location":"Exercises/Linear_regression/","text":"Simple Linear Regression Wikipedia The purpose of this exercise is not to learn about \"Linear regression\" but to exercise some simple awk operations on simple data sets that will be equivalent to performing of a simple linear regression calculation. You might be surprised how easy it could be done... The file \"regression.dat\" contains 2 rows of numbers, for convenience named as \\(x_i\\) and \\(y_i\\) , where \\(i\\) is the row (line) number. I will try to follow the nomenclature described on the wikipage. Suppose there are \\(n\\) data points \\({(x_i, y_i), i = 1, ..., n}\\) . The function that describes \\(x\\) and \\(y\\) is: \\(y_i = \\alpha + \\beta x_i + \\varepsilon_i\\) . The goal is to find the equation of the straight line \\(y=\\alpha +\\beta x\\) , which would provide a \"best\" fit for the data points. Let, at the begging, assume \\(\\alpha=0\\) which will result to a \"best\" fit of a straight line which passes through the origin \\((0,0)\\) . There is a simple expression on how to obtain the \\(\\beta\\) parameter \\({\\displaystyle {\\hat {\\beta }}={\\frac {\\sum _{i=1}^{n}x_{i}y_{i}}{\\sum _{i=1}^{n}x_{i}^{2}}}={\\frac {\\overline {xy}}{\\overline {x^{2}}}}}\\) Write an awk script (or one line) that will calculate the beta from the equation above. You will be sutprised how easy it could be calculated... The correct answer for \\(\\displaystyle {\\hat {\\beta}}\\) is 0.928142 Write two scripts, one for the average values \\(\\overline {x}, \\overline {y}\\) and then use the numbers in the second ( directly in the code ), to obtain the results in the case when \\(\\alpha\\) is not constrained to \\(0\\) . Answer: \\(\\displaystyle {\\hat {\\alpha}}=-1314.16; \\displaystyle {\\hat {\\beta}}=1.30453\\) . \\({\\displaystyle {\\hat {\\beta }}={\\frac {\\sum _{i=1}^{n}(x_{i}-\\overline {x})i(y_{i}-\\overline {y})}{\\sum _{i=1}^{n}(x_{i}-\\overline {x})^{2}}}}\\) \\({\\displaystyle {\\hat {\\alpha }}=\\overline {y}-\\displaystyle {\\hat {\\beta }}\\overline {x}}\\) You can use any program you want, to compare the results for the corresponding linear fit. Here is a solution in gnuplot: f ( x ) = a + b * x a = 0 fit f ( x ) \"regression.dat\" via b Fina l set of parameters Asymptotic Standard Error ======================= ========================== b = 0.928142 +/- 0.004221 ( 0.4548 % ) f ( x ) = a + b * x fit f ( x ) \"regression.dat\" via a , b Fina l set of parameters Asymptotic Standard Error ======================= ========================== a = -1314 .16 +/- 27.58 ( 2.099 % ) b = 1.30453 +/- 0.00794 ( 0.6086 % ) Possible solutions: 1) $ awk '{up= up + $1*$2; down= down + $1*$1;} END{print up/down}' regression . dat 0.928142 2) $ awk '{x= x + $1; y= y + $2;N=NR} END{print x/N, y/N}' regression . dat 3455.49 3193.63 $ awk '{up= up + ($1-3455.49)*($2-3193.63); down= down + ($1-3455.49)**2} END{print \"b=\"up/down\" a=\"3193.63-up/down*3455.49}' regression . dat b = 1.30453 a =- 1314.16 Files regression.dat","title":"Simple linear regresion ***"},{"location":"Exercises/Linear_regression/#simple-linear-regression-wikipedia","text":"The purpose of this exercise is not to learn about \"Linear regression\" but to exercise some simple awk operations on simple data sets that will be equivalent to performing of a simple linear regression calculation. You might be surprised how easy it could be done... The file \"regression.dat\" contains 2 rows of numbers, for convenience named as \\(x_i\\) and \\(y_i\\) , where \\(i\\) is the row (line) number. I will try to follow the nomenclature described on the wikipage. Suppose there are \\(n\\) data points \\({(x_i, y_i), i = 1, ..., n}\\) . The function that describes \\(x\\) and \\(y\\) is: \\(y_i = \\alpha + \\beta x_i + \\varepsilon_i\\) . The goal is to find the equation of the straight line \\(y=\\alpha +\\beta x\\) , which would provide a \"best\" fit for the data points. Let, at the begging, assume \\(\\alpha=0\\) which will result to a \"best\" fit of a straight line which passes through the origin \\((0,0)\\) . There is a simple expression on how to obtain the \\(\\beta\\) parameter \\({\\displaystyle {\\hat {\\beta }}={\\frac {\\sum _{i=1}^{n}x_{i}y_{i}}{\\sum _{i=1}^{n}x_{i}^{2}}}={\\frac {\\overline {xy}}{\\overline {x^{2}}}}}\\) Write an awk script (or one line) that will calculate the beta from the equation above. You will be sutprised how easy it could be calculated... The correct answer for \\(\\displaystyle {\\hat {\\beta}}\\) is 0.928142 Write two scripts, one for the average values \\(\\overline {x}, \\overline {y}\\) and then use the numbers in the second ( directly in the code ), to obtain the results in the case when \\(\\alpha\\) is not constrained to \\(0\\) . Answer: \\(\\displaystyle {\\hat {\\alpha}}=-1314.16; \\displaystyle {\\hat {\\beta}}=1.30453\\) . \\({\\displaystyle {\\hat {\\beta }}={\\frac {\\sum _{i=1}^{n}(x_{i}-\\overline {x})i(y_{i}-\\overline {y})}{\\sum _{i=1}^{n}(x_{i}-\\overline {x})^{2}}}}\\) \\({\\displaystyle {\\hat {\\alpha }}=\\overline {y}-\\displaystyle {\\hat {\\beta }}\\overline {x}}\\) You can use any program you want, to compare the results for the corresponding linear fit. Here is a solution in gnuplot: f ( x ) = a + b * x a = 0 fit f ( x ) \"regression.dat\" via b Fina l set of parameters Asymptotic Standard Error ======================= ========================== b = 0.928142 +/- 0.004221 ( 0.4548 % ) f ( x ) = a + b * x fit f ( x ) \"regression.dat\" via a , b Fina l set of parameters Asymptotic Standard Error ======================= ========================== a = -1314 .16 +/- 27.58 ( 2.099 % ) b = 1.30453 +/- 0.00794 ( 0.6086 % ) Possible solutions: 1) $ awk '{up= up + $1*$2; down= down + $1*$1;} END{print up/down}' regression . dat 0.928142 2) $ awk '{x= x + $1; y= y + $2;N=NR} END{print x/N, y/N}' regression . dat 3455.49 3193.63 $ awk '{up= up + ($1-3455.49)*($2-3193.63); down= down + ($1-3455.49)**2} END{print \"b=\"up/down\" a=\"3193.63-up/down*3455.49}' regression . dat b = 1.30453 a =- 1314.16 Files regression.dat","title":"Simple Linear Regression Wikipedia"},{"location":"Exercises/Scrubbing/","text":"Scrubbing a web page ** Warning 2021.09.10: The new link is active, if for some reason it is changed or it is not working again, please use the webarchived version bellow. Unfortunately, the web engine is changing quite often and even the address might not be correct, so it might not be possible to solve the problem as described... To make it work, please use a copy of the page at https://web.archive.org/ https://web.archive.org/web/20190323025902/http://www.kemi.uu.se/about-us/people-angstrom/ Here is another example when awk comes handy. You can get some information on a web page which is more or less well structured and you want to make some statistics from the numbers you can extract. Disclaimer: there are better tools to this but honestly they come with their own overhead... For instance, on the following web address https://kemi.uu.se/angstrom/about-us/staff one can find list of the people employed at the department of chemistry with their positions. Try to make a simple awk script which will count how many people are employed on different positions. To make the example as general as possible let's work with the HTML source code of the web page (do not worry if you are not familiar with HTML). There are many ways to get the web page from the command line but let's consider two standard tools curl or wget. The commands below will produce identical output with a lot of irrelevant HTML code. $ curl -s https://kemi.uu.se/angstrom/about-us/staff $ wget -O - https://kemi.uu.se/angstrom/about-us/staff !DOCTYPE html> <!--[if lt IE 7]> <html lang=\"sv\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]--> <!--[if IE 7]> <html lang=\"sv\" class=\"no-js lt-ie9 lt-ie8\"> <![endif]--> <!--[if IE 8]> <html lang=\"sv\" class=\"no-js lt-ie9\"> <![endif]--> <!--[if gt IE 8]><!--> < html lang = \"sv\" class = \"no-js\" > <!--<![endif]--> < head > < meta http-equiv = \"content-type\" content = \"text/html;charset=utf-8\" /> < meta http-equiv = \"X-UA-Compatible\" content = \"IE=edge,chrome=1\" /> ... Luckily, all employee titles are on a new line after a line with the following content < span class = \"emp-title\" > . Let's grep for this string and print the line bellow as well with the -A 1 option. curl -s https://kemi.uu.se/angstrom/about-us/staff | grep -A 1 \"emp-title\" | head <span class = \"emp-title\" > visiting researcher -- <span class = \"emp-title\" > degree project worker -- <span class = \"emp-title\" > doctoral/PhD student -- <span class = \"emp-title\" > Can you come with a solution (awk is a good choice) on how to count how many people are employed on each position? Possible solution curl - s https : / /kemi.uu.se/angstrom/about-us/staff | awk ' /emp-title/{getline;title[$0]++} END{for (i in title) print title[i],i}' # or from the WebArchive curl - s https : / /web.archive.org/web/20190323025902/http://www.kemi.uu.se/about-us/people-angstrom/ | awk ' /emp-title/{getline;title[$0]++} END{for (i in title) print title[i],i}' 1 computer coordinator 3 Assistant Professor 1 administrator 8 assistant undergoing research training 35 visiting researcher 3 senior professor 1 administrative manager 1 administrative assistant 1 financial coordinator 10 professor emeritus 44 post doctoral 1 course administrator 20 degree project worker 4 master's thesis students 1 personnel coordinator 1 systems administrator 3 economist 16 professor 4 research engineer 38 researcher 1 instrument maker 1 project coordinator 1 hr-generalist 1 information officer 1 technician 79 doctoral/PhD student 1 personnel administrator 1 senioruniversitetlektor i oorganisk kemi 6 guest doctoral student 4 visiting professor 23 senior lecturer 4 associate senior lecturer 3 research assistant 20 visiting student 6 senior research engineer 2 financial administrator 3 visiting senior lecturer","title":"Scrubbing a web page **"},{"location":"Exercises/Scrubbing/#scrubbing-a-web-page","text":"Warning 2021.09.10: The new link is active, if for some reason it is changed or it is not working again, please use the webarchived version bellow. Unfortunately, the web engine is changing quite often and even the address might not be correct, so it might not be possible to solve the problem as described... To make it work, please use a copy of the page at https://web.archive.org/ https://web.archive.org/web/20190323025902/http://www.kemi.uu.se/about-us/people-angstrom/ Here is another example when awk comes handy. You can get some information on a web page which is more or less well structured and you want to make some statistics from the numbers you can extract. Disclaimer: there are better tools to this but honestly they come with their own overhead... For instance, on the following web address https://kemi.uu.se/angstrom/about-us/staff one can find list of the people employed at the department of chemistry with their positions. Try to make a simple awk script which will count how many people are employed on different positions. To make the example as general as possible let's work with the HTML source code of the web page (do not worry if you are not familiar with HTML). There are many ways to get the web page from the command line but let's consider two standard tools curl or wget. The commands below will produce identical output with a lot of irrelevant HTML code. $ curl -s https://kemi.uu.se/angstrom/about-us/staff $ wget -O - https://kemi.uu.se/angstrom/about-us/staff !DOCTYPE html> <!--[if lt IE 7]> <html lang=\"sv\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]--> <!--[if IE 7]> <html lang=\"sv\" class=\"no-js lt-ie9 lt-ie8\"> <![endif]--> <!--[if IE 8]> <html lang=\"sv\" class=\"no-js lt-ie9\"> <![endif]--> <!--[if gt IE 8]><!--> < html lang = \"sv\" class = \"no-js\" > <!--<![endif]--> < head > < meta http-equiv = \"content-type\" content = \"text/html;charset=utf-8\" /> < meta http-equiv = \"X-UA-Compatible\" content = \"IE=edge,chrome=1\" /> ... Luckily, all employee titles are on a new line after a line with the following content < span class = \"emp-title\" > . Let's grep for this string and print the line bellow as well with the -A 1 option. curl -s https://kemi.uu.se/angstrom/about-us/staff | grep -A 1 \"emp-title\" | head <span class = \"emp-title\" > visiting researcher -- <span class = \"emp-title\" > degree project worker -- <span class = \"emp-title\" > doctoral/PhD student -- <span class = \"emp-title\" > Can you come with a solution (awk is a good choice) on how to count how many people are employed on each position? Possible solution curl - s https : / /kemi.uu.se/angstrom/about-us/staff | awk ' /emp-title/{getline;title[$0]++} END{for (i in title) print title[i],i}' # or from the WebArchive curl - s https : / /web.archive.org/web/20190323025902/http://www.kemi.uu.se/about-us/people-angstrom/ | awk ' /emp-title/{getline;title[$0]++} END{for (i in title) print title[i],i}' 1 computer coordinator 3 Assistant Professor 1 administrator 8 assistant undergoing research training 35 visiting researcher 3 senior professor 1 administrative manager 1 administrative assistant 1 financial coordinator 10 professor emeritus 44 post doctoral 1 course administrator 20 degree project worker 4 master's thesis students 1 personnel coordinator 1 systems administrator 3 economist 16 professor 4 research engineer 38 researcher 1 instrument maker 1 project coordinator 1 hr-generalist 1 information officer 1 technician 79 doctoral/PhD student 1 personnel administrator 1 senioruniversitetlektor i oorganisk kemi 6 guest doctoral student 4 visiting professor 23 senior lecturer 4 associate senior lecturer 3 research assistant 20 visiting student 6 senior research engineer 2 financial administrator 3 visiting senior lecturer","title":"Scrubbing a web page **"},{"location":"Exercises/String_manipulation/","text":"String manipulation ** This exercise is not specific for awk, but I keep getting questions of this kind, since it is rather common situation. Anyway, if this is what you find interesting, here are few task that will require you to read a bit on how to use the special formatting . Feel free to look in the answers and decode the code. Let us use the following file. strfunc.dat Daniel 10.32 Anders 7.44 Sven 56.898 Ali -17.2 Peter 6 Task 1 Write an awk script that aligns the data in such way. Daniel 10.320 Anders 7.440 Sven 56.898 Ali -17.200 Peter 6.000 Posible solution $ awk '{printf(\"%-10s%7.3f\\n\",$1,$2) }' strfunc . dat Task 2 There is away to align without taking into account the sign... Daniel 10.320 Anders 7.440 Sven 56.898 Ali -17.200 Peter 6.000 Posible solution $ awk '{printf(\"%-7s % .3f\\n\",$1,$2) }' strfunc . dat Task 3 Can you modify the script so you get this form which preserves the original data and formatting, but still makes it more readable. Daniel 10.32 Anders 7.44 Sven 56.898 Ali -17.2 Peter 6 Posible solution $ awk '{printf(\"%7s %-7s\\n\",$1,$2) }' strfunc . dat Task 4 Can you use the data to compose strings like this? Dan+10.320 And+7.440 Sve+56.898 Ali-17.200 Pet+6.000 Posible solution $ awk '{printf(\"%.3s%+.3f\\n\",$1,$2) }' strfunc . dat","title":"String manipulation **"},{"location":"Exercises/String_manipulation/#string-manipulation","text":"This exercise is not specific for awk, but I keep getting questions of this kind, since it is rather common situation. Anyway, if this is what you find interesting, here are few task that will require you to read a bit on how to use the special formatting . Feel free to look in the answers and decode the code. Let us use the following file. strfunc.dat Daniel 10.32 Anders 7.44 Sven 56.898 Ali -17.2 Peter 6","title":"String manipulation **"},{"location":"Exercises/String_manipulation/#task-1","text":"Write an awk script that aligns the data in such way. Daniel 10.320 Anders 7.440 Sven 56.898 Ali -17.200 Peter 6.000 Posible solution $ awk '{printf(\"%-10s%7.3f\\n\",$1,$2) }' strfunc . dat","title":"Task 1"},{"location":"Exercises/String_manipulation/#task-2","text":"There is away to align without taking into account the sign... Daniel 10.320 Anders 7.440 Sven 56.898 Ali -17.200 Peter 6.000 Posible solution $ awk '{printf(\"%-7s % .3f\\n\",$1,$2) }' strfunc . dat","title":"Task 2"},{"location":"Exercises/String_manipulation/#task-3","text":"Can you modify the script so you get this form which preserves the original data and formatting, but still makes it more readable. Daniel 10.32 Anders 7.44 Sven 56.898 Ali -17.2 Peter 6 Posible solution $ awk '{printf(\"%7s %-7s\\n\",$1,$2) }' strfunc . dat","title":"Task 3"},{"location":"Exercises/String_manipulation/#task-4","text":"Can you use the data to compose strings like this? Dan+10.320 And+7.440 Sve+56.898 Ali-17.200 Pet+6.000 Posible solution $ awk '{printf(\"%.3s%+.3f\\n\",$1,$2) }' strfunc . dat","title":"Task 4"},{"location":"Exercises/awk-json/","text":"Awk and json To begin with, awk is not strong at dealing with json or csv. Having said that, one can still tackle particular or specific problems. Here is a small piece from the stream of json formatted data coming from different sensors. rtl_433a.log { \"time\" : \"2020-12-19 08:50:39\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 183 , \"temperature_C\" : 21.800 , \"humidity\" : 45 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:50:40\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.900 , \"humidity\" : 74 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:50:44\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 184 , \"temperature_C\" : 20.200 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:50:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 20 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:50:48\" , \"model\" : \"Acurite-Rain\" , \"id\" : 34 , \"rain_mm\" : 851.000 } { \"time\" : \"2020-12-19 08:50:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 16 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:50:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 16 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:50:54\" , \"model\" : \"Fineoffset-WH2\" , \"id\" : 215 , \"temperature_C\" : 22.600 , \"humidity\" : 45 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:50:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:50:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:50:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:51:00\" , \"model\" : \"Acurite-Rain\" , \"id\" : 9 , \"rain_mm\" : 2032.000 } { \"time\" : \"2020-12-19 08:51:16\" , \"model\" : \"Nexus-TH\" , \"id\" : 4 , \"channel\" : 2 , \"battery\" : 71 , \"temperature_C\" : 6.800 , \"humidity\" : 89 } { \"time\" : \"2020-12-19 08:51:16\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 40 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:51:27\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 183 , \"temperature_C\" : 21.800 , \"humidity\" : 45 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:51:28\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.900 , \"humidity\" : 74 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:51:32\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 184 , \"temperature_C\" : 20.200 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:51:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:51:48\" , \"model\" : \"Acurite-Rain\" , \"id\" : 34 , \"rain_mm\" : 851.000 } { \"time\" : \"2020-12-19 08:51:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:51:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:51:54\" , \"model\" : \"Fineoffset-WH2\" , \"id\" : 216 , \"temperature_C\" : 21.600 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:51:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:51:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:52:15\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 183 , \"temperature_C\" : 21.800 , \"humidity\" : 45 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:52:16\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 74 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:52:16\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 74 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:52:16\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 74 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:52:20\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 184 , \"temperature_C\" : 20.100 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:52:23\" , \"model\" : \"Nexus-TH\" , \"id\" : 4 , \"channel\" : 2 , \"battery\" : 71 , \"temperature_C\" : 6.800 , \"humidity\" : 89 } { \"time\" : \"2020-12-19 08:52:23\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 30 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:52:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 14 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:52:48\" , \"model\" : \"Acurite-Rain\" , \"id\" : 34 , \"rain_mm\" : 851.000 } { \"time\" : \"2020-12-19 08:52:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:52:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:52:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:52:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:52:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:53:03\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 183 , \"temperature_C\" : 21.900 , \"humidity\" : 45 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:53:04\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 74 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:53:08\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 184 , \"temperature_C\" : 20.100 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:53:30\" , \"model\" : \"Nexus-TH\" , \"id\" : 4 , \"channel\" : 2 , \"battery\" : 71 , \"temperature_C\" : 6.800 , \"humidity\" : 89 } { \"time\" : \"2020-12-19 08:53:30\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 20 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:53:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 13 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:53:48\" , \"model\" : \"Acurite-Rain\" , \"id\" : 34 , \"rain_mm\" : 851.000 } { \"time\" : \"2020-12-19 08:53:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 12 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:53:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 12 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:53:51\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 183 , \"temperature_C\" : 21.900 , \"humidity\" : 45 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:53:52\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 74 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:53:54\" , \"model\" : \"Fineoffset-WH2\" , \"id\" : 216 , \"temperature_C\" : 21.600 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:53:56\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 184 , \"temperature_C\" : 20.100 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:53:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:53:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:53:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:54:37\" , \"model\" : \"Nexus-TH\" , \"id\" : 4 , \"channel\" : 2 , \"battery\" : 71 , \"temperature_C\" : 6.800 , \"humidity\" : 89 } { \"time\" : \"2020-12-19 08:54:37\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 20 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:54:39\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 183 , \"temperature_C\" : 21.800 , \"humidity\" : 45 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:54:40\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 75 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:54:40\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 75 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:54:40\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 75 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:54:44\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 184 , \"temperature_C\" : 20.100 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:54:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 10 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:54:48\" , \"model\" : \"Acurite-Rain\" , \"id\" : 34 , \"rain_mm\" : 851.000 } { \"time\" : \"2020-12-19 08:54:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 10 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:54:54\" , \"model\" : \"Fineoffset-WH2\" , \"id\" : 215 , \"temperature_C\" : 22.600 , \"humidity\" : 45 , \"mic\" : \"CRC\" } Task 1 We are interested only from the temperature values for Fineoffset-TelldusProove with id : 183 , so let's tabulate them to something we can easily read and plot. ( * ) 2020-12-19T08:50:39 21.800 2020-12-19T08:51:27 21.800 2020-12-19T08:52:15 21.800 2020-12-19T08:53:03 21.900 2020-12-19T08:53:51 21.900 2020-12-19T08:54:39 21.800 Possible solution awk '/\"Fineoffset-TelldusProove\", \"id\" : 183/ {gsub(\",|\\\"\",\"\"); print $3\"T\"$4,$13}' rtl_433a.log Task 2 We want to filter the stream and print the original line for sensors with value for battery less than 20%. ( *** ) { \"time\" : \"2020-12-19 08:50:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 20 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:50:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 16 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:50:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 16 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:51:16\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 40 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:51:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:51:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:51:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:52:23\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 30 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:52:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 14 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:52:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:52:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:53:30\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 20 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:53:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 13 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:53:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 12 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:53:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 12 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:54:37\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 20 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:54:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 10 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:54:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 10 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } Possible solution awk -F '\"battery\" :' '/battery/{if ($2+0<=50) print $0 }' rtl_433a.log # Same result using proper tool (something like advanced grep or basic awk for json) # https://stedolan.github.io/jq/ jq -r -c 'select(.battery > 0 and .battery <= 20)' rtl_433a.log","title":"Awk and json **"},{"location":"Exercises/awk-json/#awk-and-json","text":"To begin with, awk is not strong at dealing with json or csv. Having said that, one can still tackle particular or specific problems. Here is a small piece from the stream of json formatted data coming from different sensors. rtl_433a.log { \"time\" : \"2020-12-19 08:50:39\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 183 , \"temperature_C\" : 21.800 , \"humidity\" : 45 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:50:40\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.900 , \"humidity\" : 74 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:50:44\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 184 , \"temperature_C\" : 20.200 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:50:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 20 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:50:48\" , \"model\" : \"Acurite-Rain\" , \"id\" : 34 , \"rain_mm\" : 851.000 } { \"time\" : \"2020-12-19 08:50:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 16 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:50:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 16 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:50:54\" , \"model\" : \"Fineoffset-WH2\" , \"id\" : 215 , \"temperature_C\" : 22.600 , \"humidity\" : 45 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:50:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:50:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:50:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:51:00\" , \"model\" : \"Acurite-Rain\" , \"id\" : 9 , \"rain_mm\" : 2032.000 } { \"time\" : \"2020-12-19 08:51:16\" , \"model\" : \"Nexus-TH\" , \"id\" : 4 , \"channel\" : 2 , \"battery\" : 71 , \"temperature_C\" : 6.800 , \"humidity\" : 89 } { \"time\" : \"2020-12-19 08:51:16\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 40 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:51:27\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 183 , \"temperature_C\" : 21.800 , \"humidity\" : 45 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:51:28\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.900 , \"humidity\" : 74 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:51:32\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 184 , \"temperature_C\" : 20.200 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:51:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:51:48\" , \"model\" : \"Acurite-Rain\" , \"id\" : 34 , \"rain_mm\" : 851.000 } { \"time\" : \"2020-12-19 08:51:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:51:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:51:54\" , \"model\" : \"Fineoffset-WH2\" , \"id\" : 216 , \"temperature_C\" : 21.600 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:51:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:51:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:52:15\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 183 , \"temperature_C\" : 21.800 , \"humidity\" : 45 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:52:16\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 74 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:52:16\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 74 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:52:16\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 74 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:52:20\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 184 , \"temperature_C\" : 20.100 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:52:23\" , \"model\" : \"Nexus-TH\" , \"id\" : 4 , \"channel\" : 2 , \"battery\" : 71 , \"temperature_C\" : 6.800 , \"humidity\" : 89 } { \"time\" : \"2020-12-19 08:52:23\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 30 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:52:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 14 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:52:48\" , \"model\" : \"Acurite-Rain\" , \"id\" : 34 , \"rain_mm\" : 851.000 } { \"time\" : \"2020-12-19 08:52:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:52:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:52:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:52:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:52:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:53:03\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 183 , \"temperature_C\" : 21.900 , \"humidity\" : 45 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:53:04\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 74 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:53:08\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 184 , \"temperature_C\" : 20.100 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:53:30\" , \"model\" : \"Nexus-TH\" , \"id\" : 4 , \"channel\" : 2 , \"battery\" : 71 , \"temperature_C\" : 6.800 , \"humidity\" : 89 } { \"time\" : \"2020-12-19 08:53:30\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 20 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:53:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 13 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:53:48\" , \"model\" : \"Acurite-Rain\" , \"id\" : 34 , \"rain_mm\" : 851.000 } { \"time\" : \"2020-12-19 08:53:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 12 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:53:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 12 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:53:51\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 183 , \"temperature_C\" : 21.900 , \"humidity\" : 45 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:53:52\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 74 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:53:54\" , \"model\" : \"Fineoffset-WH2\" , \"id\" : 216 , \"temperature_C\" : 21.600 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:53:56\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 184 , \"temperature_C\" : 20.100 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:53:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:53:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:53:58\" , \"model\" : \"WT450-TH\" , \"id\" : 15 , \"channel\" : 1 , \"battery\" : 100 , \"temperature_C\" : 7.125 , \"humidity\" : 90 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:54:37\" , \"model\" : \"Nexus-TH\" , \"id\" : 4 , \"channel\" : 2 , \"battery\" : 71 , \"temperature_C\" : 6.800 , \"humidity\" : 89 } { \"time\" : \"2020-12-19 08:54:37\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 20 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:54:39\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 183 , \"temperature_C\" : 21.800 , \"humidity\" : 45 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:54:40\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 75 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:54:40\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 75 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:54:40\" , \"model\" : \"Fineoffset-WH5\" , \"id\" : 200 , \"temperature_C\" : -31.800 , \"humidity\" : 75 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:54:44\" , \"model\" : \"Fineoffset-TelldusProove\" , \"id\" : 184 , \"temperature_C\" : 20.100 , \"mic\" : \"CRC\" } { \"time\" : \"2020-12-19 08:54:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 10 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:54:48\" , \"model\" : \"Acurite-Rain\" , \"id\" : 34 , \"rain_mm\" : 851.000 } { \"time\" : \"2020-12-19 08:54:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 10 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:54:54\" , \"model\" : \"Fineoffset-WH2\" , \"id\" : 215 , \"temperature_C\" : 22.600 , \"humidity\" : 45 , \"mic\" : \"CRC\" }","title":"Awk and json"},{"location":"Exercises/awk-json/#task-1","text":"We are interested only from the temperature values for Fineoffset-TelldusProove with id : 183 , so let's tabulate them to something we can easily read and plot. ( * ) 2020-12-19T08:50:39 21.800 2020-12-19T08:51:27 21.800 2020-12-19T08:52:15 21.800 2020-12-19T08:53:03 21.900 2020-12-19T08:53:51 21.900 2020-12-19T08:54:39 21.800 Possible solution awk '/\"Fineoffset-TelldusProove\", \"id\" : 183/ {gsub(\",|\\\"\",\"\"); print $3\"T\"$4,$13}' rtl_433a.log","title":"Task 1"},{"location":"Exercises/awk-json/#task-2","text":"We want to filter the stream and print the original line for sensors with value for battery less than 20%. ( *** ) { \"time\" : \"2020-12-19 08:50:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 20 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:50:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 16 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:50:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 16 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:51:16\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 40 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:51:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:51:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:51:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:52:23\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 30 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:52:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 14 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:52:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:52:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 15 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:53:30\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 20 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:53:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 13 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:53:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 12 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } { \"time\" : \"2020-12-19 08:53:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 12 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 2 } { \"time\" : \"2020-12-19 08:54:37\" , \"model\" : \"Eurochron-TH\" , \"id\" : 4 , \"battery\" : 20 , \"temperature_C\" : -16.700 , \"humidity\" : 68 , \"button\" : 1 } { \"time\" : \"2020-12-19 08:54:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 10 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 0 } { \"time\" : \"2020-12-19 08:54:48\" , \"model\" : \"WT450-TH\" , \"id\" : 13 , \"channel\" : 1 , \"battery\" : 10 , \"temperature_C\" : 23.250 , \"humidity\" : 36 , \"seq\" : 1 } Possible solution awk -F '\"battery\" :' '/battery/{if ($2+0<=50) print $0 }' rtl_433a.log # Same result using proper tool (something like advanced grep or basic awk for json) # https://stedolan.github.io/jq/ jq -r -c 'select(.battery > 0 and .battery <= 20)' rtl_433a.log","title":"Task 2"},{"location":"Exercises/gtf-teaser/","text":"How to Analyze a Transcriptome Like a Pro This is just a teaser for the full tutorial: AWK GTF! How to Analyze a Transcriptome Like a Pro Let's use small file to exercise a bit with the content. $ wget https://raw.github.com/nachocab/nachocab.github.io/master/assets/transcriptome.gtf Hint: to get the line unwrapped in the terminal pipe the output to less -S $ head transcriptome.gtf | less -S ##description: evidence-based annotation of the human genome (GRCh37), version 18 (Ensembl 73) ##provider: GENCODE ##contact: gencode@sanger.ac.uk ##format: gtf ##date: 2013-09-02 chr1 HAVANA exon 173753 173862 . - . gene_id \"ENSG00000241860.2\" ; transcript_id \"ENST00000466557.2\" ; gene_type \"processed_transcript\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.13\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"RP11-34P13.13-001\" ; exon_number 1 ; exon_id \"ENSE00001947154.2\" ; level 2 ; tag \"not_best_in_genome_evidence\" ; havana_gene \"OTTHUMG00000002480.3\" ; havana_transcript \"OTTHUMT00000007037.2\" ; chr1 HAVANA transcript 1246986 1250550 . - . gene_id \"ENSG00000127054.14\" ; transcript_id \"ENST00000478641.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"CPSF3L\" ; transcript_type \"retained_intron\" ; transcript_status \"KNOWN\" ; transcript_name \"CPSF3L-006\" ; level 2 ; havana_gene \"OTTHUMG00000003330.11\" ; havana_transcript \"OTTHUMT00000009365.1\" ; chr1 HAVANA CDS 1461841 1461911 . + 0 gene_id \"ENSG00000197785.9\" ; transcript_id \"ENST00000378755.5\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"ATAD3A\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"ATAD3A-003\" ; exon_number 13 ; exon_id \"ENSE00001664426.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS31.1\" ; havana_gene \"OTTHUMG00000000575.6\" ; havana_transcript \"OTTHUMT00000001365.1\" ; chr1 HAVANA exon 1693391 1693474 . - . gene_id \"ENSG00000008130.11\" ; transcript_id \"ENST00000341991.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NADK\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"NADK-002\" ; exon_number 3 ; exon_id \"ENSE00003487616.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS30565.1\" ; havana_gene \"OTTHUMG00000000942.5\" ; havana_transcript \"OTTHUMT00000002768.1\" ; chr1 HAVANA CDS 1688280 1688321 . - 0 gene_id \"ENSG00000008130.11\" ; transcript_id \"ENST00000497186.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NADK\" ; transcript_type \"nonsense_mediated_decay\" ; transcript_status \"KNOWN\" ; transcript_name \"NADK-008\" ; exon_number 2 ; exon_id \"ENSE00001856899.1\" ; level 2 ; tag \"mRNA_start_NF\" ; tag \"cds_start_NF\" ; havana_gene \"OTTHUMG00000000942.5\" ; havana_transcript \"OTTHUMT00000002774.3\" ; The transcriptome has 9 columns. The first 8 are separated by tabs and look reasonable (chromosome, annotation source, feature type, start, end, score, strand, and phase), the last one is kind of hairy: it is made up of key-value pairs separated by semicolons, some fields are mandatory and others are optional, and the values are surrounded in double quotes. That\u2019s no way to live a decent life. ( text copied from the source ) let's get only the lines that have gene in the 3 th column. $ awk -F '$3 == \"gene\"' transcriptome.gtf | head | less -S chr1 HAVANA gene 11869 14412 . + . gene_id \"ENSG00000223972.4\" ; transcript_id \"ENSG00000223972.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1\" ; level 2 ; havana_gene \"OTTHUMG00000000961.2\" ; chr1 HAVANA gene 14363 29806 . - . gene_id \"ENSG00000227232.4\" ; transcript_id \"ENSG00000227232.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"WASH7P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"WASH7P\" ; level 2 ; havana_gene \"OTTHUMG00000000958.1\" ; chr1 HAVANA gene 29554 31109 . + . gene_id \"ENSG00000243485.2\" ; transcript_id \"ENSG00000243485.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"MIR1302-11\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"MIR1302-11\" ; level 2 ; havana_gene \"OTTHUMG00000000959.2\" ; chr1 HAVANA gene 34554 36081 . - . gene_id \"ENSG00000237613.2\" ; transcript_id \"ENSG00000237613.2\" ; gene_type \"lincRNA\" ; gene_status \"KNOWN\" ; gene_name \"FAM138A\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"FAM138A\" ; level 2 ; havana_gene \"OTTHUMG00000000960.1\" ; chr1 HAVANA gene 52473 54936 . + . gene_id \"ENSG00000268020.2\" ; transcript_id \"ENSG00000268020.2\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G4P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G4P\" ; level 2 ; havana_gene \"OTTHUMG00000185779.1\" ; chr1 HAVANA gene 62948 63887 . + . gene_id \"ENSG00000240361.1\" ; transcript_id \"ENSG00000240361.1\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G11P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G11P\" ; level 2 ; havana_gene \"OTTHUMG00000001095.2\" ; chr1 HAVANA gene 69091 70008 . + . gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; chr1 HAVANA gene 89295 133566 . - . gene_id \"ENSG00000238009.2\" ; transcript_id \"ENSG00000238009.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.7\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.7\" ; level 2 ; havana_gene \"OTTHUMG00000001096.2\" ; chr1 HAVANA gene 89551 91105 . - . gene_id \"ENSG00000239945.1\" ; transcript_id \"ENSG00000239945.1\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.8\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.8\" ; level 2 ; havana_gene \"OTTHUMG00000001097.2\" ; chr1 HAVANA gene 131025 134836 . + . gene_id \"ENSG00000233750.3\" ; transcript_id \"ENSG00000233750.3\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"CICP27\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"CICP27\" ; level 1 ; tag \"pseudo_consens\" ; havana_gene \"OTTHUMG00000001257.3\" ; Perhaps filter a bit more and print the content of the 9 th column in the file. $ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | head | less -S gene_id \"ENSG00000223972.4\" ; transcript_id \"ENSG00000223972.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1\" ; level 2 ; havana_gene \"OTTHUMG00000000961.2\" ; gene_id \"ENSG00000227232.4\" ; transcript_id \"ENSG00000227232.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"WASH7P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"WASH7P\" ; level 2 ; havana_gene \"OTTHUMG00000000958.1\" ; gene_id \"ENSG00000243485.2\" ; transcript_id \"ENSG00000243485.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"MIR1302-11\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"MIR1302-11\" ; level 2 ; havana_gene \"OTTHUMG00000000959.2\" ; gene_id \"ENSG00000237613.2\" ; transcript_id \"ENSG00000237613.2\" ; gene_type \"lincRNA\" ; gene_status \"KNOWN\" ; gene_name \"FAM138A\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"FAM138A\" ; level 2 ; havana_gene \"OTTHUMG00000000960.1\" ; gene_id \"ENSG00000268020.2\" ; transcript_id \"ENSG00000268020.2\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G4P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G4P\" ; level 2 ; havana_gene \"OTTHUMG00000185779.1\" ; gene_id \"ENSG00000240361.1\" ; transcript_id \"ENSG00000240361.1\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G11P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G11P\" ; level 2 ; havana_gene \"OTTHUMG00000001095.2\" ; gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; gene_id \"ENSG00000238009.2\" ; transcript_id \"ENSG00000238009.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.7\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.7\" ; level 2 ; havana_gene \"OTTHUMG00000001096.2\" ; gene_id \"ENSG00000239945.1\" ; transcript_id \"ENSG00000239945.1\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.8\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.8\" ; level 2 ; havana_gene \"OTTHUMG00000001097.2\" ; gene_id \"ENSG00000233750.3\" ; transcript_id \"ENSG00000233750.3\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"CICP27\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"CICP27\" ; level 1 ; tag \"pseudo_consens\" ; havana_gene \"OTTHUMG00000001257.3\" ; What about if we want just a specific piece from this information? We can | the output from the first awk script in to a second one. Note that we will use different field separator \"; \" . $ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | awk -F \"; \" '{ print $3 }' | head gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"protein_coding\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" At this point I will suggest to follow the original tutorial: AWK GTF! How to Analyze a Transcriptome Like a Pro","title":"How to Analyze a Transcriptome Like a Pro"},{"location":"Exercises/gtf-teaser/#how-to-analyze-a-transcriptome-like-a-pro","text":"This is just a teaser for the full tutorial: AWK GTF! How to Analyze a Transcriptome Like a Pro Let's use small file to exercise a bit with the content. $ wget https://raw.github.com/nachocab/nachocab.github.io/master/assets/transcriptome.gtf Hint: to get the line unwrapped in the terminal pipe the output to less -S $ head transcriptome.gtf | less -S ##description: evidence-based annotation of the human genome (GRCh37), version 18 (Ensembl 73) ##provider: GENCODE ##contact: gencode@sanger.ac.uk ##format: gtf ##date: 2013-09-02 chr1 HAVANA exon 173753 173862 . - . gene_id \"ENSG00000241860.2\" ; transcript_id \"ENST00000466557.2\" ; gene_type \"processed_transcript\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.13\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"RP11-34P13.13-001\" ; exon_number 1 ; exon_id \"ENSE00001947154.2\" ; level 2 ; tag \"not_best_in_genome_evidence\" ; havana_gene \"OTTHUMG00000002480.3\" ; havana_transcript \"OTTHUMT00000007037.2\" ; chr1 HAVANA transcript 1246986 1250550 . - . gene_id \"ENSG00000127054.14\" ; transcript_id \"ENST00000478641.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"CPSF3L\" ; transcript_type \"retained_intron\" ; transcript_status \"KNOWN\" ; transcript_name \"CPSF3L-006\" ; level 2 ; havana_gene \"OTTHUMG00000003330.11\" ; havana_transcript \"OTTHUMT00000009365.1\" ; chr1 HAVANA CDS 1461841 1461911 . + 0 gene_id \"ENSG00000197785.9\" ; transcript_id \"ENST00000378755.5\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"ATAD3A\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"ATAD3A-003\" ; exon_number 13 ; exon_id \"ENSE00001664426.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS31.1\" ; havana_gene \"OTTHUMG00000000575.6\" ; havana_transcript \"OTTHUMT00000001365.1\" ; chr1 HAVANA exon 1693391 1693474 . - . gene_id \"ENSG00000008130.11\" ; transcript_id \"ENST00000341991.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NADK\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"NADK-002\" ; exon_number 3 ; exon_id \"ENSE00003487616.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS30565.1\" ; havana_gene \"OTTHUMG00000000942.5\" ; havana_transcript \"OTTHUMT00000002768.1\" ; chr1 HAVANA CDS 1688280 1688321 . - 0 gene_id \"ENSG00000008130.11\" ; transcript_id \"ENST00000497186.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NADK\" ; transcript_type \"nonsense_mediated_decay\" ; transcript_status \"KNOWN\" ; transcript_name \"NADK-008\" ; exon_number 2 ; exon_id \"ENSE00001856899.1\" ; level 2 ; tag \"mRNA_start_NF\" ; tag \"cds_start_NF\" ; havana_gene \"OTTHUMG00000000942.5\" ; havana_transcript \"OTTHUMT00000002774.3\" ; The transcriptome has 9 columns. The first 8 are separated by tabs and look reasonable (chromosome, annotation source, feature type, start, end, score, strand, and phase), the last one is kind of hairy: it is made up of key-value pairs separated by semicolons, some fields are mandatory and others are optional, and the values are surrounded in double quotes. That\u2019s no way to live a decent life. ( text copied from the source ) let's get only the lines that have gene in the 3 th column. $ awk -F '$3 == \"gene\"' transcriptome.gtf | head | less -S chr1 HAVANA gene 11869 14412 . + . gene_id \"ENSG00000223972.4\" ; transcript_id \"ENSG00000223972.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1\" ; level 2 ; havana_gene \"OTTHUMG00000000961.2\" ; chr1 HAVANA gene 14363 29806 . - . gene_id \"ENSG00000227232.4\" ; transcript_id \"ENSG00000227232.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"WASH7P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"WASH7P\" ; level 2 ; havana_gene \"OTTHUMG00000000958.1\" ; chr1 HAVANA gene 29554 31109 . + . gene_id \"ENSG00000243485.2\" ; transcript_id \"ENSG00000243485.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"MIR1302-11\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"MIR1302-11\" ; level 2 ; havana_gene \"OTTHUMG00000000959.2\" ; chr1 HAVANA gene 34554 36081 . - . gene_id \"ENSG00000237613.2\" ; transcript_id \"ENSG00000237613.2\" ; gene_type \"lincRNA\" ; gene_status \"KNOWN\" ; gene_name \"FAM138A\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"FAM138A\" ; level 2 ; havana_gene \"OTTHUMG00000000960.1\" ; chr1 HAVANA gene 52473 54936 . + . gene_id \"ENSG00000268020.2\" ; transcript_id \"ENSG00000268020.2\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G4P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G4P\" ; level 2 ; havana_gene \"OTTHUMG00000185779.1\" ; chr1 HAVANA gene 62948 63887 . + . gene_id \"ENSG00000240361.1\" ; transcript_id \"ENSG00000240361.1\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G11P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G11P\" ; level 2 ; havana_gene \"OTTHUMG00000001095.2\" ; chr1 HAVANA gene 69091 70008 . + . gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; chr1 HAVANA gene 89295 133566 . - . gene_id \"ENSG00000238009.2\" ; transcript_id \"ENSG00000238009.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.7\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.7\" ; level 2 ; havana_gene \"OTTHUMG00000001096.2\" ; chr1 HAVANA gene 89551 91105 . - . gene_id \"ENSG00000239945.1\" ; transcript_id \"ENSG00000239945.1\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.8\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.8\" ; level 2 ; havana_gene \"OTTHUMG00000001097.2\" ; chr1 HAVANA gene 131025 134836 . + . gene_id \"ENSG00000233750.3\" ; transcript_id \"ENSG00000233750.3\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"CICP27\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"CICP27\" ; level 1 ; tag \"pseudo_consens\" ; havana_gene \"OTTHUMG00000001257.3\" ; Perhaps filter a bit more and print the content of the 9 th column in the file. $ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | head | less -S gene_id \"ENSG00000223972.4\" ; transcript_id \"ENSG00000223972.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1\" ; level 2 ; havana_gene \"OTTHUMG00000000961.2\" ; gene_id \"ENSG00000227232.4\" ; transcript_id \"ENSG00000227232.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"WASH7P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"WASH7P\" ; level 2 ; havana_gene \"OTTHUMG00000000958.1\" ; gene_id \"ENSG00000243485.2\" ; transcript_id \"ENSG00000243485.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"MIR1302-11\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"MIR1302-11\" ; level 2 ; havana_gene \"OTTHUMG00000000959.2\" ; gene_id \"ENSG00000237613.2\" ; transcript_id \"ENSG00000237613.2\" ; gene_type \"lincRNA\" ; gene_status \"KNOWN\" ; gene_name \"FAM138A\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"FAM138A\" ; level 2 ; havana_gene \"OTTHUMG00000000960.1\" ; gene_id \"ENSG00000268020.2\" ; transcript_id \"ENSG00000268020.2\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G4P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G4P\" ; level 2 ; havana_gene \"OTTHUMG00000185779.1\" ; gene_id \"ENSG00000240361.1\" ; transcript_id \"ENSG00000240361.1\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G11P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G11P\" ; level 2 ; havana_gene \"OTTHUMG00000001095.2\" ; gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; gene_id \"ENSG00000238009.2\" ; transcript_id \"ENSG00000238009.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.7\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.7\" ; level 2 ; havana_gene \"OTTHUMG00000001096.2\" ; gene_id \"ENSG00000239945.1\" ; transcript_id \"ENSG00000239945.1\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.8\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.8\" ; level 2 ; havana_gene \"OTTHUMG00000001097.2\" ; gene_id \"ENSG00000233750.3\" ; transcript_id \"ENSG00000233750.3\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"CICP27\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"CICP27\" ; level 1 ; tag \"pseudo_consens\" ; havana_gene \"OTTHUMG00000001257.3\" ; What about if we want just a specific piece from this information? We can | the output from the first awk script in to a second one. Note that we will use different field separator \"; \" . $ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | awk -F \"; \" '{ print $3 }' | head gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"protein_coding\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" At this point I will suggest to follow the original tutorial: AWK GTF! How to Analyze a Transcriptome Like a Pro","title":"How to Analyze a Transcriptome Like a Pro"},{"location":"More_awk/Command_params/","text":"Trick to pass paramters on the command line The common way to pass options and parameters to an Awk script is via the awk option -v varname=value $ script.awk -v variablename1 = value2 -v variablename = value2 filename1 filename2 This is really flexible and has many advantages, but in most cases, you want to use something more common - positional parameters i.e. something like this $ script.awk filename parameter1 parameter2 Without some precautions, parameter1 and parameter2 will be treated as file names... When Awk starts, the information from the command line will be stored in two internal variables ARGV - array with argument values and ARGC - scalar variable with the count of elements in ARGV . ARGV [ 0 ] will contain the name of the script itself ( script.awk in this example), ARGV [ 1 ] - the name of the first file on the command line etc. ARGV [ 0 ] = \"script.awk\" ARGV [ 1 ] = \"filename\" ARGV [ 2 ] = \"parameter1\" ARGV [ 3 ] = \"parameter2\" ARGC = 4 Essentially, awk starts to read files with names taken from the elements of the ARGV array - except for ARGV [ 0 ] . Here is the neat trick - if you change the value ARGC = 2 , awk will \"think\" that there are only 2 elements - program name and first file and run the loop as usual for these 2 elements i.e. reading only filename . Without any surprise, the values in ARGV continue to be available... input.dat 1 2 3 4 5 6 7 8 add_to_columns.awk #!/usr/bin/awk -f BEGIN { ARGC = 2 } { print $ 1 + ARGV [ 2 ], $ 2 + ARGV [ 3 ]} $ ./add_to_column.awk input.dat 1 2 2 4 4 6 6 8 8 10 This \"trick\" could be used in other ways - Rereading the Current File , changing the name of the next file to read as well (if you want) etc.","title":"Parameters on the command line"},{"location":"More_awk/Command_params/#trick-to-pass-paramters-on-the-command-line","text":"The common way to pass options and parameters to an Awk script is via the awk option -v varname=value $ script.awk -v variablename1 = value2 -v variablename = value2 filename1 filename2 This is really flexible and has many advantages, but in most cases, you want to use something more common - positional parameters i.e. something like this $ script.awk filename parameter1 parameter2 Without some precautions, parameter1 and parameter2 will be treated as file names... When Awk starts, the information from the command line will be stored in two internal variables ARGV - array with argument values and ARGC - scalar variable with the count of elements in ARGV . ARGV [ 0 ] will contain the name of the script itself ( script.awk in this example), ARGV [ 1 ] - the name of the first file on the command line etc. ARGV [ 0 ] = \"script.awk\" ARGV [ 1 ] = \"filename\" ARGV [ 2 ] = \"parameter1\" ARGV [ 3 ] = \"parameter2\" ARGC = 4 Essentially, awk starts to read files with names taken from the elements of the ARGV array - except for ARGV [ 0 ] . Here is the neat trick - if you change the value ARGC = 2 , awk will \"think\" that there are only 2 elements - program name and first file and run the loop as usual for these 2 elements i.e. reading only filename . Without any surprise, the values in ARGV continue to be available... input.dat 1 2 3 4 5 6 7 8 add_to_columns.awk #!/usr/bin/awk -f BEGIN { ARGC = 2 } { print $ 1 + ARGV [ 2 ], $ 2 + ARGV [ 3 ]} $ ./add_to_column.awk input.dat 1 2 2 4 4 6 6 8 8 10 This \"trick\" could be used in other ways - Rereading the Current File , changing the name of the next file to read as well (if you want) etc.","title":"Trick to pass paramters on the command line"},{"location":"More_awk/Input_output/","text":"Input/Output to an external program from within awk Reading output from external program Awk has a way to read output from external programs. Here is an example where we will use onle the BEGIN block in order to simplify the discussion. read_ext1.awk #!/usr/bin/awk -f BEGIN { while ( \"lsb_release -a\" | getline ){ print \"awk:\" , $ 0 } } $ ./read_ext1.awk No LSB modules are available. awk: Distributor ID: Ubuntu awk: Description: Ubuntu 18.04.4 LTS awk: Release: 18.04 awk: Codename: bionic Note No LSB modules are available. was sent to /dev/stderr by lsb_release and awk newer got to read it on first place. Warning Kepp in mind that getline will read one line and store it in $0 by replacing the content from the common lines read by awk. To avoid this use getline variablename to store the line in new variable. more... Info Awk can getline directly from another file instead of the one that awk is currently reading - getline < filename more... This second variant will produce the same result, but also illustrates the use of close () . read_ext2.awk #!/usr/bin/awk -f BEGIN { cmd = \"lsb_release -a\" while ( cmd | getline ){ print \"awk:\" , $ 0 } close ( cmd ) } Question What happens if you try to read the output second time without closing? How about if we want to get only the bionic from the Codename ( ignore that you can request this by lsb_release -c ) This version will print only what we need. read_ext3.awk #!/usr/bin/awk -f BEGIN { cmd = \"lsb_release -a\" while ( cmd | getline ){ if ( $ 1 == \"Codename:\" ) print $ 2 } close ( cmd ) } Note You need to redirect standard error to get the clean output. $ ./read_ext3.awk 2> /dev/null bionic Sending data to external program (and reading the output) These examples perhaps are not the best use but will illustrate how awk can send data to the standard input of an external program and read the produced output so you can use the data in your script. Awk does not have a function to find the greatest common divisor but python has such function math.gcd and we can use it by sending commands directly to python. gcd1.awk 1 2 3 4 #!/usr/bin/awk -f BEGIN { print \"import math; print(math.gcd(12,88))\" | \"python3\" } $ ./gcd1.awk 4 This will simply send the commands to python and the output will be printed to standard output. We want the result back . gcd2.awk 1 2 3 4 5 6 7 8 9 10 #!/usr/bin/awk -f BEGIN { cmd = \"python3\" print \"import math; print(math.gcd(12,88));\" | & cmd close ( cmd , \"to\" ) cmd | & getline print \"awk:\" , $ 0 } There is a complication, though. Python is an interactive program and expects end of stream in order to preprocess the data or in many situations - to flush the input buffer. The solution to this is to call close ( cmd , \"to\" ) function on line 6, deeply buried in the awk documentation . This last example covers more or less the most complicated situation. Usually one can get away with fewer lines. Note also that we getline -ed only once since we wanted only the first line in the output. This might not be the case and you might need to run while loop to read all lines. Summary of the eight variants of getline, listing which predefined variables are set by each one, and whether the variant is standard or a gawk extension. Variant Effect awk / gawk getline Sets $0, NF, FNR, NR, and RT awk getline var Sets var, FNR, NR, and RT awk getline < file Sets $0, NF, and RT awk getline var < file Sets var and RT awk command | getline Sets $0, NF, and RT awk command | getline var Sets var and RT awk command |& getline Sets $0, NF, and RT gawk command |& getline var Sets var and RT gawk","title":"Input/Output to external program"},{"location":"More_awk/Input_output/#inputoutput-to-an-external-program-from-within-awk","text":"","title":"Input/Output to an external program from within awk"},{"location":"More_awk/Input_output/#reading-output-from-external-program","text":"Awk has a way to read output from external programs. Here is an example where we will use onle the BEGIN block in order to simplify the discussion. read_ext1.awk #!/usr/bin/awk -f BEGIN { while ( \"lsb_release -a\" | getline ){ print \"awk:\" , $ 0 } } $ ./read_ext1.awk No LSB modules are available. awk: Distributor ID: Ubuntu awk: Description: Ubuntu 18.04.4 LTS awk: Release: 18.04 awk: Codename: bionic Note No LSB modules are available. was sent to /dev/stderr by lsb_release and awk newer got to read it on first place. Warning Kepp in mind that getline will read one line and store it in $0 by replacing the content from the common lines read by awk. To avoid this use getline variablename to store the line in new variable. more... Info Awk can getline directly from another file instead of the one that awk is currently reading - getline < filename more... This second variant will produce the same result, but also illustrates the use of close () . read_ext2.awk #!/usr/bin/awk -f BEGIN { cmd = \"lsb_release -a\" while ( cmd | getline ){ print \"awk:\" , $ 0 } close ( cmd ) } Question What happens if you try to read the output second time without closing? How about if we want to get only the bionic from the Codename ( ignore that you can request this by lsb_release -c ) This version will print only what we need. read_ext3.awk #!/usr/bin/awk -f BEGIN { cmd = \"lsb_release -a\" while ( cmd | getline ){ if ( $ 1 == \"Codename:\" ) print $ 2 } close ( cmd ) } Note You need to redirect standard error to get the clean output. $ ./read_ext3.awk 2> /dev/null bionic","title":"Reading output from external program"},{"location":"More_awk/Input_output/#sending-data-to-external-program-and-reading-the-output","text":"These examples perhaps are not the best use but will illustrate how awk can send data to the standard input of an external program and read the produced output so you can use the data in your script. Awk does not have a function to find the greatest common divisor but python has such function math.gcd and we can use it by sending commands directly to python. gcd1.awk 1 2 3 4 #!/usr/bin/awk -f BEGIN { print \"import math; print(math.gcd(12,88))\" | \"python3\" } $ ./gcd1.awk 4 This will simply send the commands to python and the output will be printed to standard output. We want the result back . gcd2.awk 1 2 3 4 5 6 7 8 9 10 #!/usr/bin/awk -f BEGIN { cmd = \"python3\" print \"import math; print(math.gcd(12,88));\" | & cmd close ( cmd , \"to\" ) cmd | & getline print \"awk:\" , $ 0 } There is a complication, though. Python is an interactive program and expects end of stream in order to preprocess the data or in many situations - to flush the input buffer. The solution to this is to call close ( cmd , \"to\" ) function on line 6, deeply buried in the awk documentation . This last example covers more or less the most complicated situation. Usually one can get away with fewer lines. Note also that we getline -ed only once since we wanted only the first line in the output. This might not be the case and you might need to run while loop to read all lines. Summary of the eight variants of getline, listing which predefined variables are set by each one, and whether the variant is standard or a gawk extension. Variant Effect awk / gawk getline Sets $0, NF, FNR, NR, and RT awk getline var Sets var, FNR, NR, and RT awk getline < file Sets $0, NF, and RT awk getline var < file Sets var and RT awk command | getline Sets $0, NF, and RT awk command | getline var Sets var and RT awk command |& getline Sets $0, NF, and RT gawk command |& getline var Sets var and RT gawk","title":"Sending data to external program (and reading the output)"},{"location":"More_awk/Running_average/","text":"Running/moving/rolling average Definition on Wikipedia Here is an example from my own experience illustrated with a typical problem. On the figure below I have series of numbers ( in this case, the instantaneous temperature from a an MD simulation ) with some typical oscillations. By now, you know how to easily calculate an average and select certain region ( I would prefer to skip at least 100 steps from the beginning... ). Anyway, there is an easier way to visualize the averages by computing the running average over selected interval. It is a commonly visualized property on stock prices link for example. There are countless solutions that you can find on the net. They do the same, but some are more efficient or elegant than others. Some time ago I wrote myself a small code, then improved several times and then I found a \"brilliant\" solution somewhere on the net. Here is the script itself - completely unreadable... The highlighted lines are added for flexibility - to select the column and the range for averaging. run-average.awk 1 2 3 4 5 6 7 8 9 10 #!/usr/bin/awk -f { if ( ! col ) col = 1 if ( ! size ) size = 5 mod = NR % size ; if ( NR <= size ){ count ++ } else { sum -= array [ mod ] }; sum += $ ( col ); array [ mod ] = $ ( col ); print sum / count } Here is how the result from the script looks like. This plot is generated from the Gnuplot command line plot [ : 2000 ] \"temp.dat\" w l , \\ \"< ./run-average.awk size=100 temp.dat\" w l lw 3 lc rgb \"blue\" , \\ \"< ./run-average.awk size=500 temp.dat\" w l lw 3 lc rgb \"green\" This easily \"filters out\" the oscillations and showing the averages for the selected size... Even more the not-so-easy to see drift is clearly visible now ( green line ). Essentially, the program makes an average over selected size range of the previous data at each point. What is particularly smart wit this solution is that the average is done by updating the sum from numbers kept in an array of size size . At every line, the new value is added to the sum and one is subtracted - the one that falls now outside the range for averaging. The position of the element is derived by the reminder computed by the modulo operation mod = NR % size 1 . In principle, the code is only two lines, but the rest is taking care of the initial region where the data is not enough to make an average over the specified size . This is a smart realization of FIFO (first in first out) manipulation of data structure implemented on few lines in this script. Compare this to the code referred in the wikipage . Note I have used to pass different parameters size=100 adn size=500 directly in the gnuplot command instead of a filename ( not all programs allow you to make such neat substitutions ) but this is one example where awk gets handy - just write you one line script between \"< \" . The value of such programs is that it makes it easy to add or modify small bits, since there is not much of a code anyway. Files run-average.awk temp.dat Gnuawk: Arithmetic Operators \u21a9","title":"Running averages"},{"location":"More_awk/Running_average/#runningmovingrolling-average","text":"Definition on Wikipedia Here is an example from my own experience illustrated with a typical problem. On the figure below I have series of numbers ( in this case, the instantaneous temperature from a an MD simulation ) with some typical oscillations. By now, you know how to easily calculate an average and select certain region ( I would prefer to skip at least 100 steps from the beginning... ). Anyway, there is an easier way to visualize the averages by computing the running average over selected interval. It is a commonly visualized property on stock prices link for example. There are countless solutions that you can find on the net. They do the same, but some are more efficient or elegant than others. Some time ago I wrote myself a small code, then improved several times and then I found a \"brilliant\" solution somewhere on the net. Here is the script itself - completely unreadable... The highlighted lines are added for flexibility - to select the column and the range for averaging. run-average.awk 1 2 3 4 5 6 7 8 9 10 #!/usr/bin/awk -f { if ( ! col ) col = 1 if ( ! size ) size = 5 mod = NR % size ; if ( NR <= size ){ count ++ } else { sum -= array [ mod ] }; sum += $ ( col ); array [ mod ] = $ ( col ); print sum / count } Here is how the result from the script looks like. This plot is generated from the Gnuplot command line plot [ : 2000 ] \"temp.dat\" w l , \\ \"< ./run-average.awk size=100 temp.dat\" w l lw 3 lc rgb \"blue\" , \\ \"< ./run-average.awk size=500 temp.dat\" w l lw 3 lc rgb \"green\" This easily \"filters out\" the oscillations and showing the averages for the selected size... Even more the not-so-easy to see drift is clearly visible now ( green line ). Essentially, the program makes an average over selected size range of the previous data at each point. What is particularly smart wit this solution is that the average is done by updating the sum from numbers kept in an array of size size . At every line, the new value is added to the sum and one is subtracted - the one that falls now outside the range for averaging. The position of the element is derived by the reminder computed by the modulo operation mod = NR % size 1 . In principle, the code is only two lines, but the rest is taking care of the initial region where the data is not enough to make an average over the specified size . This is a smart realization of FIFO (first in first out) manipulation of data structure implemented on few lines in this script. Compare this to the code referred in the wikipage . Note I have used to pass different parameters size=100 adn size=500 directly in the gnuplot command instead of a filename ( not all programs allow you to make such neat substitutions ) but this is one example where awk gets handy - just write you one line script between \"< \" . The value of such programs is that it makes it easy to add or modify small bits, since there is not much of a code anyway. Files run-average.awk temp.dat Gnuawk: Arithmetic Operators \u21a9","title":"Running/moving/rolling average"},{"location":"More_awk/User_defined_functions/","text":"User defined functions Related Awk documentation Usually at this point ( looking on how to write own functions ) you have a working script ( I doubt you want to do this on one line ) and you want to wrap your repetitive task in a function call... The User defined function syntax is as follows function name ([ parameter - list ]) { body - of - function [ return variable ] } Here is a simple example. It is rather surprising but awk dows not have funtion to return athe absolute value of a number. function abs ( x ){ if ( x >= 0 ) return x else return - x } or function abs ( x ) { return ( x >= 0 ) ? x : - x } Here is another example from one of the study cases Gaussian smearing Gaussian funtion . I have used \\(x_0\\) instead of \\(b\\) for the peak center. function gauss ( x0 , x , c ){ area = 1 ; if (( x - x0 ) ** 2 < 10000 ) { return area * exp ( - ((( x - x0 )) ** 2 ) / ( 2 . * c ** 2 ))} else { return 0.0 } } The function returns the value for the Gaussian for a point \\(x\\) away from the center. For large \\((x-x0)^2\\) the exp() was crashing, so I needed to add a condition which makes sure that for these very large numbers the function does not call the exp(...) but returns directly 0.0 instead. The function is called within a double loop on line 23 23 data [ i ] = data [ i ] + gauss ( freq [ f ], i , FWHM ); Warning It's entirely fair to say that the awk syntax for local variable definitions is appallingly awful. - Brian Kernighan source","title":"User defined functions"},{"location":"More_awk/User_defined_functions/#user-defined-functions","text":"Related Awk documentation Usually at this point ( looking on how to write own functions ) you have a working script ( I doubt you want to do this on one line ) and you want to wrap your repetitive task in a function call... The User defined function syntax is as follows function name ([ parameter - list ]) { body - of - function [ return variable ] } Here is a simple example. It is rather surprising but awk dows not have funtion to return athe absolute value of a number. function abs ( x ){ if ( x >= 0 ) return x else return - x } or function abs ( x ) { return ( x >= 0 ) ? x : - x } Here is another example from one of the study cases Gaussian smearing Gaussian funtion . I have used \\(x_0\\) instead of \\(b\\) for the peak center. function gauss ( x0 , x , c ){ area = 1 ; if (( x - x0 ) ** 2 < 10000 ) { return area * exp ( - ((( x - x0 )) ** 2 ) / ( 2 . * c ** 2 ))} else { return 0.0 } } The function returns the value for the Gaussian for a point \\(x\\) away from the center. For large \\((x-x0)^2\\) the exp() was crashing, so I needed to add a condition which makes sure that for these very large numbers the function does not call the exp(...) but returns directly 0.0 instead. The function is called within a double loop on line 23 23 data [ i ] = data [ i ] + gauss ( freq [ f ], i , FWHM ); Warning It's entirely fair to say that the awk syntax for local variable definitions is appallingly awful. - Brian Kernighan source","title":"User defined functions"},{"location":"Other/Backreferences/","text":"Backreferences This is kind of common situation. You know how to do it with sed or perl but now, for whatever reason, you want to do it with awk. Googling around and reading some other sources, can give you the impression that backreference is not implemented in awk... Here is an example how one can \"translate\" sed backreference in awk. sed $ echo \">seq12/1-100\" | sed -e 's,>seq\\(.*\\)\\/.*,>SEQ-\\1,g' >SEQ-12 awk $ echo \">seq12/1-100\" | awk ' {print gensub( />seq(.*)\\/.*/ , \">SEQ-\\\\1\" , \"g\")} ' > SEQ - 12 Note: if you are looking just to rename the entries in a Fasta file, perhaps there is way easier way to do it: Renaming Entries In A Fasta File Here is another example, where I can mention some disadvantages with awk (or perhaps I could not find how to do it properly). filedata /home/ux/user/z156256 /home/ux/user/z056254 /home/lx/user/z106253 /home/ux/user/z150252 /home/mp/user/z056254 /home/lx/user/z106253 sed $ sed -e 's,/home/\\(..\\)/user/\\(z[0-9]\\{6\\}\\),/usr/\\2/\\1,g' filedata /usr/z156256/ux /usr/z056254/ux /usr/z106253/lx /usr/z150252/ux /usr/z056254/mp /usr/z106253/lx awk $ awk '{print gensub(/\\/home\\/(..)\\/user\\/(z[0-9][0-9][0-9][0-9][0-9][0-9])/,\"/usr/\\\\2/\\\\1\",\"g\")}' filedata /usr/z156256/ux /usr/z056254/ux /usr/z106253/lx /usr/z150252/ux /usr/z056254/mp /usr/z106253/lx ... and as a colleague of mine, Matti Hellstr\u00f6m, pointed out - the proper \"awk way\" is: $ awk - F / '{print \"/usr/\" $5 \"/\" $3}' filedata /usr/z156256/ux /usr/z056254/ux /usr/z106253/lx /usr/z150252/ux /usr/z056254/mp /usr/z106253/lx","title":"Backreferences"},{"location":"Other/Backreferences/#backreferences","text":"This is kind of common situation. You know how to do it with sed or perl but now, for whatever reason, you want to do it with awk. Googling around and reading some other sources, can give you the impression that backreference is not implemented in awk... Here is an example how one can \"translate\" sed backreference in awk. sed $ echo \">seq12/1-100\" | sed -e 's,>seq\\(.*\\)\\/.*,>SEQ-\\1,g' >SEQ-12 awk $ echo \">seq12/1-100\" | awk ' {print gensub( />seq(.*)\\/.*/ , \">SEQ-\\\\1\" , \"g\")} ' > SEQ - 12 Note: if you are looking just to rename the entries in a Fasta file, perhaps there is way easier way to do it: Renaming Entries In A Fasta File Here is another example, where I can mention some disadvantages with awk (or perhaps I could not find how to do it properly). filedata /home/ux/user/z156256 /home/ux/user/z056254 /home/lx/user/z106253 /home/ux/user/z150252 /home/mp/user/z056254 /home/lx/user/z106253 sed $ sed -e 's,/home/\\(..\\)/user/\\(z[0-9]\\{6\\}\\),/usr/\\2/\\1,g' filedata /usr/z156256/ux /usr/z056254/ux /usr/z106253/lx /usr/z150252/ux /usr/z056254/mp /usr/z106253/lx awk $ awk '{print gensub(/\\/home\\/(..)\\/user\\/(z[0-9][0-9][0-9][0-9][0-9][0-9])/,\"/usr/\\\\2/\\\\1\",\"g\")}' filedata /usr/z156256/ux /usr/z056254/ux /usr/z106253/lx /usr/z150252/ux /usr/z056254/mp /usr/z106253/lx ... and as a colleague of mine, Matti Hellstr\u00f6m, pointed out - the proper \"awk way\" is: $ awk - F / '{print \"/usr/\" $5 \"/\" $3}' filedata /usr/z156256/ux /usr/z056254/ux /usr/z106253/lx /usr/z150252/ux /usr/z056254/mp /usr/z106253/lx","title":"Backreferences"},{"location":"Other/Binder/","text":"GitHub & Binder Start a Binder virtual environment for the course with all GitHub files available (click on this link ). When the Virtual environment is ready and running, choose \"New/Terminal\" from the drop-down menu on the top-right of the page.","title":"GitHub & Binder"},{"location":"Other/Binder/#github-binder","text":"Start a Binder virtual environment for the course with all GitHub files available (click on this link ). When the Virtual environment is ready and running, choose \"New/Terminal\" from the drop-down menu on the top-right of the page.","title":"GitHub &amp; Binder"},{"location":"Other/Fixed_size_fields/","text":"Fixed size fields Your file has fixed length of the data fields... Here is a good example how to deal with it: $ echo 20140805234656 | awk 'BEGIN { FIELDWIDTHS = \"4 2 2 2 2 2\" } { printf \"%s-%s-%s %s:%s:%s\\n\", $1, $2, $3, $4, $5, $6 }' 2014 - 08 - 05 23 : 46 : 56","title":"Fixed size fields"},{"location":"Other/Fixed_size_fields/#fixed-size-fields","text":"Your file has fixed length of the data fields... Here is a good example how to deal with it: $ echo 20140805234656 | awk 'BEGIN { FIELDWIDTHS = \"4 2 2 2 2 2\" } { printf \"%s-%s-%s %s:%s:%s\\n\", $1, $2, $3, $4, $5, $6 }' 2014 - 08 - 05 23 : 46 : 56","title":"Fixed size fields"},{"location":"Other/Localization/","text":"Localization problems Well, here are some annoying problems related to some local settings, for example decimal point/comma. US localization with decimal point $ echo 123.2 | gawk ' { print ; print $ 1 + 1 } \u2018 124.2 SE localization with decimal comma $ echo 123 , 2 | gawk ' { print ; print $ 1 + 1 } \u2018 124 SE localization with decimal comma - FIX $ echo 123 , 2 | LC_NUMERIC = se_SE . utf - 8 gawk -- use - lc - numeric ' { printf $ 1 + 1 } \u2018 124 , 2","title":"Localization problems"},{"location":"Other/Localization/#localization-problems","text":"Well, here are some annoying problems related to some local settings, for example decimal point/comma. US localization with decimal point $ echo 123.2 | gawk ' { print ; print $ 1 + 1 } \u2018 124.2 SE localization with decimal comma $ echo 123 , 2 | gawk ' { print ; print $ 1 + 1 } \u2018 124 SE localization with decimal comma - FIX $ echo 123 , 2 | LC_NUMERIC = se_SE . utf - 8 gawk -- use - lc - numeric ' { printf $ 1 + 1 } \u2018 124 , 2","title":"Localization problems"},{"location":"Other/neat_solution_01/","text":"Neat solution #1 Here is a neat example of using associative arrays again. Let's have a list of e-mails which we collected from different sources and unfortunately might contain duplicate entries. Let's use just some regular text instead of mail addresses... In this example \"aa\" and \"aaa\" appear 2 times. aaa bbb aaa aa ccc aa $ awk '!x[$0]++' file . txt or $ echo - e \"aaa\\nbbb\\naaa\\naa\\nccc\\naa\" | awk '!x[$0]++' aaa bbb aa ccc That is really not easy to read/understand - I agree. Do not worry, it is meant only to demonstrate some features that make awk rather god tool for text parsing and simple data manipulation. Here is what happens. By default, if there is no action defined, awk will execute print command which will print the whole line content. Essentially what we have '!x[$0]++' is a matching criteria. When awk starts, x is empty, so on the first line of the file x [ $ 0 ] is empty (that is equal to FALSE) then ! negates the result and ++ actually ads 1 to the default 0 for undefined element, so next time when there is a line with the same text, the negated result will be FALSE and the line will not be printed.","title":"Neat solution #1"},{"location":"Other/neat_solution_01/#neat-solution-1","text":"Here is a neat example of using associative arrays again. Let's have a list of e-mails which we collected from different sources and unfortunately might contain duplicate entries. Let's use just some regular text instead of mail addresses... In this example \"aa\" and \"aaa\" appear 2 times. aaa bbb aaa aa ccc aa $ awk '!x[$0]++' file . txt or $ echo - e \"aaa\\nbbb\\naaa\\naa\\nccc\\naa\" | awk '!x[$0]++' aaa bbb aa ccc That is really not easy to read/understand - I agree. Do not worry, it is meant only to demonstrate some features that make awk rather god tool for text parsing and simple data manipulation. Here is what happens. By default, if there is no action defined, awk will execute print command which will print the whole line content. Essentially what we have '!x[$0]++' is a matching criteria. When awk starts, x is empty, so on the first line of the file x [ $ 0 ] is empty (that is equal to FALSE) then ! negates the result and ++ actually ads 1 to the default 0 for undefined element, so next time when there is a line with the same text, the negated result will be FALSE and the line will not be printed.","title":"Neat solution #1"}]}